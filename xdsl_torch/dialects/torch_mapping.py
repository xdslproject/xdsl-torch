###
# This dialect is automatically generated by xdsl_torch/tools/gen_torch_dialect.py
# Please don't edit it manually!
###

from typing import Dict, Any
import torch
from xdsl_torch.dialects.torch_dialect import *

XDSL_TORCH_OPS: Dict[Any, type] = {
    torch.ops.aten.__and__.Scalar: Torch_Aten_And_ScalarOp,  # type: ignore
    torch.ops.aten.__and__.Tensor: Torch_Aten_And_TensorOp,  # type: ignore
    torch.ops.aten.__and__.bool: Torch_Aten_And_BoolOp,  # type: ignore
    torch.ops.aten.__and__.int: Torch_Aten_And_IntOp,  # type: ignore
    torch.ops.aten.__iand__.Scalar: Torch_Aten_Iand_ScalarOp,  # type: ignore
    torch.ops.aten.__iand__.Tensor: Torch_Aten_Iand_TensorOp,  # type: ignore
    torch.ops.aten.__ilshift__.Scalar: Torch_Aten_Ilshift_ScalarOp,  # type: ignore
    torch.ops.aten.__ilshift__.Tensor: Torch_Aten_Ilshift_TensorOp,  # type: ignore
    torch.ops.aten.__ior__.Scalar: Torch_Aten_Ior_ScalarOp,  # type: ignore
    torch.ops.aten.__ior__.Tensor: Torch_Aten_Ior_TensorOp,  # type: ignore
    torch.ops.aten.__irshift__.Scalar: Torch_Aten_Irshift_ScalarOp,  # type: ignore
    torch.ops.aten.__irshift__.Tensor: Torch_Aten_Irshift_TensorOp,  # type: ignore
    torch.ops.aten.__ixor__.Scalar: Torch_Aten_Ixor_ScalarOp,  # type: ignore
    torch.ops.aten.__ixor__.Tensor: Torch_Aten_Ixor_TensorOp,  # type: ignore
    torch.ops.aten.__lshift__.Scalar: Torch_Aten_Lshift_ScalarOp,  # type: ignore
    torch.ops.aten.__lshift__.Scalar_out: Torch_Aten_Lshift_ScalarOutOp,  # type: ignore
    torch.ops.aten.__lshift__.Tensor: Torch_Aten_Lshift_TensorOp,  # type: ignore
    torch.ops.aten.__lshift__.Tensor_out: Torch_Aten_Lshift_TensorOutOp,  # type: ignore
    torch.ops.aten.__lshift__.int: Torch_Aten_Lshift_IntOp,  # type: ignore
    torch.ops.aten.__or__.Scalar: Torch_Aten_Or_ScalarOp,  # type: ignore
    torch.ops.aten.__or__.Tensor: Torch_Aten_Or_TensorOp,  # type: ignore
    torch.ops.aten.__or__.bool: Torch_Aten_Or_BoolOp,  # type: ignore
    torch.ops.aten.__or__.int: Torch_Aten_Or_IntOp,  # type: ignore
    torch.ops.aten.__rshift__.Scalar: Torch_Aten_Rshift_ScalarOp,  # type: ignore
    torch.ops.aten.__rshift__.Scalar_out: Torch_Aten_Rshift_ScalarOutOp,  # type: ignore
    torch.ops.aten.__rshift__.Tensor: Torch_Aten_Rshift_TensorOp,  # type: ignore
    torch.ops.aten.__rshift__.Tensor_out: Torch_Aten_Rshift_TensorOutOp,  # type: ignore
    torch.ops.aten.__rshift__.int: Torch_Aten_Rshift_IntOp,  # type: ignore
    torch.ops.aten.__xor__.Scalar: Torch_Aten_Xor_ScalarOp,  # type: ignore
    torch.ops.aten.__xor__.Tensor: Torch_Aten_Xor_TensorOp,  # type: ignore
    torch.ops.aten.__xor__.bool: Torch_Aten_Xor_BoolOp,  # type: ignore
    torch.ops.aten.__xor__.int: Torch_Aten_Xor_IntOp,  # type: ignore
    torch.ops.aten._adaptive_avg_pool2d.default: Torch_Aten_AdaptiveAvgPool2DOp,  # type: ignore
    torch.ops.aten._adaptive_avg_pool2d_backward.default: Torch_Aten_AdaptiveAvgPool2DBackwardOp,  # type: ignore
    torch.ops.aten._adaptive_avg_pool3d.default: Torch_Aten_AdaptiveAvgPool3DOp,  # type: ignore
    torch.ops.aten._adaptive_avg_pool3d_backward.default: Torch_Aten_AdaptiveAvgPool3DBackwardOp,  # type: ignore
    torch.ops.aten._add_batch_dim.default: Torch_Aten_AddBatchDimOp,  # type: ignore
    torch.ops.aten._add_relu.Scalar: Torch_Aten_AddReluScalarOp,  # type: ignore
    torch.ops.aten._add_relu.Scalar_out: Torch_Aten_AddReluScalarOutOp,  # type: ignore
    torch.ops.aten._add_relu.Tensor: Torch_Aten_AddReluTensorOp,  # type: ignore
    torch.ops.aten._addmm_activation.default: Torch_Aten_AddmmActivationOp,  # type: ignore
    torch.ops.aten._aminmax.default: Torch_Aten_AminmaxOp,  # type: ignore
    torch.ops.aten._aminmax.dim: Torch_Aten_AminmaxDimOp,  # type: ignore
    torch.ops.aten._aminmax.dim_out: Torch_Aten_AminmaxDimOutOp,  # type: ignore
    torch.ops.aten._amp_foreach_non_finite_check_and_unscale_.default: Torch_Aten_AmpForeachNonFiniteCheckAndUnscale_Op,  # type: ignore
    torch.ops.aten._amp_update_scale_.default: Torch_Aten_AmpUpdateScale_Op,  # type: ignore
    torch.ops.aten._assert_async.default: Torch_Aten_AssertAsyncOp,  # type: ignore
    torch.ops.aten._autocast_to_full_precision.default: Torch_Aten_AutocastToFullPrecisionOp,  # type: ignore
    torch.ops.aten._autocast_to_reduced_precision.default: Torch_Aten_AutocastToReducedPrecisionOp,  # type: ignore
    torch.ops.aten._backward.default: Torch_Aten_BackwardOp,  # type: ignore
    torch.ops.aten._batch_norm_impl_index.default: Torch_Aten_BatchNormImplIndexOp,  # type: ignore
    torch.ops.aten._batch_norm_impl_index_backward.default: Torch_Aten_BatchNormImplIndexBackwardOp,  # type: ignore
    torch.ops.aten._batch_norm_no_update.default: Torch_Aten_BatchNormNoUpdateOp,  # type: ignore
    torch.ops.aten._batch_norm_with_update.default: Torch_Aten_BatchNormWithUpdateOp,  # type: ignore
    torch.ops.aten._batch_norm_with_update_functional.default: Torch_Aten_BatchNormWithUpdateFunctionalOp,  # type: ignore
    torch.ops.aten._cast_Byte.default: Torch_Aten_CastByteOp,  # type: ignore
    torch.ops.aten._cast_Char.default: Torch_Aten_CastCharOp,  # type: ignore
    torch.ops.aten._cast_Double.default: Torch_Aten_CastDoubleOp,  # type: ignore
    torch.ops.aten._cast_Float.default: Torch_Aten_CastFloatOp,  # type: ignore
    torch.ops.aten._cast_Half.default: Torch_Aten_CastHalfOp,  # type: ignore
    torch.ops.aten._cast_Int.default: Torch_Aten_CastIntOp,  # type: ignore
    torch.ops.aten._cast_Long.default: Torch_Aten_CastLongOp,  # type: ignore
    torch.ops.aten._cast_Short.default: Torch_Aten_CastShortOp,  # type: ignore
    torch.ops.aten._cdist_backward.default: Torch_Aten_CdistBackwardOp,  # type: ignore
    torch.ops.aten._cdist_forward.default: Torch_Aten_CdistForwardOp,  # type: ignore
    torch.ops.aten._cholesky_solve_helper.default: Torch_Aten_CholeskySolveHelperOp,  # type: ignore
    torch.ops.aten._choose_qparams_per_tensor.default: Torch_Aten_ChooseQparamsPerTensorOp,  # type: ignore
    torch.ops.aten._chunk_cat.default: Torch_Aten_ChunkCatOp,  # type: ignore
    torch.ops.aten._coalesce.default: Torch_Aten_CoalesceOp,  # type: ignore
    torch.ops.aten._coalesced_.default: Torch_Aten_Coalesced_Op,  # type: ignore
    torch.ops.aten._compute_linear_combination.default: Torch_Aten_ComputeLinearCombinationOp,  # type: ignore
    torch.ops.aten._conj.default: Torch_Aten_ConjOp,  # type: ignore
    torch.ops.aten._conj_copy.default: Torch_Aten_ConjCopyOp,  # type: ignore
    torch.ops.aten._conj_physical.default: Torch_Aten_ConjPhysicalOp,  # type: ignore
    torch.ops.aten._conv_depthwise2d.default: Torch_Aten_ConvDepthwise2DOp,  # type: ignore
    torch.ops.aten._convert_indices_from_coo_to_csr.default: Torch_Aten_ConvertIndicesFromCooToCsrOp,  # type: ignore
    torch.ops.aten._convert_indices_from_csr_to_coo.default: Torch_Aten_ConvertIndicesFromCsrToCooOp,  # type: ignore
    torch.ops.aten._convert_weight_to_int4pack.default: Torch_Aten_ConvertWeightToInt4PackOp,  # type: ignore
    torch.ops.aten._convert_weight_to_int4pack_for_cpu.default: Torch_Aten_ConvertWeightToInt4PackForCpuOp,  # type: ignore
    torch.ops.aten._convolution.default: Torch_Aten_ConvolutionOp,  # type: ignore
    torch.ops.aten._convolution.deprecated: Torch_Aten_ConvolutionDeprecatedOp,  # type: ignore
    torch.ops.aten._convolution_double_backward.default: Torch_Aten_ConvolutionDoubleBackwardOp,  # type: ignore
    torch.ops.aten._copy_from.default: Torch_Aten_CopyFromOp,  # type: ignore
    torch.ops.aten._copy_from_and_resize.default: Torch_Aten_CopyFromAndResizeOp,  # type: ignore
    torch.ops.aten._cslt_compress.default: Torch_Aten_CsltCompressOp,  # type: ignore
    torch.ops.aten._cslt_sparse_mm.default: Torch_Aten_CsltSparseMmOp,  # type: ignore
    torch.ops.aten._cslt_sparse_mm_search.default: Torch_Aten_CsltSparseMmSearchOp,  # type: ignore
    torch.ops.aten._ctc_loss.Tensor: Torch_Aten_CtcLossTensorOp,  # type: ignore
    torch.ops.aten._ctc_loss.Tensor_out: Torch_Aten_CtcLossTensorOutOp,  # type: ignore
    torch.ops.aten._ctc_loss.default: Torch_Aten_CtcLossOp,  # type: ignore
    torch.ops.aten._ctc_loss_backward.Tensor: Torch_Aten_CtcLossBackwardTensorOp,  # type: ignore
    torch.ops.aten._ctc_loss_backward.default: Torch_Aten_CtcLossBackwardOp,  # type: ignore
    torch.ops.aten._cudnn_ctc_loss.Tensor: Torch_Aten_CudnnCtcLossTensorOp,  # type: ignore
    torch.ops.aten._cudnn_ctc_loss.default: Torch_Aten_CudnnCtcLossOp,  # type: ignore
    torch.ops.aten._cudnn_rnn.default: Torch_Aten_CudnnRnnOp,  # type: ignore
    torch.ops.aten._cudnn_rnn_backward.default: Torch_Aten_CudnnRnnBackwardOp,  # type: ignore
    torch.ops.aten._cudnn_rnn_flatten_weight.default: Torch_Aten_CudnnRnnFlattenWeightOp,  # type: ignore
    torch.ops.aten._cufft_clear_plan_cache.default: Torch_Aten_CufftClearPlanCacheOp,  # type: ignore
    torch.ops.aten._cufft_get_plan_cache_max_size.default: Torch_Aten_CufftGetPlanCacheMaxSizeOp,  # type: ignore
    torch.ops.aten._cufft_get_plan_cache_size.default: Torch_Aten_CufftGetPlanCacheSizeOp,  # type: ignore
    torch.ops.aten._cufft_set_plan_cache_max_size.default: Torch_Aten_CufftSetPlanCacheMaxSizeOp,  # type: ignore
    torch.ops.aten._cummax_helper.default: Torch_Aten_CummaxHelperOp,  # type: ignore
    torch.ops.aten._cummin_helper.default: Torch_Aten_CumminHelperOp,  # type: ignore
    torch.ops.aten._debug_has_internal_overlap.default: Torch_Aten_DebugHasInternalOverlapOp,  # type: ignore
    torch.ops.aten._dimI.default: Torch_Aten_DimiOp,  # type: ignore
    torch.ops.aten._dimV.default: Torch_Aten_DimvOp,  # type: ignore
    torch.ops.aten._dim_arange.default: Torch_Aten_DimArangeOp,  # type: ignore
    torch.ops.aten._dirichlet_grad.default: Torch_Aten_DirichletGradOp,  # type: ignore
    torch.ops.aten._efficient_attention_backward.default: Torch_Aten_EfficientAttentionBackwardOp,  # type: ignore
    torch.ops.aten._efficient_attention_forward.default: Torch_Aten_EfficientAttentionForwardOp,  # type: ignore
    torch.ops.aten._embedding_bag.default: Torch_Aten_EmbeddingBagOp,  # type: ignore
    torch.ops.aten._embedding_bag_backward.default: Torch_Aten_EmbeddingBagBackwardOp,  # type: ignore
    torch.ops.aten._embedding_bag_dense_backward.default: Torch_Aten_EmbeddingBagDenseBackwardOp,  # type: ignore
    torch.ops.aten._embedding_bag_forward_only.default: Torch_Aten_EmbeddingBagForwardOnlyOp,  # type: ignore
    torch.ops.aten._embedding_bag_per_sample_weights_backward.default: Torch_Aten_EmbeddingBagPerSampleWeightsBackwardOp,  # type: ignore
    torch.ops.aten._embedding_bag_sparse_backward.default: Torch_Aten_EmbeddingBagSparseBackwardOp,  # type: ignore
    torch.ops.aten._euclidean_dist.default: Torch_Aten_EuclideanDistOp,  # type: ignore
    torch.ops.aten._fake_quantize_learnable_per_channel_affine.default: Torch_Aten_FakeQuantizeLearnablePerChannelAffineOp,  # type: ignore
    torch.ops.aten._fake_quantize_learnable_per_channel_affine_backward.default: Torch_Aten_FakeQuantizeLearnablePerChannelAffineBackwardOp,  # type: ignore
    torch.ops.aten._fake_quantize_learnable_per_tensor_affine.default: Torch_Aten_FakeQuantizeLearnablePerTensorAffineOp,  # type: ignore
    torch.ops.aten._fake_quantize_learnable_per_tensor_affine_backward.default: Torch_Aten_FakeQuantizeLearnablePerTensorAffineBackwardOp,  # type: ignore
    torch.ops.aten._fake_quantize_per_tensor_affine_cachemask_tensor_qparams.default: Torch_Aten_FakeQuantizePerTensorAffineCachemaskTensorQparamsOp,  # type: ignore
    torch.ops.aten._fft_c2c.default: Torch_Aten_FftC2COp,  # type: ignore
    torch.ops.aten._fft_c2r.default: Torch_Aten_FftC2ROp,  # type: ignore
    torch.ops.aten._fft_r2c.default: Torch_Aten_FftR2COp,  # type: ignore
    torch.ops.aten._fill_mem_eff_dropout_mask_.default: Torch_Aten_FillMemEffDropoutMask_Op,  # type: ignore
    torch.ops.aten._flash_attention_backward.default: Torch_Aten_FlashAttentionBackwardOp,  # type: ignore
    torch.ops.aten._flash_attention_forward.default: Torch_Aten_FlashAttentionForwardOp,  # type: ignore
    torch.ops.aten._foobar.default: Torch_Aten_FoobarOp,  # type: ignore
    torch.ops.aten._foreach_abs.default: Torch_Aten_ForeachAbsOp,  # type: ignore
    torch.ops.aten._foreach_abs_.default: Torch_Aten_ForeachAbs_Op,  # type: ignore
    torch.ops.aten._foreach_acos.default: Torch_Aten_ForeachAcosOp,  # type: ignore
    torch.ops.aten._foreach_acos_.default: Torch_Aten_ForeachAcos_Op,  # type: ignore
    torch.ops.aten._foreach_asin.default: Torch_Aten_ForeachAsinOp,  # type: ignore
    torch.ops.aten._foreach_asin_.default: Torch_Aten_ForeachAsin_Op,  # type: ignore
    torch.ops.aten._foreach_atan.default: Torch_Aten_ForeachAtanOp,  # type: ignore
    torch.ops.aten._foreach_atan_.default: Torch_Aten_ForeachAtan_Op,  # type: ignore
    torch.ops.aten._foreach_ceil.default: Torch_Aten_ForeachCeilOp,  # type: ignore
    torch.ops.aten._foreach_ceil_.default: Torch_Aten_ForeachCeil_Op,  # type: ignore
    torch.ops.aten._foreach_copy.default: Torch_Aten_ForeachCopyOp,  # type: ignore
    torch.ops.aten._foreach_copy_.default: Torch_Aten_ForeachCopy_Op,  # type: ignore
    torch.ops.aten._foreach_cos.default: Torch_Aten_ForeachCosOp,  # type: ignore
    torch.ops.aten._foreach_cos_.default: Torch_Aten_ForeachCos_Op,  # type: ignore
    torch.ops.aten._foreach_cosh.default: Torch_Aten_ForeachCoshOp,  # type: ignore
    torch.ops.aten._foreach_cosh_.default: Torch_Aten_ForeachCosh_Op,  # type: ignore
    torch.ops.aten._foreach_erf.default: Torch_Aten_ForeachErfOp,  # type: ignore
    torch.ops.aten._foreach_erf_.default: Torch_Aten_ForeachErf_Op,  # type: ignore
    torch.ops.aten._foreach_erfc.default: Torch_Aten_ForeachErfcOp,  # type: ignore
    torch.ops.aten._foreach_erfc_.default: Torch_Aten_ForeachErfc_Op,  # type: ignore
    torch.ops.aten._foreach_exp.default: Torch_Aten_ForeachExpOp,  # type: ignore
    torch.ops.aten._foreach_exp_.default: Torch_Aten_ForeachExp_Op,  # type: ignore
    torch.ops.aten._foreach_expm1.default: Torch_Aten_ForeachExpm1Op,  # type: ignore
    torch.ops.aten._foreach_expm1_.default: Torch_Aten_ForeachExpm1_Op,  # type: ignore
    torch.ops.aten._foreach_floor.default: Torch_Aten_ForeachFloorOp,  # type: ignore
    torch.ops.aten._foreach_floor_.default: Torch_Aten_ForeachFloor_Op,  # type: ignore
    torch.ops.aten._foreach_frac.default: Torch_Aten_ForeachFracOp,  # type: ignore
    torch.ops.aten._foreach_frac_.default: Torch_Aten_ForeachFrac_Op,  # type: ignore
    torch.ops.aten._foreach_lgamma.default: Torch_Aten_ForeachLgammaOp,  # type: ignore
    torch.ops.aten._foreach_lgamma_.default: Torch_Aten_ForeachLgamma_Op,  # type: ignore
    torch.ops.aten._foreach_log.default: Torch_Aten_ForeachLogOp,  # type: ignore
    torch.ops.aten._foreach_log10.default: Torch_Aten_ForeachLog10Op,  # type: ignore
    torch.ops.aten._foreach_log10_.default: Torch_Aten_ForeachLog10_Op,  # type: ignore
    torch.ops.aten._foreach_log1p.default: Torch_Aten_ForeachLog1POp,  # type: ignore
    torch.ops.aten._foreach_log1p_.default: Torch_Aten_ForeachLog1P_Op,  # type: ignore
    torch.ops.aten._foreach_log2.default: Torch_Aten_ForeachLog2Op,  # type: ignore
    torch.ops.aten._foreach_log2_.default: Torch_Aten_ForeachLog2_Op,  # type: ignore
    torch.ops.aten._foreach_log_.default: Torch_Aten_ForeachLog_Op,  # type: ignore
    torch.ops.aten._foreach_max.default: Torch_Aten_ForeachMaxOp,  # type: ignore
    torch.ops.aten._foreach_neg.default: Torch_Aten_ForeachNegOp,  # type: ignore
    torch.ops.aten._foreach_neg_.default: Torch_Aten_ForeachNeg_Op,  # type: ignore
    torch.ops.aten._foreach_reciprocal.default: Torch_Aten_ForeachReciprocalOp,  # type: ignore
    torch.ops.aten._foreach_reciprocal_.default: Torch_Aten_ForeachReciprocal_Op,  # type: ignore
    torch.ops.aten._foreach_round.default: Torch_Aten_ForeachRoundOp,  # type: ignore
    torch.ops.aten._foreach_round_.default: Torch_Aten_ForeachRound_Op,  # type: ignore
    torch.ops.aten._foreach_rsqrt.default: Torch_Aten_ForeachRsqrtOp,  # type: ignore
    torch.ops.aten._foreach_rsqrt_.default: Torch_Aten_ForeachRsqrt_Op,  # type: ignore
    torch.ops.aten._foreach_sigmoid.default: Torch_Aten_ForeachSigmoidOp,  # type: ignore
    torch.ops.aten._foreach_sigmoid_.default: Torch_Aten_ForeachSigmoid_Op,  # type: ignore
    torch.ops.aten._foreach_sign.default: Torch_Aten_ForeachSignOp,  # type: ignore
    torch.ops.aten._foreach_sign_.default: Torch_Aten_ForeachSign_Op,  # type: ignore
    torch.ops.aten._foreach_sin.default: Torch_Aten_ForeachSinOp,  # type: ignore
    torch.ops.aten._foreach_sin_.default: Torch_Aten_ForeachSin_Op,  # type: ignore
    torch.ops.aten._foreach_sinh.default: Torch_Aten_ForeachSinhOp,  # type: ignore
    torch.ops.aten._foreach_sinh_.default: Torch_Aten_ForeachSinh_Op,  # type: ignore
    torch.ops.aten._foreach_sqrt.default: Torch_Aten_ForeachSqrtOp,  # type: ignore
    torch.ops.aten._foreach_sqrt_.default: Torch_Aten_ForeachSqrt_Op,  # type: ignore
    torch.ops.aten._foreach_tan.default: Torch_Aten_ForeachTanOp,  # type: ignore
    torch.ops.aten._foreach_tan_.default: Torch_Aten_ForeachTan_Op,  # type: ignore
    torch.ops.aten._foreach_tanh.default: Torch_Aten_ForeachTanhOp,  # type: ignore
    torch.ops.aten._foreach_tanh_.default: Torch_Aten_ForeachTanh_Op,  # type: ignore
    torch.ops.aten._foreach_trunc.default: Torch_Aten_ForeachTruncOp,  # type: ignore
    torch.ops.aten._foreach_trunc_.default: Torch_Aten_ForeachTrunc_Op,  # type: ignore
    torch.ops.aten._foreach_zero_.default: Torch_Aten_ForeachZero_Op,  # type: ignore
    torch.ops.aten._functional_sym_constrain_range.default: Torch_Aten_FunctionalSymConstrainRangeOp,  # type: ignore
    torch.ops.aten._functional_sym_constrain_range_for_size.default: Torch_Aten_FunctionalSymConstrainRangeForSizeOp,  # type: ignore
    torch.ops.aten._fused_adagrad_.default: Torch_Aten_FusedAdagrad_Op,  # type: ignore
    torch.ops.aten._fused_adam.default: Torch_Aten_FusedAdamOp,  # type: ignore
    torch.ops.aten._fused_adam.tensor_lr: Torch_Aten_FusedAdamTensorLrOp,  # type: ignore
    torch.ops.aten._fused_adam.tensor_lr_out: Torch_Aten_FusedAdamTensorLrOutOp,  # type: ignore
    torch.ops.aten._fused_adam_.default: Torch_Aten_FusedAdam_Op,  # type: ignore
    torch.ops.aten._fused_adam_.tensor_lr: Torch_Aten_FusedAdam_TensorLrOp,  # type: ignore
    torch.ops.aten._fused_adamw_.default: Torch_Aten_FusedAdamw_Op,  # type: ignore
    torch.ops.aten._fused_adamw_.tensor_lr: Torch_Aten_FusedAdamw_TensorLrOp,  # type: ignore
    torch.ops.aten._fused_moving_avg_obs_fq_helper.default: Torch_Aten_FusedMovingAvgObsFqHelperOp,  # type: ignore
    torch.ops.aten._fused_sdp_choice.default: Torch_Aten_FusedSdpChoiceOp,  # type: ignore
    torch.ops.aten._fused_sgd_.default: Torch_Aten_FusedSgd_Op,  # type: ignore
    torch.ops.aten._fused_sgd_.tensor_lr: Torch_Aten_FusedSgd_TensorLrOp,  # type: ignore
    torch.ops.aten._fw_primal.default: Torch_Aten_FwPrimalOp,  # type: ignore
    torch.ops.aten._fw_primal_copy.default: Torch_Aten_FwPrimalCopyOp,  # type: ignore
    torch.ops.aten._gather_sparse_backward.default: Torch_Aten_GatherSparseBackwardOp,  # type: ignore
    torch.ops.aten._grid_sampler_2d_cpu_fallback.default: Torch_Aten_GridSampler2DCpuFallbackOp,  # type: ignore
    torch.ops.aten._grid_sampler_2d_cpu_fallback_backward.default: Torch_Aten_GridSampler2DCpuFallbackBackwardOp,  # type: ignore
    torch.ops.aten._has_same_storage_numel.default: Torch_Aten_HasSameStorageNumelOp,  # type: ignore
    torch.ops.aten._histogramdd_bin_edges.default: Torch_Aten_HistogramddBinEdgesOp,  # type: ignore
    torch.ops.aten._histogramdd_from_bin_cts.default: Torch_Aten_HistogramddFromBinCtsOp,  # type: ignore
    torch.ops.aten._histogramdd_from_bin_tensors.default: Torch_Aten_HistogramddFromBinTensorsOp,  # type: ignore
    torch.ops.aten._index_put_impl_.hacked_twin: Torch_Aten_IndexPutImpl_HackedTwinOp,  # type: ignore
    torch.ops.aten._indices.default: Torch_Aten_IndicesOp,  # type: ignore
    torch.ops.aten._indices_copy.default: Torch_Aten_IndicesCopyOp,  # type: ignore
    torch.ops.aten._int_mm.default: Torch_Aten_IntMmOp,  # type: ignore
    torch.ops.aten._is_all_true.default: Torch_Aten_IsAllTrueOp,  # type: ignore
    torch.ops.aten._is_any_true.default: Torch_Aten_IsAnyTrueOp,  # type: ignore
    torch.ops.aten._is_zerotensor.default: Torch_Aten_IsZerotensorOp,  # type: ignore
    torch.ops.aten._jagged_to_padded_dense_forward.default: Torch_Aten_JaggedToPaddedDenseForwardOp,  # type: ignore
    torch.ops.aten._lazy_clone.default: Torch_Aten_LazyCloneOp,  # type: ignore
    torch.ops.aten._linalg_det.default: Torch_Aten_LinalgDetOp,  # type: ignore
    torch.ops.aten._linalg_eigvals.default: Torch_Aten_LinalgEigvalsOp,  # type: ignore
    torch.ops.aten._linalg_slogdet.default: Torch_Aten_LinalgSlogdetOp,  # type: ignore
    torch.ops.aten._linalg_solve_ex.default: Torch_Aten_LinalgSolveExOp,  # type: ignore
    torch.ops.aten._list_to_tensor.default: Torch_Aten_ListToTensorOp,  # type: ignore
    torch.ops.aten._local_scalar_dense.default: Torch_Aten_LocalScalarDenseOp,  # type: ignore
    torch.ops.aten._log_softmax.default: Torch_Aten_LogSoftmaxOp,  # type: ignore
    torch.ops.aten._log_softmax_backward_data.default: Torch_Aten_LogSoftmaxBackwardDataOp,  # type: ignore
    torch.ops.aten._logcumsumexp.default: Torch_Aten_LogcumsumexpOp,  # type: ignore
    torch.ops.aten._lstm_mps.default: Torch_Aten_LstmMpsOp,  # type: ignore
    torch.ops.aten._lu_with_info.default: Torch_Aten_LuWithInfoOp,  # type: ignore
    torch.ops.aten._make_dual.default: Torch_Aten_MakeDualOp,  # type: ignore
    torch.ops.aten._make_dual_copy.default: Torch_Aten_MakeDualCopyOp,  # type: ignore
    torch.ops.aten._make_per_channel_quantized_tensor.default: Torch_Aten_MakePerChannelQuantizedTensorOp,  # type: ignore
    torch.ops.aten._make_per_tensor_quantized_tensor.default: Torch_Aten_MakePerTensorQuantizedTensorOp,  # type: ignore
    torch.ops.aten._masked_scale.default: Torch_Aten_MaskedScaleOp,  # type: ignore
    torch.ops.aten._masked_softmax.default: Torch_Aten_MaskedSoftmaxOp,  # type: ignore
    torch.ops.aten._masked_softmax_backward.default: Torch_Aten_MaskedSoftmaxBackwardOp,  # type: ignore
    torch.ops.aten._mkldnn_reshape.default: Torch_Aten_MkldnnReshapeOp,  # type: ignore
    torch.ops.aten._mkldnn_transpose.default: Torch_Aten_MkldnnTransposeOp,  # type: ignore
    torch.ops.aten._mkldnn_transpose_.default: Torch_Aten_MkldnnTranspose_Op,  # type: ignore
    torch.ops.aten._mps_convolution.default: Torch_Aten_MpsConvolutionOp,  # type: ignore
    torch.ops.aten._mps_convolution_transpose.default: Torch_Aten_MpsConvolutionTransposeOp,  # type: ignore
    torch.ops.aten._native_batch_norm_legit.default: Torch_Aten_NativeBatchNormLegitOp,  # type: ignore
    torch.ops.aten._native_batch_norm_legit.no_stats: Torch_Aten_NativeBatchNormLegitNoStatsOp,  # type: ignore
    torch.ops.aten._native_batch_norm_legit.no_stats_out: Torch_Aten_NativeBatchNormLegitNoStatsOutOp,  # type: ignore
    torch.ops.aten._native_batch_norm_legit_functional.default: Torch_Aten_NativeBatchNormLegitFunctionalOp,  # type: ignore
    torch.ops.aten._native_batch_norm_legit_no_training.default: Torch_Aten_NativeBatchNormLegitNoTrainingOp,  # type: ignore
    torch.ops.aten._native_multi_head_attention.default: Torch_Aten_NativeMultiHeadAttentionOp,  # type: ignore
    torch.ops.aten._neg_view.default: Torch_Aten_NegViewOp,  # type: ignore
    torch.ops.aten._neg_view_copy.default: Torch_Aten_NegViewCopyOp,  # type: ignore
    torch.ops.aten._nested_compute_contiguous_strides_offsets.default: Torch_Aten_NestedComputeContiguousStridesOffsetsOp,  # type: ignore
    torch.ops.aten._nested_from_padded.default: Torch_Aten_NestedFromPaddedOp,  # type: ignore
    torch.ops.aten._nested_from_padded_and_nested_example.default: Torch_Aten_NestedFromPaddedAndNestedExampleOp,  # type: ignore
    torch.ops.aten._nested_from_padded_tensor.default: Torch_Aten_NestedFromPaddedTensorOp,  # type: ignore
    torch.ops.aten._nested_get_jagged_dummy.default: Torch_Aten_NestedGetJaggedDummyOp,  # type: ignore
    torch.ops.aten._nested_get_lengths.default: Torch_Aten_NestedGetLengthsOp,  # type: ignore
    torch.ops.aten._nested_get_max_seqlen.default: Torch_Aten_NestedGetMaxSeqlenOp,  # type: ignore
    torch.ops.aten._nested_get_min_seqlen.default: Torch_Aten_NestedGetMinSeqlenOp,  # type: ignore
    torch.ops.aten._nested_get_offsets.default: Torch_Aten_NestedGetOffsetsOp,  # type: ignore
    torch.ops.aten._nested_get_ragged_idx.default: Torch_Aten_NestedGetRaggedIdxOp,  # type: ignore
    torch.ops.aten._nested_get_values.default: Torch_Aten_NestedGetValuesOp,  # type: ignore
    torch.ops.aten._nested_get_values_copy.default: Torch_Aten_NestedGetValuesCopyOp,  # type: ignore
    torch.ops.aten._nested_select_backward.default: Torch_Aten_NestedSelectBackwardOp,  # type: ignore
    torch.ops.aten._nested_sum_backward.default: Torch_Aten_NestedSumBackwardOp,  # type: ignore
    torch.ops.aten._nested_tensor_from_mask.default: Torch_Aten_NestedTensorFromMaskOp,  # type: ignore
    torch.ops.aten._nested_tensor_from_mask_left_aligned.default: Torch_Aten_NestedTensorFromMaskLeftAlignedOp,  # type: ignore
    torch.ops.aten._nested_tensor_size.default: Torch_Aten_NestedTensorSizeOp,  # type: ignore
    torch.ops.aten._nested_tensor_softmax_with_shape.default: Torch_Aten_NestedTensorSoftmaxWithShapeOp,  # type: ignore
    torch.ops.aten._nested_tensor_storage_offsets.default: Torch_Aten_NestedTensorStorageOffsetsOp,  # type: ignore
    torch.ops.aten._nested_tensor_strides.default: Torch_Aten_NestedTensorStridesOp,  # type: ignore
    torch.ops.aten._nested_view_from_buffer.default: Torch_Aten_NestedViewFromBufferOp,  # type: ignore
    torch.ops.aten._nested_view_from_buffer_copy.default: Torch_Aten_NestedViewFromBufferCopyOp,  # type: ignore
    torch.ops.aten._nested_view_from_jagged.default: Torch_Aten_NestedViewFromJaggedOp,  # type: ignore
    torch.ops.aten._nested_view_from_jagged_copy.default: Torch_Aten_NestedViewFromJaggedCopyOp,  # type: ignore
    torch.ops.aten._new_zeros_with_same_feature_meta.default: Torch_Aten_NewZerosWithSameFeatureMetaOp,  # type: ignore
    torch.ops.aten._nnpack_available.default: Torch_Aten_NnpackAvailableOp,  # type: ignore
    torch.ops.aten._nnpack_spatial_convolution.default: Torch_Aten_NnpackSpatialConvolutionOp,  # type: ignore
    torch.ops.aten._nnz.default: Torch_Aten_NnzOp,  # type: ignore
    torch.ops.aten._pack_padded_sequence.default: Torch_Aten_PackPaddedSequenceOp,  # type: ignore
    torch.ops.aten._pack_padded_sequence_backward.default: Torch_Aten_PackPaddedSequenceBackwardOp,  # type: ignore
    torch.ops.aten._pad_circular.default: Torch_Aten_PadCircularOp,  # type: ignore
    torch.ops.aten._pad_enum.default: Torch_Aten_PadEnumOp,  # type: ignore
    torch.ops.aten._pad_packed_sequence.default: Torch_Aten_PadPackedSequenceOp,  # type: ignore
    torch.ops.aten._padded_dense_to_jagged_forward.default: Torch_Aten_PaddedDenseToJaggedForwardOp,  # type: ignore
    torch.ops.aten._pdist_backward.default: Torch_Aten_PdistBackwardOp,  # type: ignore
    torch.ops.aten._pdist_forward.default: Torch_Aten_PdistForwardOp,  # type: ignore
    torch.ops.aten._prelu_kernel.default: Torch_Aten_PreluKernelOp,  # type: ignore
    torch.ops.aten._prelu_kernel_backward.default: Torch_Aten_PreluKernelBackwardOp,  # type: ignore
    torch.ops.aten._propagate_xla_data.default: Torch_Aten_PropagateXlaDataOp,  # type: ignore
    torch.ops.aten._remove_batch_dim.default: Torch_Aten_RemoveBatchDimOp,  # type: ignore
    torch.ops.aten._reshape_alias.default: Torch_Aten_ReshapeAliasOp,  # type: ignore
    torch.ops.aten._reshape_alias_copy.default: Torch_Aten_ReshapeAliasCopyOp,  # type: ignore
    torch.ops.aten._reshape_copy.default: Torch_Aten_ReshapeCopyOp,  # type: ignore
    torch.ops.aten._reshape_from_tensor.default: Torch_Aten_ReshapeFromTensorOp,  # type: ignore
    torch.ops.aten._rowwise_prune.default: Torch_Aten_RowwisePruneOp,  # type: ignore
    torch.ops.aten._safe_softmax.default: Torch_Aten_SafeSoftmaxOp,  # type: ignore
    torch.ops.aten._saturate_weight_to_fp16.default: Torch_Aten_SaturateWeightToFp16Op,  # type: ignore
    torch.ops.aten._scaled_dot_product_attention_math.default: Torch_Aten_ScaledDotProductAttentionMathOp,  # type: ignore
    torch.ops.aten._scaled_dot_product_attention_math_for_mps.default: Torch_Aten_ScaledDotProductAttentionMathForMpsOp,  # type: ignore
    torch.ops.aten._scaled_dot_product_cudnn_attention.default: Torch_Aten_ScaledDotProductCudnnAttentionOp,  # type: ignore
    torch.ops.aten._scaled_dot_product_cudnn_attention_backward.default: Torch_Aten_ScaledDotProductCudnnAttentionBackwardOp,  # type: ignore
    torch.ops.aten._scaled_dot_product_efficient_attention.default: Torch_Aten_ScaledDotProductEfficientAttentionOp,  # type: ignore
    torch.ops.aten._scaled_dot_product_efficient_attention_backward.default: Torch_Aten_ScaledDotProductEfficientAttentionBackwardOp,  # type: ignore
    torch.ops.aten._scaled_dot_product_flash_attention.default: Torch_Aten_ScaledDotProductFlashAttentionOp,  # type: ignore
    torch.ops.aten._scaled_dot_product_flash_attention_backward.default: Torch_Aten_ScaledDotProductFlashAttentionBackwardOp,  # type: ignore
    torch.ops.aten._scaled_dot_product_flash_attention_for_cpu.default: Torch_Aten_ScaledDotProductFlashAttentionForCpuOp,  # type: ignore
    torch.ops.aten._scaled_dot_product_flash_attention_for_cpu_backward.default: Torch_Aten_ScaledDotProductFlashAttentionForCpuBackwardOp,  # type: ignore
    torch.ops.aten._scaled_dot_product_fused_attention_overrideable.default: Torch_Aten_ScaledDotProductFusedAttentionOverrideableOp,  # type: ignore
    torch.ops.aten._scaled_dot_product_fused_attention_overrideable_backward.default: Torch_Aten_ScaledDotProductFusedAttentionOverrideableBackwardOp,  # type: ignore
    torch.ops.aten._scaled_mm.default: Torch_Aten_ScaledMmOp,  # type: ignore
    torch.ops.aten._shape_as_tensor.default: Torch_Aten_ShapeAsTensorOp,  # type: ignore
    torch.ops.aten._slow_conv2d_forward.default: Torch_Aten_SlowConv2DForwardOp,  # type: ignore
    torch.ops.aten._slow_conv2d_forward.output: Torch_Aten_SlowConv2DForwardOutputOp,  # type: ignore
    torch.ops.aten._sobol_engine_draw.default: Torch_Aten_SobolEngineDrawOp,  # type: ignore
    torch.ops.aten._sobol_engine_ff_.default: Torch_Aten_SobolEngineFf_Op,  # type: ignore
    torch.ops.aten._sobol_engine_initialize_state_.default: Torch_Aten_SobolEngineInitializeState_Op,  # type: ignore
    torch.ops.aten._sobol_engine_scramble_.default: Torch_Aten_SobolEngineScramble_Op,  # type: ignore
    torch.ops.aten._softmax.default: Torch_Aten_SoftmaxOp,  # type: ignore
    torch.ops.aten._softmax_backward_data.default: Torch_Aten_SoftmaxBackwardDataOp,  # type: ignore
    torch.ops.aten._sparse_addmm.default: Torch_Aten_SparseAddmmOp,  # type: ignore
    torch.ops.aten._sparse_broadcast_to.default: Torch_Aten_SparseBroadcastToOp,  # type: ignore
    torch.ops.aten._sparse_broadcast_to_copy.default: Torch_Aten_SparseBroadcastToCopyOp,  # type: ignore
    torch.ops.aten._sparse_log_softmax.default: Torch_Aten_SparseLogSoftmaxOp,  # type: ignore
    torch.ops.aten._sparse_log_softmax.int: Torch_Aten_SparseLogSoftmaxIntOp,  # type: ignore
    torch.ops.aten._sparse_log_softmax_backward_data.default: Torch_Aten_SparseLogSoftmaxBackwardDataOp,  # type: ignore
    torch.ops.aten._sparse_mask_projection.default: Torch_Aten_SparseMaskProjectionOp,  # type: ignore
    torch.ops.aten._sparse_mm.default: Torch_Aten_SparseMmOp,  # type: ignore
    torch.ops.aten._sparse_semi_structured_addmm.default: Torch_Aten_SparseSemiStructuredAddmmOp,  # type: ignore
    torch.ops.aten._sparse_semi_structured_apply.default: Torch_Aten_SparseSemiStructuredApplyOp,  # type: ignore
    torch.ops.aten._sparse_semi_structured_apply_dense.default: Torch_Aten_SparseSemiStructuredApplyDenseOp,  # type: ignore
    torch.ops.aten._sparse_semi_structured_mm.default: Torch_Aten_SparseSemiStructuredMmOp,  # type: ignore
    torch.ops.aten._sparse_softmax.default: Torch_Aten_SparseSoftmaxOp,  # type: ignore
    torch.ops.aten._sparse_softmax.int: Torch_Aten_SparseSoftmaxIntOp,  # type: ignore
    torch.ops.aten._sparse_softmax_backward_data.default: Torch_Aten_SparseSoftmaxBackwardDataOp,  # type: ignore
    torch.ops.aten._sparse_sparse_matmul.default: Torch_Aten_SparseSparseMatmulOp,  # type: ignore
    torch.ops.aten._sparse_sum.default: Torch_Aten_SparseSumOp,  # type: ignore
    torch.ops.aten._sparse_sum.dim: Torch_Aten_SparseSumDimOp,  # type: ignore
    torch.ops.aten._sparse_sum.dim_dtype: Torch_Aten_SparseSumDimDtypeOp,  # type: ignore
    torch.ops.aten._sparse_sum.dim_out: Torch_Aten_SparseSumDimOutOp,  # type: ignore
    torch.ops.aten._sparse_sum.dtype: Torch_Aten_SparseSumDtypeOp,  # type: ignore
    torch.ops.aten._sparse_sum_backward.default: Torch_Aten_SparseSumBackwardOp,  # type: ignore
    torch.ops.aten._spdiags.default: Torch_Aten_SpdiagsOp,  # type: ignore
    torch.ops.aten._spsolve.default: Torch_Aten_SpsolveOp,  # type: ignore
    torch.ops.aten._stack.default: Torch_Aten_StackOp,  # type: ignore
    torch.ops.aten._standard_gamma_grad.default: Torch_Aten_StandardGammaGradOp,  # type: ignore
    torch.ops.aten._test_autograd_multiple_dispatch_view.default: Torch_Aten_TestAutogradMultipleDispatchViewOp,  # type: ignore
    torch.ops.aten._test_autograd_multiple_dispatch_view_copy.default: Torch_Aten_TestAutogradMultipleDispatchViewCopyOp,  # type: ignore
    torch.ops.aten._test_check_tensor.default: Torch_Aten_TestCheckTensorOp,  # type: ignore
    torch.ops.aten._test_functorch_fallback.default: Torch_Aten_TestFunctorchFallbackOp,  # type: ignore
    torch.ops.aten._test_optional_filled_intlist.default: Torch_Aten_TestOptionalFilledIntlistOp,  # type: ignore
    torch.ops.aten._test_optional_floatlist.default: Torch_Aten_TestOptionalFloatlistOp,  # type: ignore
    torch.ops.aten._test_optional_intlist.default: Torch_Aten_TestOptionalIntlistOp,  # type: ignore
    torch.ops.aten._test_parallel_materialize.default: Torch_Aten_TestParallelMaterializeOp,  # type: ignore
    torch.ops.aten._test_serialization_subcmul.default: Torch_Aten_TestSerializationSubcmulOp,  # type: ignore
    torch.ops.aten._test_warn_in_autograd.default: Torch_Aten_TestWarnInAutogradOp,  # type: ignore
    torch.ops.aten._thnn_differentiable_gru_cell_backward.default: Torch_Aten_ThnnDifferentiableGruCellBackwardOp,  # type: ignore
    torch.ops.aten._thnn_differentiable_lstm_cell_backward.default: Torch_Aten_ThnnDifferentiableLstmCellBackwardOp,  # type: ignore
    torch.ops.aten._thnn_fused_gru_cell.default: Torch_Aten_ThnnFusedGruCellOp,  # type: ignore
    torch.ops.aten._thnn_fused_gru_cell_backward.default: Torch_Aten_ThnnFusedGruCellBackwardOp,  # type: ignore
    torch.ops.aten._thnn_fused_lstm_cell.default: Torch_Aten_ThnnFusedLstmCellOp,  # type: ignore
    torch.ops.aten._thnn_fused_lstm_cell_backward.default: Torch_Aten_ThnnFusedLstmCellBackwardOp,  # type: ignore
    torch.ops.aten._thnn_fused_lstm_cell_backward_impl.default: Torch_Aten_ThnnFusedLstmCellBackwardImplOp,  # type: ignore
    torch.ops.aten._to_cpu.default: Torch_Aten_ToCpuOp,  # type: ignore
    torch.ops.aten._to_dense.default: Torch_Aten_ToDenseOp,  # type: ignore
    torch.ops.aten._to_sparse.default: Torch_Aten_ToSparseOp,  # type: ignore
    torch.ops.aten._to_sparse.sparse_dim: Torch_Aten_ToSparseSparseDimOp,  # type: ignore
    torch.ops.aten._to_sparse.sparse_dim_out: Torch_Aten_ToSparseSparseDimOutOp,  # type: ignore
    torch.ops.aten._to_sparse_bsc.default: Torch_Aten_ToSparseBscOp,  # type: ignore
    torch.ops.aten._to_sparse_bsr.default: Torch_Aten_ToSparseBsrOp,  # type: ignore
    torch.ops.aten._to_sparse_csc.default: Torch_Aten_ToSparseCscOp,  # type: ignore
    torch.ops.aten._to_sparse_csr.default: Torch_Aten_ToSparseCsrOp,  # type: ignore
    torch.ops.aten._to_sparse_semi_structured.default: Torch_Aten_ToSparseSemiStructuredOp,  # type: ignore
    torch.ops.aten._transform_bias_rescale_qkv.default: Torch_Aten_TransformBiasRescaleQkvOp,  # type: ignore
    torch.ops.aten._transformer_encoder_layer_fwd.default: Torch_Aten_TransformerEncoderLayerFwdOp,  # type: ignore
    torch.ops.aten._trilinear.default: Torch_Aten_TrilinearOp,  # type: ignore
    torch.ops.aten._triton_multi_head_attention.default: Torch_Aten_TritonMultiHeadAttentionOp,  # type: ignore
    torch.ops.aten._triton_scaled_dot_attention.default: Torch_Aten_TritonScaledDotAttentionOp,  # type: ignore
    torch.ops.aten._unique.default: Torch_Aten_UniqueOp,  # type: ignore
    torch.ops.aten._unique2.default: Torch_Aten_Unique2Op,  # type: ignore
    torch.ops.aten._unpack_dual.default: Torch_Aten_UnpackDualOp,  # type: ignore
    torch.ops.aten._unsafe_index.Tensor_hacked_twin: Torch_Aten_UnsafeIndexTensorHackedTwinOp,  # type: ignore
    torch.ops.aten._unsafe_index_put.hacked_twin: Torch_Aten_UnsafeIndexPutHackedTwinOp,  # type: ignore
    torch.ops.aten._unsafe_view.default: Torch_Aten_UnsafeViewOp,  # type: ignore
    torch.ops.aten._upsample_bicubic2d_aa.default: Torch_Aten_UpsampleBicubic2DAaOp,  # type: ignore
    torch.ops.aten._upsample_bicubic2d_aa.vec: Torch_Aten_UpsampleBicubic2DAaVecOp,  # type: ignore
    torch.ops.aten._upsample_bicubic2d_aa_backward.default: Torch_Aten_UpsampleBicubic2DAaBackwardOp,  # type: ignore
    torch.ops.aten._upsample_bicubic2d_aa_backward.grad_input: Torch_Aten_UpsampleBicubic2DAaBackwardGradInputOp,  # type: ignore
    torch.ops.aten._upsample_bilinear2d_aa.default: Torch_Aten_UpsampleBilinear2DAaOp,  # type: ignore
    torch.ops.aten._upsample_bilinear2d_aa.vec: Torch_Aten_UpsampleBilinear2DAaVecOp,  # type: ignore
    torch.ops.aten._upsample_bilinear2d_aa_backward.default: Torch_Aten_UpsampleBilinear2DAaBackwardOp,  # type: ignore
    torch.ops.aten._upsample_bilinear2d_aa_backward.grad_input: Torch_Aten_UpsampleBilinear2DAaBackwardGradInputOp,  # type: ignore
    torch.ops.aten._upsample_nearest_exact1d.default: Torch_Aten_UpsampleNearestExact1DOp,  # type: ignore
    torch.ops.aten._upsample_nearest_exact1d.vec: Torch_Aten_UpsampleNearestExact1DVecOp,  # type: ignore
    torch.ops.aten._upsample_nearest_exact1d_backward.default: Torch_Aten_UpsampleNearestExact1DBackwardOp,  # type: ignore
    torch.ops.aten._upsample_nearest_exact1d_backward.grad_input: Torch_Aten_UpsampleNearestExact1DBackwardGradInputOp,  # type: ignore
    torch.ops.aten._upsample_nearest_exact2d.default: Torch_Aten_UpsampleNearestExact2DOp,  # type: ignore
    torch.ops.aten._upsample_nearest_exact2d.vec: Torch_Aten_UpsampleNearestExact2DVecOp,  # type: ignore
    torch.ops.aten._upsample_nearest_exact2d_backward.default: Torch_Aten_UpsampleNearestExact2DBackwardOp,  # type: ignore
    torch.ops.aten._upsample_nearest_exact2d_backward.grad_input: Torch_Aten_UpsampleNearestExact2DBackwardGradInputOp,  # type: ignore
    torch.ops.aten._upsample_nearest_exact3d.default: Torch_Aten_UpsampleNearestExact3DOp,  # type: ignore
    torch.ops.aten._upsample_nearest_exact3d.vec: Torch_Aten_UpsampleNearestExact3DVecOp,  # type: ignore
    torch.ops.aten._upsample_nearest_exact3d_backward.default: Torch_Aten_UpsampleNearestExact3DBackwardOp,  # type: ignore
    torch.ops.aten._upsample_nearest_exact3d_backward.grad_input: Torch_Aten_UpsampleNearestExact3DBackwardGradInputOp,  # type: ignore
    torch.ops.aten._use_cudnn_ctc_loss.Tensor: Torch_Aten_UseCudnnCtcLossTensorOp,  # type: ignore
    torch.ops.aten._use_cudnn_ctc_loss.default: Torch_Aten_UseCudnnCtcLossOp,  # type: ignore
    torch.ops.aten._use_cudnn_rnn_flatten_weight.default: Torch_Aten_UseCudnnRnnFlattenWeightOp,  # type: ignore
    torch.ops.aten._validate_compressed_sparse_indices.default: Torch_Aten_ValidateCompressedSparseIndicesOp,  # type: ignore
    torch.ops.aten._validate_sparse_bsc_tensor_args.default: Torch_Aten_ValidateSparseBscTensorArgsOp,  # type: ignore
    torch.ops.aten._validate_sparse_bsr_tensor_args.default: Torch_Aten_ValidateSparseBsrTensorArgsOp,  # type: ignore
    torch.ops.aten._validate_sparse_compressed_tensor_args.default: Torch_Aten_ValidateSparseCompressedTensorArgsOp,  # type: ignore
    torch.ops.aten._validate_sparse_coo_tensor_args.default: Torch_Aten_ValidateSparseCooTensorArgsOp,  # type: ignore
    torch.ops.aten._validate_sparse_csc_tensor_args.default: Torch_Aten_ValidateSparseCscTensorArgsOp,  # type: ignore
    torch.ops.aten._validate_sparse_csr_tensor_args.default: Torch_Aten_ValidateSparseCsrTensorArgsOp,  # type: ignore
    torch.ops.aten._values.default: Torch_Aten_ValuesOp,  # type: ignore
    torch.ops.aten._values_copy.default: Torch_Aten_ValuesCopyOp,  # type: ignore
    torch.ops.aten._version.default: Torch_Aten_VersionOp,  # type: ignore
    torch.ops.aten._weight_int4pack_mm.default: Torch_Aten_WeightInt4PackMmOp,  # type: ignore
    torch.ops.aten._weight_int4pack_mm_for_cpu.default: Torch_Aten_WeightInt4PackMmForCpuOp,  # type: ignore
    torch.ops.aten._weight_int8pack_mm.default: Torch_Aten_WeightInt8PackMmOp,  # type: ignore
    torch.ops.aten._weight_norm.default: Torch_Aten_WeightNormOp,  # type: ignore
    torch.ops.aten._weight_norm_differentiable_backward.default: Torch_Aten_WeightNormDifferentiableBackwardOp,  # type: ignore
    torch.ops.aten._weight_norm_interface.default: Torch_Aten_WeightNormInterfaceOp,  # type: ignore
    torch.ops.aten._weight_norm_interface_backward.default: Torch_Aten_WeightNormInterfaceBackwardOp,  # type: ignore
    torch.ops.aten._wrapped_linear_prepack.default: Torch_Aten_WrappedLinearPrepackOp,  # type: ignore
    torch.ops.aten._wrapped_quantized_linear_prepacked.default: Torch_Aten_WrappedQuantizedLinearPrepackedOp,  # type: ignore
    torch.ops.aten.abs.default: Torch_AtenAbsOp,  # type: ignore
    torch.ops.aten.abs_.default: Torch_AtenAbs_Op,  # type: ignore
    torch.ops.aten.absolute.default: Torch_AtenAbsoluteOp,  # type: ignore
    torch.ops.aten.absolute_.default: Torch_AtenAbsolute_Op,  # type: ignore
    torch.ops.aten.acos.Scalar: Torch_AtenAcosScalarOp,  # type: ignore
    torch.ops.aten.acos.default: Torch_AtenAcosOp,  # type: ignore
    torch.ops.aten.acos.float: Torch_AtenAcosFloatOp,  # type: ignore
    torch.ops.aten.acos.int: Torch_AtenAcosIntOp,  # type: ignore
    torch.ops.aten.acos_.default: Torch_AtenAcos_Op,  # type: ignore
    torch.ops.aten.acosh.Scalar: Torch_AtenAcoshScalarOp,  # type: ignore
    torch.ops.aten.acosh.default: Torch_AtenAcoshOp,  # type: ignore
    torch.ops.aten.acosh.float: Torch_AtenAcoshFloatOp,  # type: ignore
    torch.ops.aten.acosh.int: Torch_AtenAcoshIntOp,  # type: ignore
    torch.ops.aten.acosh_.default: Torch_AtenAcosh_Op,  # type: ignore
    torch.ops.aten.adaptive_avg_pool1d.default: Torch_AtenAdaptiveAvgPool1DOp,  # type: ignore
    torch.ops.aten.adaptive_avg_pool2d.default: Torch_AtenAdaptiveAvgPool2DOp,  # type: ignore
    torch.ops.aten.adaptive_avg_pool3d.default: Torch_AtenAdaptiveAvgPool3DOp,  # type: ignore
    torch.ops.aten.adaptive_max_pool1d.default: Torch_AtenAdaptiveMaxPool1DOp,  # type: ignore
    torch.ops.aten.adaptive_max_pool2d.default: Torch_AtenAdaptiveMaxPool2DOp,  # type: ignore
    torch.ops.aten.adaptive_max_pool2d_backward.default: Torch_AtenAdaptiveMaxPool2DBackwardOp,  # type: ignore
    torch.ops.aten.adaptive_max_pool2d_backward.grad_input: Torch_AtenAdaptiveMaxPool2DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.adaptive_max_pool3d.default: Torch_AtenAdaptiveMaxPool3DOp,  # type: ignore
    torch.ops.aten.adaptive_max_pool3d_backward.default: Torch_AtenAdaptiveMaxPool3DBackwardOp,  # type: ignore
    torch.ops.aten.adaptive_max_pool3d_backward.grad_input: Torch_AtenAdaptiveMaxPool3DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.add.Scalar: Torch_AtenAddScalarOp,  # type: ignore
    torch.ops.aten.add.Scalar_out: Torch_AtenAddScalarOutOp,  # type: ignore
    torch.ops.aten.add.Tensor: Torch_AtenAddTensorOp,  # type: ignore
    torch.ops.aten.add.default: Torch_AtenAddOp,  # type: ignore
    torch.ops.aten.add.float: Torch_AtenAddFloatOp,  # type: ignore
    torch.ops.aten.add.float_int: Torch_AtenAddFloatIntOp,  # type: ignore
    torch.ops.aten.add.int: Torch_AtenAddIntOp,  # type: ignore
    torch.ops.aten.add.int_float: Torch_AtenAddIntFloatOp,  # type: ignore
    torch.ops.aten.add_.Scalar: Torch_AtenAdd_ScalarOp,  # type: ignore
    torch.ops.aten.add_.Tensor: Torch_AtenAdd_TensorOp,  # type: ignore
    torch.ops.aten.addbmm.default: Torch_AtenAddbmmOp,  # type: ignore
    torch.ops.aten.addbmm_.default: Torch_AtenAddbmm_Op,  # type: ignore
    torch.ops.aten.addcdiv.default: Torch_AtenAddcdivOp,  # type: ignore
    torch.ops.aten.addcdiv_.default: Torch_AtenAddcdiv_Op,  # type: ignore
    torch.ops.aten.addcmul.default: Torch_AtenAddcmulOp,  # type: ignore
    torch.ops.aten.addcmul_.default: Torch_AtenAddcmul_Op,  # type: ignore
    torch.ops.aten.addmm.default: Torch_AtenAddmmOp,  # type: ignore
    torch.ops.aten.addmm_.default: Torch_AtenAddmm_Op,  # type: ignore
    torch.ops.aten.addmv.default: Torch_AtenAddmvOp,  # type: ignore
    torch.ops.aten.addmv_.default: Torch_AtenAddmv_Op,  # type: ignore
    torch.ops.aten.addr.default: Torch_AtenAddrOp,  # type: ignore
    torch.ops.aten.addr_.default: Torch_AtenAddr_Op,  # type: ignore
    torch.ops.aten.adjoint.default: Torch_AtenAdjointOp,  # type: ignore
    torch.ops.aten.affine_grid_generator.default: Torch_AtenAffineGridGeneratorOp,  # type: ignore
    torch.ops.aten.affine_grid_generator_backward.default: Torch_AtenAffineGridGeneratorBackwardOp,  # type: ignore
    torch.ops.aten.alias.default: Torch_AtenAliasOp,  # type: ignore
    torch.ops.aten.alias_copy.default: Torch_AtenAliasCopyOp,  # type: ignore
    torch.ops.aten.align_as.default: Torch_AtenAlignAsOp,  # type: ignore
    torch.ops.aten.align_tensors.default: Torch_AtenAlignTensorsOp,  # type: ignore
    torch.ops.aten.all.all_out: Torch_AtenAllAllOutOp,  # type: ignore
    torch.ops.aten.all.bool: Torch_AtenAllBoolOp,  # type: ignore
    torch.ops.aten.all.default: Torch_AtenAllOp,  # type: ignore
    torch.ops.aten.all.dim: Torch_AtenAllDimOp,  # type: ignore
    torch.ops.aten.all.dims: Torch_AtenAllDimsOp,  # type: ignore
    torch.ops.aten.all.dims_out: Torch_AtenAllDimsOutOp,  # type: ignore
    torch.ops.aten.all.float: Torch_AtenAllFloatOp,  # type: ignore
    torch.ops.aten.all.int: Torch_AtenAllIntOp,  # type: ignore
    torch.ops.aten.allclose.default: Torch_AtenAllcloseOp,  # type: ignore
    torch.ops.aten.alpha_dropout.default: Torch_AtenAlphaDropoutOp,  # type: ignore
    torch.ops.aten.alpha_dropout_.default: Torch_AtenAlphaDropout_Op,  # type: ignore
    torch.ops.aten.amax.default: Torch_AtenAmaxOp,  # type: ignore
    torch.ops.aten.amin.default: Torch_AtenAminOp,  # type: ignore
    torch.ops.aten.aminmax.default: Torch_AtenAminmaxOp,  # type: ignore
    torch.ops.aten.angle.Scalar: Torch_AtenAngleScalarOp,  # type: ignore
    torch.ops.aten.angle.default: Torch_AtenAngleOp,  # type: ignore
    torch.ops.aten.angle.float: Torch_AtenAngleFloatOp,  # type: ignore
    torch.ops.aten.angle.int: Torch_AtenAngleIntOp,  # type: ignore
    torch.ops.aten.any.all_out: Torch_AtenAnyAllOutOp,  # type: ignore
    torch.ops.aten.any.bool: Torch_AtenAnyBoolOp,  # type: ignore
    torch.ops.aten.any.default: Torch_AtenAnyOp,  # type: ignore
    torch.ops.aten.any.dim: Torch_AtenAnyDimOp,  # type: ignore
    torch.ops.aten.any.dims: Torch_AtenAnyDimsOp,  # type: ignore
    torch.ops.aten.any.dims_out: Torch_AtenAnyDimsOutOp,  # type: ignore
    torch.ops.aten.any.float: Torch_AtenAnyFloatOp,  # type: ignore
    torch.ops.aten.any.int: Torch_AtenAnyIntOp,  # type: ignore
    torch.ops.aten.arange.start_out: Torch_AtenArangeStartOutOp,  # type: ignore
    torch.ops.aten.arccos.default: Torch_AtenArccosOp,  # type: ignore
    torch.ops.aten.arccos_.default: Torch_AtenArccos_Op,  # type: ignore
    torch.ops.aten.arccosh.default: Torch_AtenArccoshOp,  # type: ignore
    torch.ops.aten.arccosh_.default: Torch_AtenArccosh_Op,  # type: ignore
    torch.ops.aten.arcsin.default: Torch_AtenArcsinOp,  # type: ignore
    torch.ops.aten.arcsin_.default: Torch_AtenArcsin_Op,  # type: ignore
    torch.ops.aten.arcsinh.default: Torch_AtenArcsinhOp,  # type: ignore
    torch.ops.aten.arcsinh_.default: Torch_AtenArcsinh_Op,  # type: ignore
    torch.ops.aten.arctan.default: Torch_AtenArctanOp,  # type: ignore
    torch.ops.aten.arctan2.default: Torch_AtenArctan2Op,  # type: ignore
    torch.ops.aten.arctan2_.default: Torch_AtenArctan2_Op,  # type: ignore
    torch.ops.aten.arctan_.default: Torch_AtenArctan_Op,  # type: ignore
    torch.ops.aten.arctanh.default: Torch_AtenArctanhOp,  # type: ignore
    torch.ops.aten.arctanh_.default: Torch_AtenArctanh_Op,  # type: ignore
    torch.ops.aten.argmax.default: Torch_AtenArgmaxOp,  # type: ignore
    torch.ops.aten.argmin.default: Torch_AtenArgminOp,  # type: ignore
    torch.ops.aten.argsort.default: Torch_AtenArgsortOp,  # type: ignore
    torch.ops.aten.argsort.stable: Torch_AtenArgsortStableOp,  # type: ignore
    torch.ops.aten.argsort.stable_out: Torch_AtenArgsortStableOutOp,  # type: ignore
    torch.ops.aten.argwhere.default: Torch_AtenArgwhereOp,  # type: ignore
    torch.ops.aten.as_strided.default: Torch_AtenAsStridedOp,  # type: ignore
    torch.ops.aten.as_strided_.default: Torch_AtenAsStrided_Op,  # type: ignore
    torch.ops.aten.as_strided_copy.default: Torch_AtenAsStridedCopyOp,  # type: ignore
    torch.ops.aten.as_strided_scatter.default: Torch_AtenAsStridedScatterOp,  # type: ignore
    torch.ops.aten.asin.Scalar: Torch_AtenAsinScalarOp,  # type: ignore
    torch.ops.aten.asin.default: Torch_AtenAsinOp,  # type: ignore
    torch.ops.aten.asin.float: Torch_AtenAsinFloatOp,  # type: ignore
    torch.ops.aten.asin.int: Torch_AtenAsinIntOp,  # type: ignore
    torch.ops.aten.asin_.default: Torch_AtenAsin_Op,  # type: ignore
    torch.ops.aten.asinh.Scalar: Torch_AtenAsinhScalarOp,  # type: ignore
    torch.ops.aten.asinh.default: Torch_AtenAsinhOp,  # type: ignore
    torch.ops.aten.asinh.float: Torch_AtenAsinhFloatOp,  # type: ignore
    torch.ops.aten.asinh.int: Torch_AtenAsinhIntOp,  # type: ignore
    torch.ops.aten.asinh_.default: Torch_AtenAsinh_Op,  # type: ignore
    torch.ops.aten.atan.Scalar: Torch_AtenAtanScalarOp,  # type: ignore
    torch.ops.aten.atan.default: Torch_AtenAtanOp,  # type: ignore
    torch.ops.aten.atan.float: Torch_AtenAtanFloatOp,  # type: ignore
    torch.ops.aten.atan.int: Torch_AtenAtanIntOp,  # type: ignore
    torch.ops.aten.atan2.Scalar_Scalar: Torch_AtenAtan2ScalarScalarOp,  # type: ignore
    torch.ops.aten.atan2.default: Torch_AtenAtan2Op,  # type: ignore
    torch.ops.aten.atan2.float: Torch_AtenAtan2FloatOp,  # type: ignore
    torch.ops.aten.atan2.float_int: Torch_AtenAtan2FloatIntOp,  # type: ignore
    torch.ops.aten.atan2.int: Torch_AtenAtan2IntOp,  # type: ignore
    torch.ops.aten.atan2.int_float: Torch_AtenAtan2IntFloatOp,  # type: ignore
    torch.ops.aten.atan2_.default: Torch_AtenAtan2_Op,  # type: ignore
    torch.ops.aten.atan_.default: Torch_AtenAtan_Op,  # type: ignore
    torch.ops.aten.atanh.Scalar: Torch_AtenAtanhScalarOp,  # type: ignore
    torch.ops.aten.atanh.default: Torch_AtenAtanhOp,  # type: ignore
    torch.ops.aten.atanh.float: Torch_AtenAtanhFloatOp,  # type: ignore
    torch.ops.aten.atanh.int: Torch_AtenAtanhIntOp,  # type: ignore
    torch.ops.aten.atanh_.default: Torch_AtenAtanh_Op,  # type: ignore
    torch.ops.aten.atleast_1d.Sequence: Torch_AtenAtleast1DSequenceOp,  # type: ignore
    torch.ops.aten.atleast_1d.default: Torch_AtenAtleast1DOp,  # type: ignore
    torch.ops.aten.atleast_2d.Sequence: Torch_AtenAtleast2DSequenceOp,  # type: ignore
    torch.ops.aten.atleast_2d.default: Torch_AtenAtleast2DOp,  # type: ignore
    torch.ops.aten.atleast_3d.Sequence: Torch_AtenAtleast3DSequenceOp,  # type: ignore
    torch.ops.aten.atleast_3d.default: Torch_AtenAtleast3DOp,  # type: ignore
    torch.ops.aten.avg_pool1d.default: Torch_AtenAvgPool1DOp,  # type: ignore
    torch.ops.aten.avg_pool2d.default: Torch_AtenAvgPool2DOp,  # type: ignore
    torch.ops.aten.avg_pool2d_backward.default: Torch_AtenAvgPool2DBackwardOp,  # type: ignore
    torch.ops.aten.avg_pool2d_backward.grad_input: Torch_AtenAvgPool2DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.avg_pool3d.default: Torch_AtenAvgPool3DOp,  # type: ignore
    torch.ops.aten.avg_pool3d_backward.default: Torch_AtenAvgPool3DBackwardOp,  # type: ignore
    torch.ops.aten.avg_pool3d_backward.grad_input: Torch_AtenAvgPool3DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.baddbmm.default: Torch_AtenBaddbmmOp,  # type: ignore
    torch.ops.aten.baddbmm_.default: Torch_AtenBaddbmm_Op,  # type: ignore
    torch.ops.aten.bartlett_window.periodic_out: Torch_AtenBartlettWindowPeriodicOutOp,  # type: ignore
    torch.ops.aten.batch_norm.default: Torch_AtenBatchNormOp,  # type: ignore
    torch.ops.aten.batch_norm_backward.default: Torch_AtenBatchNormBackwardOp,  # type: ignore
    torch.ops.aten.batch_norm_backward_elemt.default: Torch_AtenBatchNormBackwardElemtOp,  # type: ignore
    torch.ops.aten.batch_norm_backward_reduce.default: Torch_AtenBatchNormBackwardReduceOp,  # type: ignore
    torch.ops.aten.batch_norm_elemt.default: Torch_AtenBatchNormElemtOp,  # type: ignore
    torch.ops.aten.batch_norm_gather_stats.default: Torch_AtenBatchNormGatherStatsOp,  # type: ignore
    torch.ops.aten.batch_norm_gather_stats_with_counts.default: Torch_AtenBatchNormGatherStatsWithCountsOp,  # type: ignore
    torch.ops.aten.batch_norm_stats.default: Torch_AtenBatchNormStatsOp,  # type: ignore
    torch.ops.aten.batch_norm_update_stats.default: Torch_AtenBatchNormUpdateStatsOp,  # type: ignore
    torch.ops.aten.bilinear.default: Torch_AtenBilinearOp,  # type: ignore
    torch.ops.aten.binary_cross_entropy.default: Torch_AtenBinaryCrossEntropyOp,  # type: ignore
    torch.ops.aten.binary_cross_entropy_backward.default: Torch_AtenBinaryCrossEntropyBackwardOp,  # type: ignore
    torch.ops.aten.binary_cross_entropy_backward.grad_input: Torch_AtenBinaryCrossEntropyBackwardGradInputOp,  # type: ignore
    torch.ops.aten.binary_cross_entropy_with_logits.default: Torch_AtenBinaryCrossEntropyWithLogitsOp,  # type: ignore
    torch.ops.aten.bincount.default: Torch_AtenBincountOp,  # type: ignore
    torch.ops.aten.bitwise_and.Scalar: Torch_AtenBitwiseAndScalarOp,  # type: ignore
    torch.ops.aten.bitwise_and.Scalar_Tensor: Torch_AtenBitwiseAndScalarTensorOp,  # type: ignore
    torch.ops.aten.bitwise_and.Scalar_Tensor_out: Torch_AtenBitwiseAndScalarTensorOutOp,  # type: ignore
    torch.ops.aten.bitwise_and.Scalar_out: Torch_AtenBitwiseAndScalarOutOp,  # type: ignore
    torch.ops.aten.bitwise_and.Tensor: Torch_AtenBitwiseAndTensorOp,  # type: ignore
    torch.ops.aten.bitwise_and.Tensor_out: Torch_AtenBitwiseAndTensorOutOp,  # type: ignore
    torch.ops.aten.bitwise_and_.Scalar: Torch_AtenBitwiseAnd_ScalarOp,  # type: ignore
    torch.ops.aten.bitwise_and_.Tensor: Torch_AtenBitwiseAnd_TensorOp,  # type: ignore
    torch.ops.aten.bitwise_left_shift.Scalar_Tensor: Torch_AtenBitwiseLeftShiftScalarTensorOp,  # type: ignore
    torch.ops.aten.bitwise_left_shift.Scalar_Tensor_out: Torch_AtenBitwiseLeftShiftScalarTensorOutOp,  # type: ignore
    torch.ops.aten.bitwise_left_shift.Tensor: Torch_AtenBitwiseLeftShiftTensorOp,  # type: ignore
    torch.ops.aten.bitwise_left_shift.Tensor_Scalar: Torch_AtenBitwiseLeftShiftTensorScalarOp,  # type: ignore
    torch.ops.aten.bitwise_left_shift.Tensor_Scalar_out: Torch_AtenBitwiseLeftShiftTensorScalarOutOp,  # type: ignore
    torch.ops.aten.bitwise_left_shift.Tensor_out: Torch_AtenBitwiseLeftShiftTensorOutOp,  # type: ignore
    torch.ops.aten.bitwise_left_shift_.Tensor: Torch_AtenBitwiseLeftShift_TensorOp,  # type: ignore
    torch.ops.aten.bitwise_left_shift_.Tensor_Scalar: Torch_AtenBitwiseLeftShift_TensorScalarOp,  # type: ignore
    torch.ops.aten.bitwise_not.default: Torch_AtenBitwiseNotOp,  # type: ignore
    torch.ops.aten.bitwise_not_.default: Torch_AtenBitwiseNot_Op,  # type: ignore
    torch.ops.aten.bitwise_or.Scalar: Torch_AtenBitwiseOrScalarOp,  # type: ignore
    torch.ops.aten.bitwise_or.Scalar_Tensor: Torch_AtenBitwiseOrScalarTensorOp,  # type: ignore
    torch.ops.aten.bitwise_or.Scalar_Tensor_out: Torch_AtenBitwiseOrScalarTensorOutOp,  # type: ignore
    torch.ops.aten.bitwise_or.Scalar_out: Torch_AtenBitwiseOrScalarOutOp,  # type: ignore
    torch.ops.aten.bitwise_or.Tensor: Torch_AtenBitwiseOrTensorOp,  # type: ignore
    torch.ops.aten.bitwise_or.Tensor_out: Torch_AtenBitwiseOrTensorOutOp,  # type: ignore
    torch.ops.aten.bitwise_or_.Scalar: Torch_AtenBitwiseOr_ScalarOp,  # type: ignore
    torch.ops.aten.bitwise_or_.Tensor: Torch_AtenBitwiseOr_TensorOp,  # type: ignore
    torch.ops.aten.bitwise_right_shift.Scalar_Tensor: Torch_AtenBitwiseRightShiftScalarTensorOp,  # type: ignore
    torch.ops.aten.bitwise_right_shift.Scalar_Tensor_out: Torch_AtenBitwiseRightShiftScalarTensorOutOp,  # type: ignore
    torch.ops.aten.bitwise_right_shift.Tensor: Torch_AtenBitwiseRightShiftTensorOp,  # type: ignore
    torch.ops.aten.bitwise_right_shift.Tensor_Scalar: Torch_AtenBitwiseRightShiftTensorScalarOp,  # type: ignore
    torch.ops.aten.bitwise_right_shift.Tensor_Scalar_out: Torch_AtenBitwiseRightShiftTensorScalarOutOp,  # type: ignore
    torch.ops.aten.bitwise_right_shift.Tensor_out: Torch_AtenBitwiseRightShiftTensorOutOp,  # type: ignore
    torch.ops.aten.bitwise_right_shift_.Tensor: Torch_AtenBitwiseRightShift_TensorOp,  # type: ignore
    torch.ops.aten.bitwise_right_shift_.Tensor_Scalar: Torch_AtenBitwiseRightShift_TensorScalarOp,  # type: ignore
    torch.ops.aten.bitwise_xor.Scalar: Torch_AtenBitwiseXorScalarOp,  # type: ignore
    torch.ops.aten.bitwise_xor.Scalar_Tensor: Torch_AtenBitwiseXorScalarTensorOp,  # type: ignore
    torch.ops.aten.bitwise_xor.Scalar_Tensor_out: Torch_AtenBitwiseXorScalarTensorOutOp,  # type: ignore
    torch.ops.aten.bitwise_xor.Scalar_out: Torch_AtenBitwiseXorScalarOutOp,  # type: ignore
    torch.ops.aten.bitwise_xor.Tensor: Torch_AtenBitwiseXorTensorOp,  # type: ignore
    torch.ops.aten.bitwise_xor.Tensor_out: Torch_AtenBitwiseXorTensorOutOp,  # type: ignore
    torch.ops.aten.bitwise_xor_.Scalar: Torch_AtenBitwiseXor_ScalarOp,  # type: ignore
    torch.ops.aten.bitwise_xor_.Tensor: Torch_AtenBitwiseXor_TensorOp,  # type: ignore
    torch.ops.aten.blackman_window.periodic_out: Torch_AtenBlackmanWindowPeriodicOutOp,  # type: ignore
    torch.ops.aten.block_diag.default: Torch_AtenBlockDiagOp,  # type: ignore
    torch.ops.aten.bmm.default: Torch_AtenBmmOp,  # type: ignore
    torch.ops.aten.broadcast_tensors.default: Torch_AtenBroadcastTensorsOp,  # type: ignore
    torch.ops.aten.broadcast_to.default: Torch_AtenBroadcastToOp,  # type: ignore
    torch.ops.aten.bucketize.Scalar: Torch_AtenBucketizeScalarOp,  # type: ignore
    torch.ops.aten.bucketize.Scalar_out: Torch_AtenBucketizeScalarOutOp,  # type: ignore
    torch.ops.aten.bucketize.Tensor: Torch_AtenBucketizeTensorOp,  # type: ignore
    torch.ops.aten.bucketize.Tensor_out: Torch_AtenBucketizeTensorOutOp,  # type: ignore
    torch.ops.aten.can_cast.default: Torch_AtenCanCastOp,  # type: ignore
    torch.ops.aten.cartesian_prod.default: Torch_AtenCartesianProdOp,  # type: ignore
    torch.ops.aten.cat.default: Torch_AtenCatOp,  # type: ignore
    torch.ops.aten.ccol_indices.default: Torch_AtenCcolIndicesOp,  # type: ignore
    torch.ops.aten.ccol_indices_copy.default: Torch_AtenCcolIndicesCopyOp,  # type: ignore
    torch.ops.aten.cdist.default: Torch_AtenCdistOp,  # type: ignore
    torch.ops.aten.ceil.Scalar: Torch_AtenCeilScalarOp,  # type: ignore
    torch.ops.aten.ceil.default: Torch_AtenCeilOp,  # type: ignore
    torch.ops.aten.ceil.float: Torch_AtenCeilFloatOp,  # type: ignore
    torch.ops.aten.ceil.int: Torch_AtenCeilIntOp,  # type: ignore
    torch.ops.aten.ceil_.default: Torch_AtenCeil_Op,  # type: ignore
    torch.ops.aten.celu.default: Torch_AtenCeluOp,  # type: ignore
    torch.ops.aten.celu_.default: Torch_AtenCelu_Op,  # type: ignore
    torch.ops.aten.chain_matmul.default: Torch_AtenChainMatmulOp,  # type: ignore
    torch.ops.aten.chalf.default: Torch_AtenChalfOp,  # type: ignore
    torch.ops.aten.channel_shuffle.default: Torch_AtenChannelShuffleOp,  # type: ignore
    torch.ops.aten.cholesky.default: Torch_AtenCholeskyOp,  # type: ignore
    torch.ops.aten.cholesky_inverse.default: Torch_AtenCholeskyInverseOp,  # type: ignore
    torch.ops.aten.cholesky_solve.default: Torch_AtenCholeskySolveOp,  # type: ignore
    torch.ops.aten.choose_qparams_optimized.default: Torch_AtenChooseQparamsOptimizedOp,  # type: ignore
    torch.ops.aten.chunk.default: Torch_AtenChunkOp,  # type: ignore
    torch.ops.aten.clamp.Tensor: Torch_AtenClampTensorOp,  # type: ignore
    torch.ops.aten.clamp.Tensor_out: Torch_AtenClampTensorOutOp,  # type: ignore
    torch.ops.aten.clamp.default: Torch_AtenClampOp,  # type: ignore
    torch.ops.aten.clamp_.Tensor: Torch_AtenClamp_TensorOp,  # type: ignore
    torch.ops.aten.clamp_.default: Torch_AtenClamp_Op,  # type: ignore
    torch.ops.aten.clamp_max.Tensor: Torch_AtenClampMaxTensorOp,  # type: ignore
    torch.ops.aten.clamp_max.Tensor_out: Torch_AtenClampMaxTensorOutOp,  # type: ignore
    torch.ops.aten.clamp_max.default: Torch_AtenClampMaxOp,  # type: ignore
    torch.ops.aten.clamp_max_.Tensor: Torch_AtenClampMax_TensorOp,  # type: ignore
    torch.ops.aten.clamp_max_.default: Torch_AtenClampMax_Op,  # type: ignore
    torch.ops.aten.clamp_min.Tensor: Torch_AtenClampMinTensorOp,  # type: ignore
    torch.ops.aten.clamp_min.Tensor_out: Torch_AtenClampMinTensorOutOp,  # type: ignore
    torch.ops.aten.clamp_min.default: Torch_AtenClampMinOp,  # type: ignore
    torch.ops.aten.clamp_min_.Tensor: Torch_AtenClampMin_TensorOp,  # type: ignore
    torch.ops.aten.clamp_min_.default: Torch_AtenClampMin_Op,  # type: ignore
    torch.ops.aten.clip.Tensor: Torch_AtenClipTensorOp,  # type: ignore
    torch.ops.aten.clip.Tensor_out: Torch_AtenClipTensorOutOp,  # type: ignore
    torch.ops.aten.clip.default: Torch_AtenClipOp,  # type: ignore
    torch.ops.aten.clip_.Tensor: Torch_AtenClip_TensorOp,  # type: ignore
    torch.ops.aten.clip_.default: Torch_AtenClip_Op,  # type: ignore
    torch.ops.aten.clone.default: Torch_AtenCloneOp,  # type: ignore
    torch.ops.aten.coalesce.default: Torch_AtenCoalesceOp,  # type: ignore
    torch.ops.aten.col2im.default: Torch_AtenCol2ImOp,  # type: ignore
    torch.ops.aten.col_indices.default: Torch_AtenColIndicesOp,  # type: ignore
    torch.ops.aten.col_indices_copy.default: Torch_AtenColIndicesCopyOp,  # type: ignore
    torch.ops.aten.column_stack.default: Torch_AtenColumnStackOp,  # type: ignore
    torch.ops.aten.combinations.default: Torch_AtenCombinationsOp,  # type: ignore
    torch.ops.aten.complex.default: Torch_AtenComplexOp,  # type: ignore
    torch.ops.aten.concat.default: Torch_AtenConcatOp,  # type: ignore
    torch.ops.aten.concatenate.default: Torch_AtenConcatenateOp,  # type: ignore
    torch.ops.aten.conj.default: Torch_AtenConjOp,  # type: ignore
    torch.ops.aten.conj_physical.default: Torch_AtenConjPhysicalOp,  # type: ignore
    torch.ops.aten.conj_physical_.default: Torch_AtenConjPhysical_Op,  # type: ignore
    torch.ops.aten.constant_pad_nd.default: Torch_AtenConstantPadNdOp,  # type: ignore
    torch.ops.aten.contiguous.default: Torch_AtenContiguousOp,  # type: ignore
    torch.ops.aten.conv1d.default: Torch_AtenConv1DOp,  # type: ignore
    torch.ops.aten.conv2d.default: Torch_AtenConv2DOp,  # type: ignore
    torch.ops.aten.conv3d.default: Torch_AtenConv3DOp,  # type: ignore
    torch.ops.aten.conv_depthwise3d.default: Torch_AtenConvDepthwise3DOp,  # type: ignore
    torch.ops.aten.conv_tbc.default: Torch_AtenConvTbcOp,  # type: ignore
    torch.ops.aten.conv_tbc_backward.default: Torch_AtenConvTbcBackwardOp,  # type: ignore
    torch.ops.aten.conv_transpose1d.default: Torch_AtenConvTranspose1DOp,  # type: ignore
    torch.ops.aten.convolution.default: Torch_AtenConvolutionOp,  # type: ignore
    torch.ops.aten.convolution_backward.default: Torch_AtenConvolutionBackwardOp,  # type: ignore
    torch.ops.aten.convolution_backward_overrideable.default: Torch_AtenConvolutionBackwardOverrideableOp,  # type: ignore
    torch.ops.aten.convolution_overrideable.default: Torch_AtenConvolutionOverrideableOp,  # type: ignore
    torch.ops.aten.copy.default: Torch_AtenCopyOp,  # type: ignore
    torch.ops.aten.copy_.Tensor: Torch_AtenCopy_TensorOp,  # type: ignore
    torch.ops.aten.copy_.default: Torch_AtenCopy_Op,  # type: ignore
    torch.ops.aten.copy_.float: Torch_AtenCopy_FloatOp,  # type: ignore
    torch.ops.aten.copy_.int: Torch_AtenCopy_IntOp,  # type: ignore
    torch.ops.aten.copy_sparse_to_sparse_.default: Torch_AtenCopySparseToSparse_Op,  # type: ignore
    torch.ops.aten.copysign.Scalar: Torch_AtenCopysignScalarOp,  # type: ignore
    torch.ops.aten.copysign.Scalar_out: Torch_AtenCopysignScalarOutOp,  # type: ignore
    torch.ops.aten.copysign.Tensor: Torch_AtenCopysignTensorOp,  # type: ignore
    torch.ops.aten.copysign.default: Torch_AtenCopysignOp,  # type: ignore
    torch.ops.aten.copysign.float: Torch_AtenCopysignFloatOp,  # type: ignore
    torch.ops.aten.copysign.float_int: Torch_AtenCopysignFloatIntOp,  # type: ignore
    torch.ops.aten.copysign.int: Torch_AtenCopysignIntOp,  # type: ignore
    torch.ops.aten.copysign.int_float: Torch_AtenCopysignIntFloatOp,  # type: ignore
    torch.ops.aten.copysign_.Scalar: Torch_AtenCopysign_ScalarOp,  # type: ignore
    torch.ops.aten.copysign_.Tensor: Torch_AtenCopysign_TensorOp,  # type: ignore
    torch.ops.aten.corrcoef.default: Torch_AtenCorrcoefOp,  # type: ignore
    torch.ops.aten.cos.Scalar: Torch_AtenCosScalarOp,  # type: ignore
    torch.ops.aten.cos.default: Torch_AtenCosOp,  # type: ignore
    torch.ops.aten.cos.float: Torch_AtenCosFloatOp,  # type: ignore
    torch.ops.aten.cos.int: Torch_AtenCosIntOp,  # type: ignore
    torch.ops.aten.cos_.default: Torch_AtenCos_Op,  # type: ignore
    torch.ops.aten.cosh.Scalar: Torch_AtenCoshScalarOp,  # type: ignore
    torch.ops.aten.cosh.default: Torch_AtenCoshOp,  # type: ignore
    torch.ops.aten.cosh.float: Torch_AtenCoshFloatOp,  # type: ignore
    torch.ops.aten.cosh.int: Torch_AtenCoshIntOp,  # type: ignore
    torch.ops.aten.cosh_.default: Torch_AtenCosh_Op,  # type: ignore
    torch.ops.aten.cosine_embedding_loss.default: Torch_AtenCosineEmbeddingLossOp,  # type: ignore
    torch.ops.aten.cosine_similarity.default: Torch_AtenCosineSimilarityOp,  # type: ignore
    torch.ops.aten.count_nonzero.default: Torch_AtenCountNonzeroOp,  # type: ignore
    torch.ops.aten.count_nonzero.dim_IntList: Torch_AtenCountNonzeroDimIntlistOp,  # type: ignore
    torch.ops.aten.count_nonzero.dim_IntList_out: Torch_AtenCountNonzeroDimIntlistOutOp,  # type: ignore
    torch.ops.aten.cov.default: Torch_AtenCovOp,  # type: ignore
    torch.ops.aten.cross.default: Torch_AtenCrossOp,  # type: ignore
    torch.ops.aten.cross_entropy_loss.default: Torch_AtenCrossEntropyLossOp,  # type: ignore
    torch.ops.aten.crow_indices.default: Torch_AtenCrowIndicesOp,  # type: ignore
    torch.ops.aten.crow_indices_copy.default: Torch_AtenCrowIndicesCopyOp,  # type: ignore
    torch.ops.aten.cudnn_affine_grid_generator.default: Torch_AtenCudnnAffineGridGeneratorOp,  # type: ignore
    torch.ops.aten.cudnn_affine_grid_generator_backward.default: Torch_AtenCudnnAffineGridGeneratorBackwardOp,  # type: ignore
    torch.ops.aten.cudnn_batch_norm.default: Torch_AtenCudnnBatchNormOp,  # type: ignore
    torch.ops.aten.cudnn_batch_norm_backward.default: Torch_AtenCudnnBatchNormBackwardOp,  # type: ignore
    torch.ops.aten.cudnn_convolution.default: Torch_AtenCudnnConvolutionOp,  # type: ignore
    torch.ops.aten.cudnn_convolution_add_relu.default: Torch_AtenCudnnConvolutionAddReluOp,  # type: ignore
    torch.ops.aten.cudnn_convolution_relu.default: Torch_AtenCudnnConvolutionReluOp,  # type: ignore
    torch.ops.aten.cudnn_convolution_transpose.default: Torch_AtenCudnnConvolutionTransposeOp,  # type: ignore
    torch.ops.aten.cudnn_grid_sampler.default: Torch_AtenCudnnGridSamplerOp,  # type: ignore
    torch.ops.aten.cudnn_grid_sampler_backward.default: Torch_AtenCudnnGridSamplerBackwardOp,  # type: ignore
    torch.ops.aten.cudnn_is_acceptable.default: Torch_AtenCudnnIsAcceptableOp,  # type: ignore
    torch.ops.aten.cummax.default: Torch_AtenCummaxOp,  # type: ignore
    torch.ops.aten.cummaxmin_backward.default: Torch_AtenCummaxminBackwardOp,  # type: ignore
    torch.ops.aten.cummin.default: Torch_AtenCumminOp,  # type: ignore
    torch.ops.aten.cumprod.default: Torch_AtenCumprodOp,  # type: ignore
    torch.ops.aten.cumprod_.default: Torch_AtenCumprod_Op,  # type: ignore
    torch.ops.aten.cumprod_backward.default: Torch_AtenCumprodBackwardOp,  # type: ignore
    torch.ops.aten.cumsum.default: Torch_AtenCumsumOp,  # type: ignore
    torch.ops.aten.cumsum_.default: Torch_AtenCumsum_Op,  # type: ignore
    torch.ops.aten.data.default: Torch_AtenDataOp,  # type: ignore
    torch.ops.aten.deg2rad.default: Torch_AtenDeg2RadOp,  # type: ignore
    torch.ops.aten.deg2rad_.default: Torch_AtenDeg2Rad_Op,  # type: ignore
    torch.ops.aten.dense_dim.default: Torch_AtenDenseDimOp,  # type: ignore
    torch.ops.aten.det.default: Torch_AtenDetOp,  # type: ignore
    torch.ops.aten.detach.default: Torch_AtenDetachOp,  # type: ignore
    torch.ops.aten.detach_.default: Torch_AtenDetach_Op,  # type: ignore
    torch.ops.aten.detach_copy.default: Torch_AtenDetachCopyOp,  # type: ignore
    torch.ops.aten.diag.default: Torch_AtenDiagOp,  # type: ignore
    torch.ops.aten.diag_embed.default: Torch_AtenDiagEmbedOp,  # type: ignore
    torch.ops.aten.diagflat.default: Torch_AtenDiagflatOp,  # type: ignore
    torch.ops.aten.diagonal.default: Torch_AtenDiagonalOp,  # type: ignore
    torch.ops.aten.diagonal_backward.default: Torch_AtenDiagonalBackwardOp,  # type: ignore
    torch.ops.aten.diagonal_copy.default: Torch_AtenDiagonalCopyOp,  # type: ignore
    torch.ops.aten.diagonal_scatter.default: Torch_AtenDiagonalScatterOp,  # type: ignore
    torch.ops.aten.diff.default: Torch_AtenDiffOp,  # type: ignore
    torch.ops.aten.digamma.default: Torch_AtenDigammaOp,  # type: ignore
    torch.ops.aten.digamma_.default: Torch_AtenDigamma_Op,  # type: ignore
    torch.ops.aten.dim.default: Torch_AtenDimOp,  # type: ignore
    torch.ops.aten.dist.default: Torch_AtenDistOp,  # type: ignore
    torch.ops.aten.div.Scalar: Torch_AtenDivScalarOp,  # type: ignore
    torch.ops.aten.div.Scalar_out: Torch_AtenDivScalarOutOp,  # type: ignore
    torch.ops.aten.div.Tensor: Torch_AtenDivTensorOp,  # type: ignore
    torch.ops.aten.div.default: Torch_AtenDivOp,  # type: ignore
    torch.ops.aten.div.float: Torch_AtenDivFloatOp,  # type: ignore
    torch.ops.aten.div.int: Torch_AtenDivIntOp,  # type: ignore
    torch.ops.aten.div_.Scalar: Torch_AtenDiv_ScalarOp,  # type: ignore
    torch.ops.aten.div_.Tensor: Torch_AtenDiv_TensorOp,  # type: ignore
    torch.ops.aten.divide.Scalar: Torch_AtenDivideScalarOp,  # type: ignore
    torch.ops.aten.divide.Tensor: Torch_AtenDivideTensorOp,  # type: ignore
    torch.ops.aten.divide_.Scalar: Torch_AtenDivide_ScalarOp,  # type: ignore
    torch.ops.aten.divide_.Tensor: Torch_AtenDivide_TensorOp,  # type: ignore
    torch.ops.aten.dot.default: Torch_AtenDotOp,  # type: ignore
    torch.ops.aten.dropout.default: Torch_AtenDropoutOp,  # type: ignore
    torch.ops.aten.dropout_.default: Torch_AtenDropout_Op,  # type: ignore
    torch.ops.aten.dstack.default: Torch_AtenDstackOp,  # type: ignore
    torch.ops.aten.einsum.sublist: Torch_AtenEinsumSublistOp,  # type: ignore
    torch.ops.aten.elu.default: Torch_AtenEluOp,  # type: ignore
    torch.ops.aten.elu_.default: Torch_AtenElu_Op,  # type: ignore
    torch.ops.aten.elu_backward.default: Torch_AtenEluBackwardOp,  # type: ignore
    torch.ops.aten.elu_backward.grad_input: Torch_AtenEluBackwardGradInputOp,  # type: ignore
    torch.ops.aten.embedding.default: Torch_AtenEmbeddingOp,  # type: ignore
    torch.ops.aten.embedding_backward.default: Torch_AtenEmbeddingBackwardOp,  # type: ignore
    torch.ops.aten.embedding_bag.default: Torch_AtenEmbeddingBagOp,  # type: ignore
    torch.ops.aten.embedding_bag.padding_idx: Torch_AtenEmbeddingBagPaddingIdxOp,  # type: ignore
    torch.ops.aten.embedding_dense_backward.default: Torch_AtenEmbeddingDenseBackwardOp,  # type: ignore
    torch.ops.aten.embedding_renorm_.default: Torch_AtenEmbeddingRenorm_Op,  # type: ignore
    torch.ops.aten.embedding_sparse_backward.default: Torch_AtenEmbeddingSparseBackwardOp,  # type: ignore
    torch.ops.aten.eq.Scalar: Torch_AtenEqScalarOp,  # type: ignore
    torch.ops.aten.eq.Scalar_out: Torch_AtenEqScalarOutOp,  # type: ignore
    torch.ops.aten.eq.Tensor: Torch_AtenEqTensorOp,  # type: ignore
    torch.ops.aten.eq.Tensor_list: Torch_AtenEqTensorListOp,  # type: ignore
    torch.ops.aten.eq.Tensor_out: Torch_AtenEqTensorOutOp,  # type: ignore
    torch.ops.aten.eq.bool: Torch_AtenEqBoolOp,  # type: ignore
    torch.ops.aten.eq.bool_list: Torch_AtenEqBoolListOp,  # type: ignore
    torch.ops.aten.eq.default: Torch_AtenEqOp,  # type: ignore
    torch.ops.aten.eq.float: Torch_AtenEqFloatOp,  # type: ignore
    torch.ops.aten.eq.float_int: Torch_AtenEqFloatIntOp,  # type: ignore
    torch.ops.aten.eq.float_list: Torch_AtenEqFloatListOp,  # type: ignore
    torch.ops.aten.eq.int: Torch_AtenEqIntOp,  # type: ignore
    torch.ops.aten.eq.int_float: Torch_AtenEqIntFloatOp,  # type: ignore
    torch.ops.aten.eq.int_list: Torch_AtenEqIntListOp,  # type: ignore
    torch.ops.aten.eq_.Scalar: Torch_AtenEq_ScalarOp,  # type: ignore
    torch.ops.aten.eq_.Tensor: Torch_AtenEq_TensorOp,  # type: ignore
    torch.ops.aten.equal.default: Torch_AtenEqualOp,  # type: ignore
    torch.ops.aten.erf.Scalar: Torch_AtenErfScalarOp,  # type: ignore
    torch.ops.aten.erf.default: Torch_AtenErfOp,  # type: ignore
    torch.ops.aten.erf.float: Torch_AtenErfFloatOp,  # type: ignore
    torch.ops.aten.erf.int: Torch_AtenErfIntOp,  # type: ignore
    torch.ops.aten.erf_.default: Torch_AtenErf_Op,  # type: ignore
    torch.ops.aten.erfc.Scalar: Torch_AtenErfcScalarOp,  # type: ignore
    torch.ops.aten.erfc.default: Torch_AtenErfcOp,  # type: ignore
    torch.ops.aten.erfc.float: Torch_AtenErfcFloatOp,  # type: ignore
    torch.ops.aten.erfc.int: Torch_AtenErfcIntOp,  # type: ignore
    torch.ops.aten.erfc_.default: Torch_AtenErfc_Op,  # type: ignore
    torch.ops.aten.erfinv.default: Torch_AtenErfinvOp,  # type: ignore
    torch.ops.aten.erfinv_.default: Torch_AtenErfinv_Op,  # type: ignore
    torch.ops.aten.exp.Scalar: Torch_AtenExpScalarOp,  # type: ignore
    torch.ops.aten.exp.default: Torch_AtenExpOp,  # type: ignore
    torch.ops.aten.exp.float: Torch_AtenExpFloatOp,  # type: ignore
    torch.ops.aten.exp.int: Torch_AtenExpIntOp,  # type: ignore
    torch.ops.aten.exp2.default: Torch_AtenExp2Op,  # type: ignore
    torch.ops.aten.exp2_.default: Torch_AtenExp2_Op,  # type: ignore
    torch.ops.aten.exp_.default: Torch_AtenExp_Op,  # type: ignore
    torch.ops.aten.expand.default: Torch_AtenExpandOp,  # type: ignore
    torch.ops.aten.expand_as.default: Torch_AtenExpandAsOp,  # type: ignore
    torch.ops.aten.expand_copy.default: Torch_AtenExpandCopyOp,  # type: ignore
    torch.ops.aten.expm1.Scalar: Torch_AtenExpm1ScalarOp,  # type: ignore
    torch.ops.aten.expm1.default: Torch_AtenExpm1Op,  # type: ignore
    torch.ops.aten.expm1.float: Torch_AtenExpm1FloatOp,  # type: ignore
    torch.ops.aten.expm1.int: Torch_AtenExpm1IntOp,  # type: ignore
    torch.ops.aten.expm1_.default: Torch_AtenExpm1_Op,  # type: ignore
    torch.ops.aten.eye.m_out: Torch_AtenEyeMOutOp,  # type: ignore
    torch.ops.aten.fake_quantize_per_channel_affine.default: Torch_AtenFakeQuantizePerChannelAffineOp,  # type: ignore
    torch.ops.aten.fake_quantize_per_channel_affine_cachemask.default: Torch_AtenFakeQuantizePerChannelAffineCachemaskOp,  # type: ignore
    torch.ops.aten.fake_quantize_per_channel_affine_cachemask_backward.default: Torch_AtenFakeQuantizePerChannelAffineCachemaskBackwardOp,  # type: ignore
    torch.ops.aten.fake_quantize_per_tensor_affine.default: Torch_AtenFakeQuantizePerTensorAffineOp,  # type: ignore
    torch.ops.aten.fake_quantize_per_tensor_affine.tensor_qparams: Torch_AtenFakeQuantizePerTensorAffineTensorQparamsOp,  # type: ignore
    torch.ops.aten.fake_quantize_per_tensor_affine_cachemask.default: Torch_AtenFakeQuantizePerTensorAffineCachemaskOp,  # type: ignore
    torch.ops.aten.fake_quantize_per_tensor_affine_cachemask_backward.default: Torch_AtenFakeQuantizePerTensorAffineCachemaskBackwardOp,  # type: ignore
    torch.ops.aten.fbgemm_linear_fp16_weight.default: Torch_AtenFbgemmLinearFp16WeightOp,  # type: ignore
    torch.ops.aten.fbgemm_linear_fp16_weight_fp32_activation.default: Torch_AtenFbgemmLinearFp16WeightFp32ActivationOp,  # type: ignore
    torch.ops.aten.fbgemm_linear_int8_weight.default: Torch_AtenFbgemmLinearInt8WeightOp,  # type: ignore
    torch.ops.aten.fbgemm_linear_int8_weight_fp32_activation.default: Torch_AtenFbgemmLinearInt8WeightFp32ActivationOp,  # type: ignore
    torch.ops.aten.fbgemm_linear_quantize_weight.default: Torch_AtenFbgemmLinearQuantizeWeightOp,  # type: ignore
    torch.ops.aten.fbgemm_pack_gemm_matrix_fp16.default: Torch_AtenFbgemmPackGemmMatrixFp16Op,  # type: ignore
    torch.ops.aten.fbgemm_pack_quantized_matrix.KN: Torch_AtenFbgemmPackQuantizedMatrixKnOp,  # type: ignore
    torch.ops.aten.fbgemm_pack_quantized_matrix.default: Torch_AtenFbgemmPackQuantizedMatrixOp,  # type: ignore
    torch.ops.aten.feature_alpha_dropout.default: Torch_AtenFeatureAlphaDropoutOp,  # type: ignore
    torch.ops.aten.feature_alpha_dropout_.default: Torch_AtenFeatureAlphaDropout_Op,  # type: ignore
    torch.ops.aten.feature_dropout.default: Torch_AtenFeatureDropoutOp,  # type: ignore
    torch.ops.aten.feature_dropout_.default: Torch_AtenFeatureDropout_Op,  # type: ignore
    torch.ops.aten.fft_fftshift.default: Torch_AtenFftFftshiftOp,  # type: ignore
    torch.ops.aten.fft_ifftshift.default: Torch_AtenFftIfftshiftOp,  # type: ignore
    torch.ops.aten.fill.Scalar: Torch_AtenFillScalarOp,  # type: ignore
    torch.ops.aten.fill.Scalar_out: Torch_AtenFillScalarOutOp,  # type: ignore
    torch.ops.aten.fill.Tensor: Torch_AtenFillTensorOp,  # type: ignore
    torch.ops.aten.fill.Tensor_out: Torch_AtenFillTensorOutOp,  # type: ignore
    torch.ops.aten.fill_.Scalar: Torch_AtenFill_ScalarOp,  # type: ignore
    torch.ops.aten.fill_.Tensor: Torch_AtenFill_TensorOp,  # type: ignore
    torch.ops.aten.fill_diagonal_.default: Torch_AtenFillDiagonal_Op,  # type: ignore
    torch.ops.aten.fix.default: Torch_AtenFixOp,  # type: ignore
    torch.ops.aten.fix_.default: Torch_AtenFix_Op,  # type: ignore
    torch.ops.aten.flatten_dense_tensors.default: Torch_AtenFlattenDenseTensorsOp,  # type: ignore
    torch.ops.aten.flip.default: Torch_AtenFlipOp,  # type: ignore
    torch.ops.aten.fliplr.default: Torch_AtenFliplrOp,  # type: ignore
    torch.ops.aten.flipud.default: Torch_AtenFlipudOp,  # type: ignore
    torch.ops.aten.float_power_.Scalar: Torch_AtenFloatPower_ScalarOp,  # type: ignore
    torch.ops.aten.float_power_.Tensor: Torch_AtenFloatPower_TensorOp,  # type: ignore
    torch.ops.aten.floor.Scalar: Torch_AtenFloorScalarOp,  # type: ignore
    torch.ops.aten.floor.default: Torch_AtenFloorOp,  # type: ignore
    torch.ops.aten.floor.float: Torch_AtenFloorFloatOp,  # type: ignore
    torch.ops.aten.floor.int: Torch_AtenFloorIntOp,  # type: ignore
    torch.ops.aten.floor_.default: Torch_AtenFloor_Op,  # type: ignore
    torch.ops.aten.floor_divide.Scalar: Torch_AtenFloorDivideScalarOp,  # type: ignore
    torch.ops.aten.floor_divide.Scalar_out: Torch_AtenFloorDivideScalarOutOp,  # type: ignore
    torch.ops.aten.floor_divide.default: Torch_AtenFloorDivideOp,  # type: ignore
    torch.ops.aten.floor_divide_.Scalar: Torch_AtenFloorDivide_ScalarOp,  # type: ignore
    torch.ops.aten.floor_divide_.Tensor: Torch_AtenFloorDivide_TensorOp,  # type: ignore
    torch.ops.aten.fmax.default: Torch_AtenFmaxOp,  # type: ignore
    torch.ops.aten.fmin.default: Torch_AtenFminOp,  # type: ignore
    torch.ops.aten.fmod.Scalar: Torch_AtenFmodScalarOp,  # type: ignore
    torch.ops.aten.fmod.Scalar_out: Torch_AtenFmodScalarOutOp,  # type: ignore
    torch.ops.aten.fmod.Tensor: Torch_AtenFmodTensorOp,  # type: ignore
    torch.ops.aten.fmod.Tensor_out: Torch_AtenFmodTensorOutOp,  # type: ignore
    torch.ops.aten.fmod.default: Torch_AtenFmodOp,  # type: ignore
    torch.ops.aten.fmod.float: Torch_AtenFmodFloatOp,  # type: ignore
    torch.ops.aten.fmod.float_int: Torch_AtenFmodFloatIntOp,  # type: ignore
    torch.ops.aten.fmod.int: Torch_AtenFmodIntOp,  # type: ignore
    torch.ops.aten.fmod.int_float: Torch_AtenFmodIntFloatOp,  # type: ignore
    torch.ops.aten.fmod_.Scalar: Torch_AtenFmod_ScalarOp,  # type: ignore
    torch.ops.aten.fmod_.Tensor: Torch_AtenFmod_TensorOp,  # type: ignore
    torch.ops.aten.frac.default: Torch_AtenFracOp,  # type: ignore
    torch.ops.aten.frac_.default: Torch_AtenFrac_Op,  # type: ignore
    torch.ops.aten.fractional_max_pool2d.default: Torch_AtenFractionalMaxPool2DOp,  # type: ignore
    torch.ops.aten.fractional_max_pool2d.output: Torch_AtenFractionalMaxPool2DOutputOp,  # type: ignore
    torch.ops.aten.fractional_max_pool2d_backward.default: Torch_AtenFractionalMaxPool2DBackwardOp,  # type: ignore
    torch.ops.aten.fractional_max_pool2d_backward.grad_input: Torch_AtenFractionalMaxPool2DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.fractional_max_pool3d.default: Torch_AtenFractionalMaxPool3DOp,  # type: ignore
    torch.ops.aten.fractional_max_pool3d.output: Torch_AtenFractionalMaxPool3DOutputOp,  # type: ignore
    torch.ops.aten.fractional_max_pool3d_backward.default: Torch_AtenFractionalMaxPool3DBackwardOp,  # type: ignore
    torch.ops.aten.fractional_max_pool3d_backward.grad_input: Torch_AtenFractionalMaxPool3DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.frexp.Tensor: Torch_AtenFrexpTensorOp,  # type: ignore
    torch.ops.aten.frexp.default: Torch_AtenFrexpOp,  # type: ignore
    torch.ops.aten.fused_moving_avg_obs_fake_quant.default: Torch_AtenFusedMovingAvgObsFakeQuantOp,  # type: ignore
    torch.ops.aten.gather.default: Torch_AtenGatherOp,  # type: ignore
    torch.ops.aten.gather_backward.default: Torch_AtenGatherBackwardOp,  # type: ignore
    torch.ops.aten.gcd.default: Torch_AtenGcdOp,  # type: ignore
    torch.ops.aten.gcd.int: Torch_AtenGcdIntOp,  # type: ignore
    torch.ops.aten.gcd_.default: Torch_AtenGcd_Op,  # type: ignore
    torch.ops.aten.ge.Scalar: Torch_AtenGeScalarOp,  # type: ignore
    torch.ops.aten.ge.Scalar_out: Torch_AtenGeScalarOutOp,  # type: ignore
    torch.ops.aten.ge.Tensor: Torch_AtenGeTensorOp,  # type: ignore
    torch.ops.aten.ge.Tensor_out: Torch_AtenGeTensorOutOp,  # type: ignore
    torch.ops.aten.ge.default: Torch_AtenGeOp,  # type: ignore
    torch.ops.aten.ge.float: Torch_AtenGeFloatOp,  # type: ignore
    torch.ops.aten.ge.float_int: Torch_AtenGeFloatIntOp,  # type: ignore
    torch.ops.aten.ge.int: Torch_AtenGeIntOp,  # type: ignore
    torch.ops.aten.ge.int_float: Torch_AtenGeIntFloatOp,  # type: ignore
    torch.ops.aten.ge_.Scalar: Torch_AtenGe_ScalarOp,  # type: ignore
    torch.ops.aten.ge_.Tensor: Torch_AtenGe_TensorOp,  # type: ignore
    torch.ops.aten.geqrf.default: Torch_AtenGeqrfOp,  # type: ignore
    torch.ops.aten.ger.default: Torch_AtenGerOp,  # type: ignore
    torch.ops.aten.glu.default: Torch_AtenGluOp,  # type: ignore
    torch.ops.aten.glu_backward.default: Torch_AtenGluBackwardOp,  # type: ignore
    torch.ops.aten.glu_backward.grad_input: Torch_AtenGluBackwardGradInputOp,  # type: ignore
    torch.ops.aten.glu_backward_jvp.default: Torch_AtenGluBackwardJvpOp,  # type: ignore
    torch.ops.aten.glu_jvp.default: Torch_AtenGluJvpOp,  # type: ignore
    torch.ops.aten.greater.Scalar: Torch_AtenGreaterScalarOp,  # type: ignore
    torch.ops.aten.greater.Scalar_out: Torch_AtenGreaterScalarOutOp,  # type: ignore
    torch.ops.aten.greater.Tensor: Torch_AtenGreaterTensorOp,  # type: ignore
    torch.ops.aten.greater.Tensor_out: Torch_AtenGreaterTensorOutOp,  # type: ignore
    torch.ops.aten.greater_.Scalar: Torch_AtenGreater_ScalarOp,  # type: ignore
    torch.ops.aten.greater_.Tensor: Torch_AtenGreater_TensorOp,  # type: ignore
    torch.ops.aten.greater_equal.Scalar: Torch_AtenGreaterEqualScalarOp,  # type: ignore
    torch.ops.aten.greater_equal.Scalar_out: Torch_AtenGreaterEqualScalarOutOp,  # type: ignore
    torch.ops.aten.greater_equal.Tensor: Torch_AtenGreaterEqualTensorOp,  # type: ignore
    torch.ops.aten.greater_equal.Tensor_out: Torch_AtenGreaterEqualTensorOutOp,  # type: ignore
    torch.ops.aten.greater_equal_.Scalar: Torch_AtenGreaterEqual_ScalarOp,  # type: ignore
    torch.ops.aten.greater_equal_.Tensor: Torch_AtenGreaterEqual_TensorOp,  # type: ignore
    torch.ops.aten.grid_sampler.default: Torch_AtenGridSamplerOp,  # type: ignore
    torch.ops.aten.grid_sampler_2d.default: Torch_AtenGridSampler2DOp,  # type: ignore
    torch.ops.aten.grid_sampler_2d_backward.default: Torch_AtenGridSampler2DBackwardOp,  # type: ignore
    torch.ops.aten.grid_sampler_3d.default: Torch_AtenGridSampler3DOp,  # type: ignore
    torch.ops.aten.grid_sampler_3d_backward.default: Torch_AtenGridSampler3DBackwardOp,  # type: ignore
    torch.ops.aten.group_norm.default: Torch_AtenGroupNormOp,  # type: ignore
    torch.ops.aten.gru.data: Torch_AtenGruDataOp,  # type: ignore
    torch.ops.aten.gru.input: Torch_AtenGruInputOp,  # type: ignore
    torch.ops.aten.gru_cell.default: Torch_AtenGruCellOp,  # type: ignore
    torch.ops.aten.gt.Scalar: Torch_AtenGtScalarOp,  # type: ignore
    torch.ops.aten.gt.Scalar_out: Torch_AtenGtScalarOutOp,  # type: ignore
    torch.ops.aten.gt.Tensor: Torch_AtenGtTensorOp,  # type: ignore
    torch.ops.aten.gt.Tensor_out: Torch_AtenGtTensorOutOp,  # type: ignore
    torch.ops.aten.gt.default: Torch_AtenGtOp,  # type: ignore
    torch.ops.aten.gt.float: Torch_AtenGtFloatOp,  # type: ignore
    torch.ops.aten.gt.float_int: Torch_AtenGtFloatIntOp,  # type: ignore
    torch.ops.aten.gt.int: Torch_AtenGtIntOp,  # type: ignore
    torch.ops.aten.gt.int_float: Torch_AtenGtIntFloatOp,  # type: ignore
    torch.ops.aten.gt_.Scalar: Torch_AtenGt_ScalarOp,  # type: ignore
    torch.ops.aten.gt_.Tensor: Torch_AtenGt_TensorOp,  # type: ignore
    torch.ops.aten.hamming_window.periodic_alpha_beta_out: Torch_AtenHammingWindowPeriodicAlphaBetaOutOp,  # type: ignore
    torch.ops.aten.hamming_window.periodic_alpha_out: Torch_AtenHammingWindowPeriodicAlphaOutOp,  # type: ignore
    torch.ops.aten.hamming_window.periodic_out: Torch_AtenHammingWindowPeriodicOutOp,  # type: ignore
    torch.ops.aten.hann_window.periodic_out: Torch_AtenHannWindowPeriodicOutOp,  # type: ignore
    torch.ops.aten.hardshrink.default: Torch_AtenHardshrinkOp,  # type: ignore
    torch.ops.aten.hardshrink_backward.default: Torch_AtenHardshrinkBackwardOp,  # type: ignore
    torch.ops.aten.hardshrink_backward.grad_input: Torch_AtenHardshrinkBackwardGradInputOp,  # type: ignore
    torch.ops.aten.hardsigmoid.default: Torch_AtenHardsigmoidOp,  # type: ignore
    torch.ops.aten.hardsigmoid_.default: Torch_AtenHardsigmoid_Op,  # type: ignore
    torch.ops.aten.hardsigmoid_backward.default: Torch_AtenHardsigmoidBackwardOp,  # type: ignore
    torch.ops.aten.hardsigmoid_backward.grad_input: Torch_AtenHardsigmoidBackwardGradInputOp,  # type: ignore
    torch.ops.aten.hardswish.default: Torch_AtenHardswishOp,  # type: ignore
    torch.ops.aten.hardswish_.default: Torch_AtenHardswish_Op,  # type: ignore
    torch.ops.aten.hardswish_backward.default: Torch_AtenHardswishBackwardOp,  # type: ignore
    torch.ops.aten.hardtanh.default: Torch_AtenHardtanhOp,  # type: ignore
    torch.ops.aten.hardtanh_.default: Torch_AtenHardtanh_Op,  # type: ignore
    torch.ops.aten.hardtanh_backward.default: Torch_AtenHardtanhBackwardOp,  # type: ignore
    torch.ops.aten.hardtanh_backward.grad_input: Torch_AtenHardtanhBackwardGradInputOp,  # type: ignore
    torch.ops.aten.heaviside.default: Torch_AtenHeavisideOp,  # type: ignore
    torch.ops.aten.heaviside_.default: Torch_AtenHeaviside_Op,  # type: ignore
    torch.ops.aten.hinge_embedding_loss.default: Torch_AtenHingeEmbeddingLossOp,  # type: ignore
    torch.ops.aten.histc.default: Torch_AtenHistcOp,  # type: ignore
    torch.ops.aten.histogramdd.TensorList_bins: Torch_AtenHistogramddTensorlistBinsOp,  # type: ignore
    torch.ops.aten.histogramdd.default: Torch_AtenHistogramddOp,  # type: ignore
    torch.ops.aten.histogramdd.int_bins: Torch_AtenHistogramddIntBinsOp,  # type: ignore
    torch.ops.aten.hspmm.default: Torch_AtenHspmmOp,  # type: ignore
    torch.ops.aten.hstack.default: Torch_AtenHstackOp,  # type: ignore
    torch.ops.aten.huber_loss.default: Torch_AtenHuberLossOp,  # type: ignore
    torch.ops.aten.huber_loss_backward.default: Torch_AtenHuberLossBackwardOp,  # type: ignore
    torch.ops.aten.hypot.default: Torch_AtenHypotOp,  # type: ignore
    torch.ops.aten.hypot_.default: Torch_AtenHypot_Op,  # type: ignore
    torch.ops.aten.i0.default: Torch_AtenI0Op,  # type: ignore
    torch.ops.aten.i0_.default: Torch_AtenI0_Op,  # type: ignore
    torch.ops.aten.igamma.default: Torch_AtenIgammaOp,  # type: ignore
    torch.ops.aten.igamma_.default: Torch_AtenIgamma_Op,  # type: ignore
    torch.ops.aten.igammac.default: Torch_AtenIgammacOp,  # type: ignore
    torch.ops.aten.igammac_.default: Torch_AtenIgammac_Op,  # type: ignore
    torch.ops.aten.im2col.default: Torch_AtenIm2ColOp,  # type: ignore
    torch.ops.aten.imag.default: Torch_AtenImagOp,  # type: ignore
    torch.ops.aten.index.Tensor_hacked_twin: Torch_AtenIndexTensorHackedTwinOp,  # type: ignore
    torch.ops.aten.index.list_Tensor: Torch_AtenIndexListTensorOp,  # type: ignore
    torch.ops.aten.index.list_bool: Torch_AtenIndexListBoolOp,  # type: ignore
    torch.ops.aten.index.list_float: Torch_AtenIndexListFloatOp,  # type: ignore
    torch.ops.aten.index.list_int: Torch_AtenIndexListIntOp,  # type: ignore
    torch.ops.aten.index_add.default: Torch_AtenIndexAddOp,  # type: ignore
    torch.ops.aten.index_add_.default: Torch_AtenIndexAdd_Op,  # type: ignore
    torch.ops.aten.index_copy.default: Torch_AtenIndexCopyOp,  # type: ignore
    torch.ops.aten.index_copy_.default: Torch_AtenIndexCopy_Op,  # type: ignore
    torch.ops.aten.index_fill.int_Scalar: Torch_AtenIndexFillIntScalarOp,  # type: ignore
    torch.ops.aten.index_fill.int_Scalar_out: Torch_AtenIndexFillIntScalarOutOp,  # type: ignore
    torch.ops.aten.index_fill.int_Tensor: Torch_AtenIndexFillIntTensorOp,  # type: ignore
    torch.ops.aten.index_fill.int_Tensor_out: Torch_AtenIndexFillIntTensorOutOp,  # type: ignore
    torch.ops.aten.index_fill_.int_Scalar: Torch_AtenIndexFill_IntScalarOp,  # type: ignore
    torch.ops.aten.index_fill_.int_Tensor: Torch_AtenIndexFill_IntTensorOp,  # type: ignore
    torch.ops.aten.index_put.hacked_twin: Torch_AtenIndexPutHackedTwinOp,  # type: ignore
    torch.ops.aten.index_put_.hacked_twin: Torch_AtenIndexPut_HackedTwinOp,  # type: ignore
    torch.ops.aten.index_select.default: Torch_AtenIndexSelectOp,  # type: ignore
    torch.ops.aten.index_select_backward.default: Torch_AtenIndexSelectBackwardOp,  # type: ignore
    torch.ops.aten.indices.default: Torch_AtenIndicesOp,  # type: ignore
    torch.ops.aten.indices_copy.default: Torch_AtenIndicesCopyOp,  # type: ignore
    torch.ops.aten.infinitely_differentiable_gelu_backward.default: Torch_AtenInfinitelyDifferentiableGeluBackwardOp,  # type: ignore
    torch.ops.aten.inner.default: Torch_AtenInnerOp,  # type: ignore
    torch.ops.aten.instance_norm.default: Torch_AtenInstanceNormOp,  # type: ignore
    torch.ops.aten.int_repr.default: Torch_AtenIntReprOp,  # type: ignore
    torch.ops.aten.inverse.default: Torch_AtenInverseOp,  # type: ignore
    torch.ops.aten.is_coalesced.default: Torch_AtenIsCoalescedOp,  # type: ignore
    torch.ops.aten.is_complex.default: Torch_AtenIsComplexOp,  # type: ignore
    torch.ops.aten.is_conj.default: Torch_AtenIsConjOp,  # type: ignore
    torch.ops.aten.is_contiguous.default: Torch_AtenIsContiguousOp,  # type: ignore
    torch.ops.aten.is_contiguous.memory_format: Torch_AtenIsContiguousMemoryFormatOp,  # type: ignore
    torch.ops.aten.is_distributed.default: Torch_AtenIsDistributedOp,  # type: ignore
    torch.ops.aten.is_floating_point.default: Torch_AtenIsFloatingPointOp,  # type: ignore
    torch.ops.aten.is_inference.default: Torch_AtenIsInferenceOp,  # type: ignore
    torch.ops.aten.is_leaf.default: Torch_AtenIsLeafOp,  # type: ignore
    torch.ops.aten.is_neg.default: Torch_AtenIsNegOp,  # type: ignore
    torch.ops.aten.is_non_overlapping_and_dense.default: Torch_AtenIsNonOverlappingAndDenseOp,  # type: ignore
    torch.ops.aten.is_nonzero.default: Torch_AtenIsNonzeroOp,  # type: ignore
    torch.ops.aten.is_same_size.default: Torch_AtenIsSameSizeOp,  # type: ignore
    torch.ops.aten.is_set_to.default: Torch_AtenIsSetToOp,  # type: ignore
    torch.ops.aten.is_signed.default: Torch_AtenIsSignedOp,  # type: ignore
    torch.ops.aten.is_strides_like_format.default: Torch_AtenIsStridesLikeFormatOp,  # type: ignore
    torch.ops.aten.is_vulkan_available.default: Torch_AtenIsVulkanAvailableOp,  # type: ignore
    torch.ops.aten.isclose.default: Torch_AtenIscloseOp,  # type: ignore
    torch.ops.aten.isfinite.default: Torch_AtenIsfiniteOp,  # type: ignore
    torch.ops.aten.isfinite.float: Torch_AtenIsfiniteFloatOp,  # type: ignore
    torch.ops.aten.isin.Scalar_Tensor: Torch_AtenIsinScalarTensorOp,  # type: ignore
    torch.ops.aten.isin.Scalar_Tensor_out: Torch_AtenIsinScalarTensorOutOp,  # type: ignore
    torch.ops.aten.isin.Tensor_Scalar: Torch_AtenIsinTensorScalarOp,  # type: ignore
    torch.ops.aten.isin.Tensor_Scalar_out: Torch_AtenIsinTensorScalarOutOp,  # type: ignore
    torch.ops.aten.isin.Tensor_Tensor: Torch_AtenIsinTensorTensorOp,  # type: ignore
    torch.ops.aten.isin.Tensor_Tensor_out: Torch_AtenIsinTensorTensorOutOp,  # type: ignore
    torch.ops.aten.isinf.default: Torch_AtenIsinfOp,  # type: ignore
    torch.ops.aten.isinf.float: Torch_AtenIsinfFloatOp,  # type: ignore
    torch.ops.aten.isnan.default: Torch_AtenIsnanOp,  # type: ignore
    torch.ops.aten.isnan.float: Torch_AtenIsnanFloatOp,  # type: ignore
    torch.ops.aten.isneginf.default: Torch_AtenIsneginfOp,  # type: ignore
    torch.ops.aten.isposinf.default: Torch_AtenIsposinfOp,  # type: ignore
    torch.ops.aten.isreal.default: Torch_AtenIsrealOp,  # type: ignore
    torch.ops.aten.istft.default: Torch_AtenIstftOp,  # type: ignore
    torch.ops.aten.item.default: Torch_AtenItemOp,  # type: ignore
    torch.ops.aten.kaiser_window.beta_out: Torch_AtenKaiserWindowBetaOutOp,  # type: ignore
    torch.ops.aten.kaiser_window.periodic_out: Torch_AtenKaiserWindowPeriodicOutOp,  # type: ignore
    torch.ops.aten.kl_div.default: Torch_AtenKlDivOp,  # type: ignore
    torch.ops.aten.kron.default: Torch_AtenKronOp,  # type: ignore
    torch.ops.aten.kthvalue.default: Torch_AtenKthvalueOp,  # type: ignore
    torch.ops.aten.l1_loss.default: Torch_AtenL1LossOp,  # type: ignore
    torch.ops.aten.layer_norm.default: Torch_AtenLayerNormOp,  # type: ignore
    torch.ops.aten.lcm.default: Torch_AtenLcmOp,  # type: ignore
    torch.ops.aten.lcm_.default: Torch_AtenLcm_Op,  # type: ignore
    torch.ops.aten.ldexp_.default: Torch_AtenLdexp_Op,  # type: ignore
    torch.ops.aten.le.Scalar: Torch_AtenLeScalarOp,  # type: ignore
    torch.ops.aten.le.Scalar_out: Torch_AtenLeScalarOutOp,  # type: ignore
    torch.ops.aten.le.Tensor: Torch_AtenLeTensorOp,  # type: ignore
    torch.ops.aten.le.Tensor_out: Torch_AtenLeTensorOutOp,  # type: ignore
    torch.ops.aten.le.default: Torch_AtenLeOp,  # type: ignore
    torch.ops.aten.le.float: Torch_AtenLeFloatOp,  # type: ignore
    torch.ops.aten.le.float_int: Torch_AtenLeFloatIntOp,  # type: ignore
    torch.ops.aten.le.int: Torch_AtenLeIntOp,  # type: ignore
    torch.ops.aten.le.int_float: Torch_AtenLeIntFloatOp,  # type: ignore
    torch.ops.aten.le_.Scalar: Torch_AtenLe_ScalarOp,  # type: ignore
    torch.ops.aten.le_.Tensor: Torch_AtenLe_TensorOp,  # type: ignore
    torch.ops.aten.leaky_relu.default: Torch_AtenLeakyReluOp,  # type: ignore
    torch.ops.aten.leaky_relu_.default: Torch_AtenLeakyRelu_Op,  # type: ignore
    torch.ops.aten.leaky_relu_backward.default: Torch_AtenLeakyReluBackwardOp,  # type: ignore
    torch.ops.aten.leaky_relu_backward.grad_input: Torch_AtenLeakyReluBackwardGradInputOp,  # type: ignore
    torch.ops.aten.lerp.Scalar: Torch_AtenLerpScalarOp,  # type: ignore
    torch.ops.aten.lerp.Scalar_out: Torch_AtenLerpScalarOutOp,  # type: ignore
    torch.ops.aten.lerp.Tensor: Torch_AtenLerpTensorOp,  # type: ignore
    torch.ops.aten.lerp.Tensor_out: Torch_AtenLerpTensorOutOp,  # type: ignore
    torch.ops.aten.lerp_.Scalar: Torch_AtenLerp_ScalarOp,  # type: ignore
    torch.ops.aten.lerp_.Tensor: Torch_AtenLerp_TensorOp,  # type: ignore
    torch.ops.aten.less.Scalar: Torch_AtenLessScalarOp,  # type: ignore
    torch.ops.aten.less.Scalar_out: Torch_AtenLessScalarOutOp,  # type: ignore
    torch.ops.aten.less.Tensor: Torch_AtenLessTensorOp,  # type: ignore
    torch.ops.aten.less.Tensor_out: Torch_AtenLessTensorOutOp,  # type: ignore
    torch.ops.aten.less_.Scalar: Torch_AtenLess_ScalarOp,  # type: ignore
    torch.ops.aten.less_.Tensor: Torch_AtenLess_TensorOp,  # type: ignore
    torch.ops.aten.less_equal.Scalar: Torch_AtenLessEqualScalarOp,  # type: ignore
    torch.ops.aten.less_equal.Scalar_out: Torch_AtenLessEqualScalarOutOp,  # type: ignore
    torch.ops.aten.less_equal.Tensor: Torch_AtenLessEqualTensorOp,  # type: ignore
    torch.ops.aten.less_equal.Tensor_out: Torch_AtenLessEqualTensorOutOp,  # type: ignore
    torch.ops.aten.less_equal_.Scalar: Torch_AtenLessEqual_ScalarOp,  # type: ignore
    torch.ops.aten.less_equal_.Tensor: Torch_AtenLessEqual_TensorOp,  # type: ignore
    torch.ops.aten.lgamma.Scalar: Torch_AtenLgammaScalarOp,  # type: ignore
    torch.ops.aten.lgamma.default: Torch_AtenLgammaOp,  # type: ignore
    torch.ops.aten.lgamma.float: Torch_AtenLgammaFloatOp,  # type: ignore
    torch.ops.aten.lgamma.int: Torch_AtenLgammaIntOp,  # type: ignore
    torch.ops.aten.lgamma_.default: Torch_AtenLgamma_Op,  # type: ignore
    torch.ops.aten.lift.default: Torch_AtenLiftOp,  # type: ignore
    torch.ops.aten.lift_fresh.default: Torch_AtenLiftFreshOp,  # type: ignore
    torch.ops.aten.lift_fresh_copy.default: Torch_AtenLiftFreshCopyOp,  # type: ignore
    torch.ops.aten.linalg_cholesky.default: Torch_AtenLinalgCholeskyOp,  # type: ignore
    torch.ops.aten.linalg_cholesky_ex.default: Torch_AtenLinalgCholeskyExOp,  # type: ignore
    torch.ops.aten.linalg_cond.default: Torch_AtenLinalgCondOp,  # type: ignore
    torch.ops.aten.linalg_cross.default: Torch_AtenLinalgCrossOp,  # type: ignore
    torch.ops.aten.linalg_det.default: Torch_AtenLinalgDetOp,  # type: ignore
    torch.ops.aten.linalg_diagonal.default: Torch_AtenLinalgDiagonalOp,  # type: ignore
    torch.ops.aten.linalg_eig.default: Torch_AtenLinalgEigOp,  # type: ignore
    torch.ops.aten.linalg_eigvals.default: Torch_AtenLinalgEigvalsOp,  # type: ignore
    torch.ops.aten.linalg_householder_product.default: Torch_AtenLinalgHouseholderProductOp,  # type: ignore
    torch.ops.aten.linalg_inv.default: Torch_AtenLinalgInvOp,  # type: ignore
    torch.ops.aten.linalg_inv_ex.default: Torch_AtenLinalgInvExOp,  # type: ignore
    torch.ops.aten.linalg_ldl_factor.default: Torch_AtenLinalgLdlFactorOp,  # type: ignore
    torch.ops.aten.linalg_ldl_factor_ex.default: Torch_AtenLinalgLdlFactorExOp,  # type: ignore
    torch.ops.aten.linalg_ldl_solve.default: Torch_AtenLinalgLdlSolveOp,  # type: ignore
    torch.ops.aten.linalg_lu.default: Torch_AtenLinalgLuOp,  # type: ignore
    torch.ops.aten.linalg_lu_factor.default: Torch_AtenLinalgLuFactorOp,  # type: ignore
    torch.ops.aten.linalg_lu_factor_ex.default: Torch_AtenLinalgLuFactorExOp,  # type: ignore
    torch.ops.aten.linalg_lu_solve.default: Torch_AtenLinalgLuSolveOp,  # type: ignore
    torch.ops.aten.linalg_matmul.default: Torch_AtenLinalgMatmulOp,  # type: ignore
    torch.ops.aten.linalg_matrix_exp.default: Torch_AtenLinalgMatrixExpOp,  # type: ignore
    torch.ops.aten.linalg_matrix_norm.default: Torch_AtenLinalgMatrixNormOp,  # type: ignore
    torch.ops.aten.linalg_matrix_power.default: Torch_AtenLinalgMatrixPowerOp,  # type: ignore
    torch.ops.aten.linalg_matrix_rank.atol_rtol_float: Torch_AtenLinalgMatrixRankAtolRtolFloatOp,  # type: ignore
    torch.ops.aten.linalg_matrix_rank.atol_rtol_float_out: Torch_AtenLinalgMatrixRankAtolRtolFloatOutOp,  # type: ignore
    torch.ops.aten.linalg_matrix_rank.atol_rtol_tensor: Torch_AtenLinalgMatrixRankAtolRtolTensorOp,  # type: ignore
    torch.ops.aten.linalg_matrix_rank.atol_rtol_tensor_out: Torch_AtenLinalgMatrixRankAtolRtolTensorOutOp,  # type: ignore
    torch.ops.aten.linalg_matrix_rank.default: Torch_AtenLinalgMatrixRankOp,  # type: ignore
    torch.ops.aten.linalg_matrix_rank.out_tol_tensor: Torch_AtenLinalgMatrixRankOutTolTensorOp,  # type: ignore
    torch.ops.aten.linalg_matrix_rank.tol_tensor: Torch_AtenLinalgMatrixRankTolTensorOp,  # type: ignore
    torch.ops.aten.linalg_multi_dot.default: Torch_AtenLinalgMultiDotOp,  # type: ignore
    torch.ops.aten.linalg_norm.default: Torch_AtenLinalgNormOp,  # type: ignore
    torch.ops.aten.linalg_pinv.atol_rtol_float: Torch_AtenLinalgPinvAtolRtolFloatOp,  # type: ignore
    torch.ops.aten.linalg_pinv.atol_rtol_float_out: Torch_AtenLinalgPinvAtolRtolFloatOutOp,  # type: ignore
    torch.ops.aten.linalg_pinv.atol_rtol_tensor: Torch_AtenLinalgPinvAtolRtolTensorOp,  # type: ignore
    torch.ops.aten.linalg_pinv.atol_rtol_tensor_out: Torch_AtenLinalgPinvAtolRtolTensorOutOp,  # type: ignore
    torch.ops.aten.linalg_pinv.default: Torch_AtenLinalgPinvOp,  # type: ignore
    torch.ops.aten.linalg_pinv.out_rcond_tensor: Torch_AtenLinalgPinvOutRcondTensorOp,  # type: ignore
    torch.ops.aten.linalg_pinv.rcond_tensor: Torch_AtenLinalgPinvRcondTensorOp,  # type: ignore
    torch.ops.aten.linalg_slogdet.default: Torch_AtenLinalgSlogdetOp,  # type: ignore
    torch.ops.aten.linalg_solve.default: Torch_AtenLinalgSolveOp,  # type: ignore
    torch.ops.aten.linalg_solve_ex.default: Torch_AtenLinalgSolveExOp,  # type: ignore
    torch.ops.aten.linalg_solve_triangular.default: Torch_AtenLinalgSolveTriangularOp,  # type: ignore
    torch.ops.aten.linalg_tensorinv.default: Torch_AtenLinalgTensorinvOp,  # type: ignore
    torch.ops.aten.linalg_tensorsolve.default: Torch_AtenLinalgTensorsolveOp,  # type: ignore
    torch.ops.aten.linalg_vander.default: Torch_AtenLinalgVanderOp,  # type: ignore
    torch.ops.aten.linalg_vecdot.default: Torch_AtenLinalgVecdotOp,  # type: ignore
    torch.ops.aten.linalg_vector_norm.default: Torch_AtenLinalgVectorNormOp,  # type: ignore
    torch.ops.aten.linear.default: Torch_AtenLinearOp,  # type: ignore
    torch.ops.aten.linear_backward.default: Torch_AtenLinearBackwardOp,  # type: ignore
    torch.ops.aten.linspace.Scalar_Tensor_out: Torch_AtenLinspaceScalarTensorOutOp,  # type: ignore
    torch.ops.aten.linspace.Tensor_Scalar_out: Torch_AtenLinspaceTensorScalarOutOp,  # type: ignore
    torch.ops.aten.linspace.Tensor_Tensor_out: Torch_AtenLinspaceTensorTensorOutOp,  # type: ignore
    torch.ops.aten.log.Scalar: Torch_AtenLogScalarOp,  # type: ignore
    torch.ops.aten.log.Scalar_Scalar: Torch_AtenLogScalarScalarOp,  # type: ignore
    torch.ops.aten.log.default: Torch_AtenLogOp,  # type: ignore
    torch.ops.aten.log.float: Torch_AtenLogFloatOp,  # type: ignore
    torch.ops.aten.log.float_float: Torch_AtenLogFloatFloatOp,  # type: ignore
    torch.ops.aten.log.float_int: Torch_AtenLogFloatIntOp,  # type: ignore
    torch.ops.aten.log.int: Torch_AtenLogIntOp,  # type: ignore
    torch.ops.aten.log.int_float: Torch_AtenLogIntFloatOp,  # type: ignore
    torch.ops.aten.log.int_int: Torch_AtenLogIntIntOp,  # type: ignore
    torch.ops.aten.log10.Scalar: Torch_AtenLog10ScalarOp,  # type: ignore
    torch.ops.aten.log10.default: Torch_AtenLog10Op,  # type: ignore
    torch.ops.aten.log10.float: Torch_AtenLog10FloatOp,  # type: ignore
    torch.ops.aten.log10.int: Torch_AtenLog10IntOp,  # type: ignore
    torch.ops.aten.log10_.default: Torch_AtenLog10_Op,  # type: ignore
    torch.ops.aten.log1p.Scalar: Torch_AtenLog1PScalarOp,  # type: ignore
    torch.ops.aten.log1p.default: Torch_AtenLog1POp,  # type: ignore
    torch.ops.aten.log1p.float: Torch_AtenLog1PFloatOp,  # type: ignore
    torch.ops.aten.log1p.int: Torch_AtenLog1PIntOp,  # type: ignore
    torch.ops.aten.log1p_.default: Torch_AtenLog1P_Op,  # type: ignore
    torch.ops.aten.log2.default: Torch_AtenLog2Op,  # type: ignore
    torch.ops.aten.log2_.default: Torch_AtenLog2_Op,  # type: ignore
    torch.ops.aten.log_.default: Torch_AtenLog_Op,  # type: ignore
    torch.ops.aten.log_sigmoid.default: Torch_AtenLogSigmoidOp,  # type: ignore
    torch.ops.aten.log_sigmoid_backward.default: Torch_AtenLogSigmoidBackwardOp,  # type: ignore
    torch.ops.aten.log_sigmoid_backward.grad_input: Torch_AtenLogSigmoidBackwardGradInputOp,  # type: ignore
    torch.ops.aten.log_sigmoid_forward.default: Torch_AtenLogSigmoidForwardOp,  # type: ignore
    torch.ops.aten.log_sigmoid_forward.output: Torch_AtenLogSigmoidForwardOutputOp,  # type: ignore
    torch.ops.aten.logaddexp.default: Torch_AtenLogaddexpOp,  # type: ignore
    torch.ops.aten.logaddexp2.default: Torch_AtenLogaddexp2Op,  # type: ignore
    torch.ops.aten.logcumsumexp.default: Torch_AtenLogcumsumexpOp,  # type: ignore
    torch.ops.aten.logdet.default: Torch_AtenLogdetOp,  # type: ignore
    torch.ops.aten.logical_and.default: Torch_AtenLogicalAndOp,  # type: ignore
    torch.ops.aten.logical_and_.default: Torch_AtenLogicalAnd_Op,  # type: ignore
    torch.ops.aten.logical_not.default: Torch_AtenLogicalNotOp,  # type: ignore
    torch.ops.aten.logical_not_.default: Torch_AtenLogicalNot_Op,  # type: ignore
    torch.ops.aten.logical_or.default: Torch_AtenLogicalOrOp,  # type: ignore
    torch.ops.aten.logical_or_.default: Torch_AtenLogicalOr_Op,  # type: ignore
    torch.ops.aten.logical_xor.default: Torch_AtenLogicalXorOp,  # type: ignore
    torch.ops.aten.logical_xor_.default: Torch_AtenLogicalXor_Op,  # type: ignore
    torch.ops.aten.logit.default: Torch_AtenLogitOp,  # type: ignore
    torch.ops.aten.logit_.default: Torch_AtenLogit_Op,  # type: ignore
    torch.ops.aten.logit_backward.default: Torch_AtenLogitBackwardOp,  # type: ignore
    torch.ops.aten.logit_backward.grad_input: Torch_AtenLogitBackwardGradInputOp,  # type: ignore
    torch.ops.aten.logspace.Scalar_Tensor_out: Torch_AtenLogspaceScalarTensorOutOp,  # type: ignore
    torch.ops.aten.logspace.Tensor_Scalar_out: Torch_AtenLogspaceTensorScalarOutOp,  # type: ignore
    torch.ops.aten.logspace.Tensor_Tensor_out: Torch_AtenLogspaceTensorTensorOutOp,  # type: ignore
    torch.ops.aten.logsumexp.default: Torch_AtenLogsumexpOp,  # type: ignore
    torch.ops.aten.lstm.data: Torch_AtenLstmDataOp,  # type: ignore
    torch.ops.aten.lstm.input: Torch_AtenLstmInputOp,  # type: ignore
    torch.ops.aten.lstm_cell.default: Torch_AtenLstmCellOp,  # type: ignore
    torch.ops.aten.lstm_mps_backward.default: Torch_AtenLstmMpsBackwardOp,  # type: ignore
    torch.ops.aten.lt.Scalar: Torch_AtenLtScalarOp,  # type: ignore
    torch.ops.aten.lt.Scalar_out: Torch_AtenLtScalarOutOp,  # type: ignore
    torch.ops.aten.lt.Tensor: Torch_AtenLtTensorOp,  # type: ignore
    torch.ops.aten.lt.Tensor_out: Torch_AtenLtTensorOutOp,  # type: ignore
    torch.ops.aten.lt.default: Torch_AtenLtOp,  # type: ignore
    torch.ops.aten.lt.float: Torch_AtenLtFloatOp,  # type: ignore
    torch.ops.aten.lt.float_int: Torch_AtenLtFloatIntOp,  # type: ignore
    torch.ops.aten.lt.int: Torch_AtenLtIntOp,  # type: ignore
    torch.ops.aten.lt.int_float: Torch_AtenLtIntFloatOp,  # type: ignore
    torch.ops.aten.lt_.Scalar: Torch_AtenLt_ScalarOp,  # type: ignore
    torch.ops.aten.lt_.Tensor: Torch_AtenLt_TensorOp,  # type: ignore
    torch.ops.aten.lu_solve.default: Torch_AtenLuSolveOp,  # type: ignore
    torch.ops.aten.lu_unpack.default: Torch_AtenLuUnpackOp,  # type: ignore
    torch.ops.aten.mH.a: Torch_AtenMhAOp,  # type: ignore
    torch.ops.aten.mH.default: Torch_AtenMhOp,  # type: ignore
    torch.ops.aten.mT.a: Torch_AtenMtAOp,  # type: ignore
    torch.ops.aten.mT.default: Torch_AtenMtOp,  # type: ignore
    torch.ops.aten.margin_ranking_loss.default: Torch_AtenMarginRankingLossOp,  # type: ignore
    torch.ops.aten.masked_fill.Scalar: Torch_AtenMaskedFillScalarOp,  # type: ignore
    torch.ops.aten.masked_fill.Scalar_out: Torch_AtenMaskedFillScalarOutOp,  # type: ignore
    torch.ops.aten.masked_fill.Tensor: Torch_AtenMaskedFillTensorOp,  # type: ignore
    torch.ops.aten.masked_fill.Tensor_out: Torch_AtenMaskedFillTensorOutOp,  # type: ignore
    torch.ops.aten.masked_fill_.Scalar: Torch_AtenMaskedFill_ScalarOp,  # type: ignore
    torch.ops.aten.masked_fill_.Tensor: Torch_AtenMaskedFill_TensorOp,  # type: ignore
    torch.ops.aten.masked_scatter.default: Torch_AtenMaskedScatterOp,  # type: ignore
    torch.ops.aten.masked_scatter_.default: Torch_AtenMaskedScatter_Op,  # type: ignore
    torch.ops.aten.masked_scatter_backward.default: Torch_AtenMaskedScatterBackwardOp,  # type: ignore
    torch.ops.aten.masked_select.default: Torch_AtenMaskedSelectOp,  # type: ignore
    torch.ops.aten.masked_select_backward.default: Torch_AtenMaskedSelectBackwardOp,  # type: ignore
    torch.ops.aten.matmul.default: Torch_AtenMatmulOp,  # type: ignore
    torch.ops.aten.matmul_backward.default: Torch_AtenMatmulBackwardOp,  # type: ignore
    torch.ops.aten.matrix_H.a: Torch_AtenMatrixHAOp,  # type: ignore
    torch.ops.aten.matrix_H.default: Torch_AtenMatrixHOp,  # type: ignore
    torch.ops.aten.matrix_exp.default: Torch_AtenMatrixExpOp,  # type: ignore
    torch.ops.aten.matrix_exp_backward.default: Torch_AtenMatrixExpBackwardOp,  # type: ignore
    torch.ops.aten.matrix_power.default: Torch_AtenMatrixPowerOp,  # type: ignore
    torch.ops.aten.max.default: Torch_AtenMaxOp,  # type: ignore
    torch.ops.aten.max.dim: Torch_AtenMaxDimOp,  # type: ignore
    torch.ops.aten.max.dim_max: Torch_AtenMaxDimMaxOp,  # type: ignore
    torch.ops.aten.max.other: Torch_AtenMaxOtherOp,  # type: ignore
    torch.ops.aten.max.unary_out: Torch_AtenMaxUnaryOutOp,  # type: ignore
    torch.ops.aten.max_pool1d.default: Torch_AtenMaxPool1DOp,  # type: ignore
    torch.ops.aten.max_pool1d_with_indices.default: Torch_AtenMaxPool1DWithIndicesOp,  # type: ignore
    torch.ops.aten.max_pool2d.default: Torch_AtenMaxPool2DOp,  # type: ignore
    torch.ops.aten.max_pool2d_backward.default: Torch_AtenMaxPool2DBackwardOp,  # type: ignore
    torch.ops.aten.max_pool2d_with_indices.default: Torch_AtenMaxPool2DWithIndicesOp,  # type: ignore
    torch.ops.aten.max_pool2d_with_indices_backward.default: Torch_AtenMaxPool2DWithIndicesBackwardOp,  # type: ignore
    torch.ops.aten.max_pool2d_with_indices_backward.grad_input: Torch_AtenMaxPool2DWithIndicesBackwardGradInputOp,  # type: ignore
    torch.ops.aten.max_pool3d.default: Torch_AtenMaxPool3DOp,  # type: ignore
    torch.ops.aten.max_pool3d_with_indices.default: Torch_AtenMaxPool3DWithIndicesOp,  # type: ignore
    torch.ops.aten.max_pool3d_with_indices_backward.default: Torch_AtenMaxPool3DWithIndicesBackwardOp,  # type: ignore
    torch.ops.aten.max_pool3d_with_indices_backward.grad_input: Torch_AtenMaxPool3DWithIndicesBackwardGradInputOp,  # type: ignore
    torch.ops.aten.max_unpool2d.default: Torch_AtenMaxUnpool2DOp,  # type: ignore
    torch.ops.aten.max_unpool3d.default: Torch_AtenMaxUnpool3DOp,  # type: ignore
    torch.ops.aten.maximum.default: Torch_AtenMaximumOp,  # type: ignore
    torch.ops.aten.mean.default: Torch_AtenMeanOp,  # type: ignore
    torch.ops.aten.mean.dim: Torch_AtenMeanDimOp,  # type: ignore
    torch.ops.aten.mean.dtype_out: Torch_AtenMeanDtypeOutOp,  # type: ignore
    torch.ops.aten.median.default: Torch_AtenMedianOp,  # type: ignore
    torch.ops.aten.median.dim: Torch_AtenMedianDimOp,  # type: ignore
    torch.ops.aten.meshgrid.default: Torch_AtenMeshgridOp,  # type: ignore
    torch.ops.aten.min.default: Torch_AtenMinOp,  # type: ignore
    torch.ops.aten.min.dim: Torch_AtenMinDimOp,  # type: ignore
    torch.ops.aten.min.dim_min: Torch_AtenMinDimMinOp,  # type: ignore
    torch.ops.aten.min.other: Torch_AtenMinOtherOp,  # type: ignore
    torch.ops.aten.min.unary_out: Torch_AtenMinUnaryOutOp,  # type: ignore
    torch.ops.aten.minimum.default: Torch_AtenMinimumOp,  # type: ignore
    torch.ops.aten.miopen_batch_norm.default: Torch_AtenMiopenBatchNormOp,  # type: ignore
    torch.ops.aten.miopen_batch_norm_backward.default: Torch_AtenMiopenBatchNormBackwardOp,  # type: ignore
    torch.ops.aten.miopen_convolution.default: Torch_AtenMiopenConvolutionOp,  # type: ignore
    torch.ops.aten.miopen_convolution_add_relu.default: Torch_AtenMiopenConvolutionAddReluOp,  # type: ignore
    torch.ops.aten.miopen_convolution_relu.default: Torch_AtenMiopenConvolutionReluOp,  # type: ignore
    torch.ops.aten.miopen_convolution_transpose.default: Torch_AtenMiopenConvolutionTransposeOp,  # type: ignore
    torch.ops.aten.miopen_depthwise_convolution.default: Torch_AtenMiopenDepthwiseConvolutionOp,  # type: ignore
    torch.ops.aten.miopen_rnn.default: Torch_AtenMiopenRnnOp,  # type: ignore
    torch.ops.aten.miopen_rnn_backward.default: Torch_AtenMiopenRnnBackwardOp,  # type: ignore
    torch.ops.aten.mish.default: Torch_AtenMishOp,  # type: ignore
    torch.ops.aten.mish_.default: Torch_AtenMish_Op,  # type: ignore
    torch.ops.aten.mish_backward.default: Torch_AtenMishBackwardOp,  # type: ignore
    torch.ops.aten.mkldnn_adaptive_avg_pool2d.default: Torch_AtenMkldnnAdaptiveAvgPool2DOp,  # type: ignore
    torch.ops.aten.mkldnn_adaptive_avg_pool2d_backward.default: Torch_AtenMkldnnAdaptiveAvgPool2DBackwardOp,  # type: ignore
    torch.ops.aten.mkldnn_convolution.default: Torch_AtenMkldnnConvolutionOp,  # type: ignore
    torch.ops.aten.mkldnn_linear.default: Torch_AtenMkldnnLinearOp,  # type: ignore
    torch.ops.aten.mkldnn_linear_backward.default: Torch_AtenMkldnnLinearBackwardOp,  # type: ignore
    torch.ops.aten.mkldnn_linear_backward_input.default: Torch_AtenMkldnnLinearBackwardInputOp,  # type: ignore
    torch.ops.aten.mkldnn_linear_backward_weights.default: Torch_AtenMkldnnLinearBackwardWeightsOp,  # type: ignore
    torch.ops.aten.mkldnn_max_pool2d.default: Torch_AtenMkldnnMaxPool2DOp,  # type: ignore
    torch.ops.aten.mkldnn_max_pool2d_backward.default: Torch_AtenMkldnnMaxPool2DBackwardOp,  # type: ignore
    torch.ops.aten.mkldnn_max_pool3d.default: Torch_AtenMkldnnMaxPool3DOp,  # type: ignore
    torch.ops.aten.mkldnn_max_pool3d_backward.default: Torch_AtenMkldnnMaxPool3DBackwardOp,  # type: ignore
    torch.ops.aten.mkldnn_reorder_conv2d_weight.default: Torch_AtenMkldnnReorderConv2DWeightOp,  # type: ignore
    torch.ops.aten.mkldnn_reorder_conv3d_weight.default: Torch_AtenMkldnnReorderConv3DWeightOp,  # type: ignore
    torch.ops.aten.mkldnn_rnn_layer.default: Torch_AtenMkldnnRnnLayerOp,  # type: ignore
    torch.ops.aten.mkldnn_rnn_layer_backward.default: Torch_AtenMkldnnRnnLayerBackwardOp,  # type: ignore
    torch.ops.aten.mm.default: Torch_AtenMmOp,  # type: ignore
    torch.ops.aten.mode.default: Torch_AtenModeOp,  # type: ignore
    torch.ops.aten.mps_convolution_backward.default: Torch_AtenMpsConvolutionBackwardOp,  # type: ignore
    torch.ops.aten.mps_convolution_transpose_backward.default: Torch_AtenMpsConvolutionTransposeBackwardOp,  # type: ignore
    torch.ops.aten.mse_loss.default: Torch_AtenMseLossOp,  # type: ignore
    torch.ops.aten.mse_loss_backward.default: Torch_AtenMseLossBackwardOp,  # type: ignore
    torch.ops.aten.mse_loss_backward.grad_input: Torch_AtenMseLossBackwardGradInputOp,  # type: ignore
    torch.ops.aten.msort.default: Torch_AtenMsortOp,  # type: ignore
    torch.ops.aten.mul.Scalar: Torch_AtenMulScalarOp,  # type: ignore
    torch.ops.aten.mul.Scalar_out: Torch_AtenMulScalarOutOp,  # type: ignore
    torch.ops.aten.mul.Tensor: Torch_AtenMulTensorOp,  # type: ignore
    torch.ops.aten.mul.default: Torch_AtenMulOp,  # type: ignore
    torch.ops.aten.mul.float: Torch_AtenMulFloatOp,  # type: ignore
    torch.ops.aten.mul.float_int: Torch_AtenMulFloatIntOp,  # type: ignore
    torch.ops.aten.mul.int: Torch_AtenMulIntOp,  # type: ignore
    torch.ops.aten.mul.int_float: Torch_AtenMulIntFloatOp,  # type: ignore
    torch.ops.aten.mul_.Scalar: Torch_AtenMul_ScalarOp,  # type: ignore
    torch.ops.aten.mul_.Tensor: Torch_AtenMul_TensorOp,  # type: ignore
    torch.ops.aten.multi_margin_loss.default: Torch_AtenMultiMarginLossOp,  # type: ignore
    torch.ops.aten.multi_margin_loss_backward.default: Torch_AtenMultiMarginLossBackwardOp,  # type: ignore
    torch.ops.aten.multi_margin_loss_backward.grad_input: Torch_AtenMultiMarginLossBackwardGradInputOp,  # type: ignore
    torch.ops.aten.multilabel_margin_loss.default: Torch_AtenMultilabelMarginLossOp,  # type: ignore
    torch.ops.aten.multilabel_margin_loss_backward.default: Torch_AtenMultilabelMarginLossBackwardOp,  # type: ignore
    torch.ops.aten.multilabel_margin_loss_backward.grad_input: Torch_AtenMultilabelMarginLossBackwardGradInputOp,  # type: ignore
    torch.ops.aten.multilabel_margin_loss_forward.default: Torch_AtenMultilabelMarginLossForwardOp,  # type: ignore
    torch.ops.aten.multilabel_margin_loss_forward.output: Torch_AtenMultilabelMarginLossForwardOutputOp,  # type: ignore
    torch.ops.aten.multiply.Scalar: Torch_AtenMultiplyScalarOp,  # type: ignore
    torch.ops.aten.multiply.Tensor: Torch_AtenMultiplyTensorOp,  # type: ignore
    torch.ops.aten.multiply_.Scalar: Torch_AtenMultiply_ScalarOp,  # type: ignore
    torch.ops.aten.multiply_.Tensor: Torch_AtenMultiply_TensorOp,  # type: ignore
    torch.ops.aten.mv.default: Torch_AtenMvOp,  # type: ignore
    torch.ops.aten.mvlgamma.default: Torch_AtenMvlgammaOp,  # type: ignore
    torch.ops.aten.mvlgamma_.default: Torch_AtenMvlgamma_Op,  # type: ignore
    torch.ops.aten.nan_to_num.default: Torch_AtenNanToNumOp,  # type: ignore
    torch.ops.aten.nan_to_num_.default: Torch_AtenNanToNum_Op,  # type: ignore
    torch.ops.aten.nanmean.default: Torch_AtenNanmeanOp,  # type: ignore
    torch.ops.aten.nanmedian.default: Torch_AtenNanmedianOp,  # type: ignore
    torch.ops.aten.nanmedian.dim: Torch_AtenNanmedianDimOp,  # type: ignore
    torch.ops.aten.nansum.default: Torch_AtenNansumOp,  # type: ignore
    torch.ops.aten.narrow.Tensor: Torch_AtenNarrowTensorOp,  # type: ignore
    torch.ops.aten.narrow.default: Torch_AtenNarrowOp,  # type: ignore
    torch.ops.aten.narrow_copy.default: Torch_AtenNarrowCopyOp,  # type: ignore
    torch.ops.aten.native_batch_norm.default: Torch_AtenNativeBatchNormOp,  # type: ignore
    torch.ops.aten.native_batch_norm_backward.default: Torch_AtenNativeBatchNormBackwardOp,  # type: ignore
    torch.ops.aten.native_channel_shuffle.default: Torch_AtenNativeChannelShuffleOp,  # type: ignore
    torch.ops.aten.native_dropout.default: Torch_AtenNativeDropoutOp,  # type: ignore
    torch.ops.aten.native_dropout_backward.default: Torch_AtenNativeDropoutBackwardOp,  # type: ignore
    torch.ops.aten.native_group_norm.default: Torch_AtenNativeGroupNormOp,  # type: ignore
    torch.ops.aten.native_group_norm_backward.default: Torch_AtenNativeGroupNormBackwardOp,  # type: ignore
    torch.ops.aten.native_layer_norm.default: Torch_AtenNativeLayerNormOp,  # type: ignore
    torch.ops.aten.native_layer_norm_backward.default: Torch_AtenNativeLayerNormBackwardOp,  # type: ignore
    torch.ops.aten.native_norm.ScalarOpt_dim_dtype: Torch_AtenNativeNormScalaroptDimDtypeOp,  # type: ignore
    torch.ops.aten.native_norm.ScalarOpt_dim_dtype_out: Torch_AtenNativeNormScalaroptDimDtypeOutOp,  # type: ignore
    torch.ops.aten.native_norm.default: Torch_AtenNativeNormOp,  # type: ignore
    torch.ops.aten.ne.Scalar: Torch_AtenNeScalarOp,  # type: ignore
    torch.ops.aten.ne.Scalar_out: Torch_AtenNeScalarOutOp,  # type: ignore
    torch.ops.aten.ne.Tensor: Torch_AtenNeTensorOp,  # type: ignore
    torch.ops.aten.ne.Tensor_list: Torch_AtenNeTensorListOp,  # type: ignore
    torch.ops.aten.ne.Tensor_out: Torch_AtenNeTensorOutOp,  # type: ignore
    torch.ops.aten.ne.bool: Torch_AtenNeBoolOp,  # type: ignore
    torch.ops.aten.ne.bool_list: Torch_AtenNeBoolListOp,  # type: ignore
    torch.ops.aten.ne.default: Torch_AtenNeOp,  # type: ignore
    torch.ops.aten.ne.float: Torch_AtenNeFloatOp,  # type: ignore
    torch.ops.aten.ne.float_int: Torch_AtenNeFloatIntOp,  # type: ignore
    torch.ops.aten.ne.float_list: Torch_AtenNeFloatListOp,  # type: ignore
    torch.ops.aten.ne.int: Torch_AtenNeIntOp,  # type: ignore
    torch.ops.aten.ne.int_float: Torch_AtenNeIntFloatOp,  # type: ignore
    torch.ops.aten.ne.int_list: Torch_AtenNeIntListOp,  # type: ignore
    torch.ops.aten.ne_.Scalar: Torch_AtenNe_ScalarOp,  # type: ignore
    torch.ops.aten.ne_.Tensor: Torch_AtenNe_TensorOp,  # type: ignore
    torch.ops.aten.neg.Scalar: Torch_AtenNegScalarOp,  # type: ignore
    torch.ops.aten.neg.default: Torch_AtenNegOp,  # type: ignore
    torch.ops.aten.neg.float: Torch_AtenNegFloatOp,  # type: ignore
    torch.ops.aten.neg.int: Torch_AtenNegIntOp,  # type: ignore
    torch.ops.aten.neg_.default: Torch_AtenNeg_Op,  # type: ignore
    torch.ops.aten.negative.default: Torch_AtenNegativeOp,  # type: ignore
    torch.ops.aten.negative_.default: Torch_AtenNegative_Op,  # type: ignore
    torch.ops.aten.nested_to_padded_tensor.default: Torch_AtenNestedToPaddedTensorOp,  # type: ignore
    torch.ops.aten.nextafter.default: Torch_AtenNextafterOp,  # type: ignore
    torch.ops.aten.nextafter_.default: Torch_AtenNextafter_Op,  # type: ignore
    torch.ops.aten.nll_loss.default: Torch_AtenNllLossOp,  # type: ignore
    torch.ops.aten.nll_loss2d.default: Torch_AtenNllLoss2DOp,  # type: ignore
    torch.ops.aten.nll_loss2d_backward.default: Torch_AtenNllLoss2DBackwardOp,  # type: ignore
    torch.ops.aten.nll_loss2d_backward.grad_input: Torch_AtenNllLoss2DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.nll_loss2d_forward.default: Torch_AtenNllLoss2DForwardOp,  # type: ignore
    torch.ops.aten.nll_loss2d_forward.output: Torch_AtenNllLoss2DForwardOutputOp,  # type: ignore
    torch.ops.aten.nll_loss_backward.default: Torch_AtenNllLossBackwardOp,  # type: ignore
    torch.ops.aten.nll_loss_backward.grad_input: Torch_AtenNllLossBackwardGradInputOp,  # type: ignore
    torch.ops.aten.nll_loss_forward.default: Torch_AtenNllLossForwardOp,  # type: ignore
    torch.ops.aten.nll_loss_forward.output: Torch_AtenNllLossForwardOutputOp,  # type: ignore
    torch.ops.aten.nll_loss_nd.default: Torch_AtenNllLossNdOp,  # type: ignore
    torch.ops.aten.nonzero.default: Torch_AtenNonzeroOp,  # type: ignore
    torch.ops.aten.nonzero_numpy.default: Torch_AtenNonzeroNumpyOp,  # type: ignore
    torch.ops.aten.nonzero_static.default: Torch_AtenNonzeroStaticOp,  # type: ignore
    torch.ops.aten.norm.Scalar: Torch_AtenNormScalarOp,  # type: ignore
    torch.ops.aten.norm.ScalarOpt_dim: Torch_AtenNormScalaroptDimOp,  # type: ignore
    torch.ops.aten.norm.ScalarOpt_dim_dtype: Torch_AtenNormScalaroptDimDtypeOp,  # type: ignore
    torch.ops.aten.norm.ScalarOpt_dtype: Torch_AtenNormScalaroptDtypeOp,  # type: ignore
    torch.ops.aten.norm.ScalarOpt_dtype_out: Torch_AtenNormScalaroptDtypeOutOp,  # type: ignore
    torch.ops.aten.norm.Scalar_out: Torch_AtenNormScalarOutOp,  # type: ignore
    torch.ops.aten.norm.dtype_out: Torch_AtenNormDtypeOutOp,  # type: ignore
    torch.ops.aten.norm_except_dim.default: Torch_AtenNormExceptDimOp,  # type: ignore
    torch.ops.aten.not_equal.Scalar: Torch_AtenNotEqualScalarOp,  # type: ignore
    torch.ops.aten.not_equal.Scalar_out: Torch_AtenNotEqualScalarOutOp,  # type: ignore
    torch.ops.aten.not_equal.Tensor: Torch_AtenNotEqualTensorOp,  # type: ignore
    torch.ops.aten.not_equal.Tensor_out: Torch_AtenNotEqualTensorOutOp,  # type: ignore
    torch.ops.aten.not_equal_.Scalar: Torch_AtenNotEqual_ScalarOp,  # type: ignore
    torch.ops.aten.not_equal_.Tensor: Torch_AtenNotEqual_TensorOp,  # type: ignore
    torch.ops.aten.nuclear_norm.default: Torch_AtenNuclearNormOp,  # type: ignore
    torch.ops.aten.nuclear_norm.dim: Torch_AtenNuclearNormDimOp,  # type: ignore
    torch.ops.aten.nuclear_norm.dim_out: Torch_AtenNuclearNormDimOutOp,  # type: ignore
    torch.ops.aten.numel.default: Torch_AtenNumelOp,  # type: ignore
    torch.ops.aten.numpy_T.a: Torch_AtenNumpyTAOp,  # type: ignore
    torch.ops.aten.numpy_T.default: Torch_AtenNumpyTOp,  # type: ignore
    torch.ops.aten.one_hot.default: Torch_AtenOneHotOp,  # type: ignore
    torch.ops.aten.orgqr.default: Torch_AtenOrgqrOp,  # type: ignore
    torch.ops.aten.ormqr.default: Torch_AtenOrmqrOp,  # type: ignore
    torch.ops.aten.outer.default: Torch_AtenOuterOp,  # type: ignore
    torch.ops.aten.output_nr.default: Torch_AtenOutputNrOp,  # type: ignore
    torch.ops.aten.pairwise_distance.default: Torch_AtenPairwiseDistanceOp,  # type: ignore
    torch.ops.aten.pdist.default: Torch_AtenPdistOp,  # type: ignore
    torch.ops.aten.permute.default: Torch_AtenPermuteOp,  # type: ignore
    torch.ops.aten.permute_copy.default: Torch_AtenPermuteCopyOp,  # type: ignore
    torch.ops.aten.pinverse.default: Torch_AtenPinverseOp,  # type: ignore
    torch.ops.aten.pixel_shuffle.default: Torch_AtenPixelShuffleOp,  # type: ignore
    torch.ops.aten.pixel_unshuffle.default: Torch_AtenPixelUnshuffleOp,  # type: ignore
    torch.ops.aten.poisson_nll_loss.default: Torch_AtenPoissonNllLossOp,  # type: ignore
    torch.ops.aten.polar.Scalar_Scalar: Torch_AtenPolarScalarScalarOp,  # type: ignore
    torch.ops.aten.polar.default: Torch_AtenPolarOp,  # type: ignore
    torch.ops.aten.polygamma.default: Torch_AtenPolygammaOp,  # type: ignore
    torch.ops.aten.polygamma_.default: Torch_AtenPolygamma_Op,  # type: ignore
    torch.ops.aten.positive.default: Torch_AtenPositiveOp,  # type: ignore
    torch.ops.aten.pow.Scalar: Torch_AtenPowScalarOp,  # type: ignore
    torch.ops.aten.pow.Scalar_Scalar: Torch_AtenPowScalarScalarOp,  # type: ignore
    torch.ops.aten.pow.Scalar_out: Torch_AtenPowScalarOutOp,  # type: ignore
    torch.ops.aten.pow.Tensor_Scalar: Torch_AtenPowTensorScalarOp,  # type: ignore
    torch.ops.aten.pow.Tensor_Scalar_out: Torch_AtenPowTensorScalarOutOp,  # type: ignore
    torch.ops.aten.pow.Tensor_Tensor: Torch_AtenPowTensorTensorOp,  # type: ignore
    torch.ops.aten.pow.Tensor_Tensor_out: Torch_AtenPowTensorTensorOutOp,  # type: ignore
    torch.ops.aten.pow.float: Torch_AtenPowFloatOp,  # type: ignore
    torch.ops.aten.pow.float_int: Torch_AtenPowFloatIntOp,  # type: ignore
    torch.ops.aten.pow.int: Torch_AtenPowIntOp,  # type: ignore
    torch.ops.aten.pow.int_float: Torch_AtenPowIntFloatOp,  # type: ignore
    torch.ops.aten.pow.int_to_int: Torch_AtenPowIntToIntOp,  # type: ignore
    torch.ops.aten.pow_.Scalar: Torch_AtenPow_ScalarOp,  # type: ignore
    torch.ops.aten.pow_.Tensor: Torch_AtenPow_TensorOp,  # type: ignore
    torch.ops.aten.prelu.default: Torch_AtenPreluOp,  # type: ignore
    torch.ops.aten.prod.default: Torch_AtenProdOp,  # type: ignore
    torch.ops.aten.prod.dim_int: Torch_AtenProdDimIntOp,  # type: ignore
    torch.ops.aten.prod.int_out: Torch_AtenProdIntOutOp,  # type: ignore
    torch.ops.aten.promote_types.default: Torch_AtenPromoteTypesOp,  # type: ignore
    torch.ops.aten.put.default: Torch_AtenPutOp,  # type: ignore
    torch.ops.aten.put_.default: Torch_AtenPut_Op,  # type: ignore
    torch.ops.aten.q_per_channel_axis.default: Torch_AtenQPerChannelAxisOp,  # type: ignore
    torch.ops.aten.q_per_channel_scales.default: Torch_AtenQPerChannelScalesOp,  # type: ignore
    torch.ops.aten.q_per_channel_zero_points.default: Torch_AtenQPerChannelZeroPointsOp,  # type: ignore
    torch.ops.aten.q_scale.default: Torch_AtenQScaleOp,  # type: ignore
    torch.ops.aten.q_zero_point.default: Torch_AtenQZeroPointOp,  # type: ignore
    torch.ops.aten.qr.default: Torch_AtenQrOp,  # type: ignore
    torch.ops.aten.quantize_per_channel.default: Torch_AtenQuantizePerChannelOp,  # type: ignore
    torch.ops.aten.quantize_per_tensor.default: Torch_AtenQuantizePerTensorOp,  # type: ignore
    torch.ops.aten.quantize_per_tensor.tensor_qparams: Torch_AtenQuantizePerTensorTensorQparamsOp,  # type: ignore
    torch.ops.aten.quantize_per_tensor.tensor_qparams_out: Torch_AtenQuantizePerTensorTensorQparamsOutOp,  # type: ignore
    torch.ops.aten.quantize_per_tensor.tensors: Torch_AtenQuantizePerTensorTensorsOp,  # type: ignore
    torch.ops.aten.quantize_per_tensor.tensors_out: Torch_AtenQuantizePerTensorTensorsOutOp,  # type: ignore
    torch.ops.aten.quantize_per_tensor_dynamic.default: Torch_AtenQuantizePerTensorDynamicOp,  # type: ignore
    torch.ops.aten.quantized_batch_norm.default: Torch_AtenQuantizedBatchNormOp,  # type: ignore
    torch.ops.aten.quantized_gru.data_legacy: Torch_AtenQuantizedGruDataLegacyOp,  # type: ignore
    torch.ops.aten.quantized_gru.input_legacy: Torch_AtenQuantizedGruInputLegacyOp,  # type: ignore
    torch.ops.aten.quantized_gru_cell.default: Torch_AtenQuantizedGruCellOp,  # type: ignore
    torch.ops.aten.quantized_lstm.data_legacy: Torch_AtenQuantizedLstmDataLegacyOp,  # type: ignore
    torch.ops.aten.quantized_lstm.input_legacy: Torch_AtenQuantizedLstmInputLegacyOp,  # type: ignore
    torch.ops.aten.quantized_lstm_cell.default: Torch_AtenQuantizedLstmCellOp,  # type: ignore
    torch.ops.aten.quantized_max_pool1d.default: Torch_AtenQuantizedMaxPool1DOp,  # type: ignore
    torch.ops.aten.quantized_max_pool2d.default: Torch_AtenQuantizedMaxPool2DOp,  # type: ignore
    torch.ops.aten.quantized_max_pool3d.default: Torch_AtenQuantizedMaxPool3DOp,  # type: ignore
    torch.ops.aten.quantized_rnn_relu_cell.default: Torch_AtenQuantizedRnnReluCellOp,  # type: ignore
    torch.ops.aten.quantized_rnn_tanh_cell.default: Torch_AtenQuantizedRnnTanhCellOp,  # type: ignore
    torch.ops.aten.rad2deg.default: Torch_AtenRad2DegOp,  # type: ignore
    torch.ops.aten.rad2deg_.default: Torch_AtenRad2Deg_Op,  # type: ignore
    torch.ops.aten.randint.low_out: Torch_AtenRandintLowOutOp,  # type: ignore
    torch.ops.aten.randint_like.low_dtype_out: Torch_AtenRandintLikeLowDtypeOutOp,  # type: ignore
    torch.ops.aten.range.out_: Torch_AtenRangeOut_Op,  # type: ignore
    torch.ops.aten.ravel.default: Torch_AtenRavelOp,  # type: ignore
    torch.ops.aten.real.default: Torch_AtenRealOp,  # type: ignore
    torch.ops.aten.reciprocal.default: Torch_AtenReciprocalOp,  # type: ignore
    torch.ops.aten.reciprocal_.default: Torch_AtenReciprocal_Op,  # type: ignore
    torch.ops.aten.reflection_pad1d.default: Torch_AtenReflectionPad1DOp,  # type: ignore
    torch.ops.aten.reflection_pad1d_backward.default: Torch_AtenReflectionPad1DBackwardOp,  # type: ignore
    torch.ops.aten.reflection_pad1d_backward.grad_input: Torch_AtenReflectionPad1DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.reflection_pad2d.default: Torch_AtenReflectionPad2DOp,  # type: ignore
    torch.ops.aten.reflection_pad2d_backward.default: Torch_AtenReflectionPad2DBackwardOp,  # type: ignore
    torch.ops.aten.reflection_pad2d_backward.grad_input: Torch_AtenReflectionPad2DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.reflection_pad3d.default: Torch_AtenReflectionPad3DOp,  # type: ignore
    torch.ops.aten.reflection_pad3d_backward.default: Torch_AtenReflectionPad3DBackwardOp,  # type: ignore
    torch.ops.aten.reflection_pad3d_backward.grad_input: Torch_AtenReflectionPad3DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.relu.default: Torch_AtenReluOp,  # type: ignore
    torch.ops.aten.relu6.default: Torch_AtenRelu6Op,  # type: ignore
    torch.ops.aten.relu6_.default: Torch_AtenRelu6_Op,  # type: ignore
    torch.ops.aten.relu_.default: Torch_AtenRelu_Op,  # type: ignore
    torch.ops.aten.remainder.Scalar: Torch_AtenRemainderScalarOp,  # type: ignore
    torch.ops.aten.remainder.Scalar_Tensor: Torch_AtenRemainderScalarTensorOp,  # type: ignore
    torch.ops.aten.remainder.Scalar_Tensor_out: Torch_AtenRemainderScalarTensorOutOp,  # type: ignore
    torch.ops.aten.remainder.Scalar_out: Torch_AtenRemainderScalarOutOp,  # type: ignore
    torch.ops.aten.remainder.Tensor: Torch_AtenRemainderTensorOp,  # type: ignore
    torch.ops.aten.remainder.Tensor_out: Torch_AtenRemainderTensorOutOp,  # type: ignore
    torch.ops.aten.remainder.default: Torch_AtenRemainderOp,  # type: ignore
    torch.ops.aten.remainder.float: Torch_AtenRemainderFloatOp,  # type: ignore
    torch.ops.aten.remainder.float_int: Torch_AtenRemainderFloatIntOp,  # type: ignore
    torch.ops.aten.remainder.int: Torch_AtenRemainderIntOp,  # type: ignore
    torch.ops.aten.remainder.int_float: Torch_AtenRemainderIntFloatOp,  # type: ignore
    torch.ops.aten.remainder_.Scalar: Torch_AtenRemainder_ScalarOp,  # type: ignore
    torch.ops.aten.remainder_.Tensor: Torch_AtenRemainder_TensorOp,  # type: ignore
    torch.ops.aten.renorm.default: Torch_AtenRenormOp,  # type: ignore
    torch.ops.aten.renorm_.default: Torch_AtenRenorm_Op,  # type: ignore
    torch.ops.aten.repeat.default: Torch_AtenRepeatOp,  # type: ignore
    torch.ops.aten.repeat_interleave.Tensor: Torch_AtenRepeatInterleaveTensorOp,  # type: ignore
    torch.ops.aten.repeat_interleave.Tensor_out: Torch_AtenRepeatInterleaveTensorOutOp,  # type: ignore
    torch.ops.aten.repeat_interleave.self_Tensor: Torch_AtenRepeatInterleaveSelfTensorOp,  # type: ignore
    torch.ops.aten.repeat_interleave.self_int: Torch_AtenRepeatInterleaveSelfIntOp,  # type: ignore
    torch.ops.aten.replication_pad1d.default: Torch_AtenReplicationPad1DOp,  # type: ignore
    torch.ops.aten.replication_pad1d_backward.default: Torch_AtenReplicationPad1DBackwardOp,  # type: ignore
    torch.ops.aten.replication_pad1d_backward.grad_input: Torch_AtenReplicationPad1DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.replication_pad2d.default: Torch_AtenReplicationPad2DOp,  # type: ignore
    torch.ops.aten.replication_pad2d_backward.default: Torch_AtenReplicationPad2DBackwardOp,  # type: ignore
    torch.ops.aten.replication_pad2d_backward.grad_input: Torch_AtenReplicationPad2DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.replication_pad3d.default: Torch_AtenReplicationPad3DOp,  # type: ignore
    torch.ops.aten.replication_pad3d_backward.default: Torch_AtenReplicationPad3DBackwardOp,  # type: ignore
    torch.ops.aten.replication_pad3d_backward.grad_input: Torch_AtenReplicationPad3DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.requires_grad_.default: Torch_AtenRequiresGrad_Op,  # type: ignore
    torch.ops.aten.reshape.default: Torch_AtenReshapeOp,  # type: ignore
    torch.ops.aten.reshape_as.default: Torch_AtenReshapeAsOp,  # type: ignore
    torch.ops.aten.resize.default: Torch_AtenResizeOp,  # type: ignore
    torch.ops.aten.resize_.default: Torch_AtenResize_Op,  # type: ignore
    torch.ops.aten.resize_as.default: Torch_AtenResizeAsOp,  # type: ignore
    torch.ops.aten.resize_as_.default: Torch_AtenResizeAs_Op,  # type: ignore
    torch.ops.aten.resize_as_sparse_.default: Torch_AtenResizeAsSparse_Op,  # type: ignore
    torch.ops.aten.resolve_conj.default: Torch_AtenResolveConjOp,  # type: ignore
    torch.ops.aten.resolve_neg.default: Torch_AtenResolveNegOp,  # type: ignore
    torch.ops.aten.retain_grad.default: Torch_AtenRetainGradOp,  # type: ignore
    torch.ops.aten.retains_grad.default: Torch_AtenRetainsGradOp,  # type: ignore
    torch.ops.aten.rms_norm.default: Torch_AtenRmsNormOp,  # type: ignore
    torch.ops.aten.rnn_relu.data: Torch_AtenRnnReluDataOp,  # type: ignore
    torch.ops.aten.rnn_relu.input: Torch_AtenRnnReluInputOp,  # type: ignore
    torch.ops.aten.rnn_relu_cell.default: Torch_AtenRnnReluCellOp,  # type: ignore
    torch.ops.aten.rnn_tanh.data: Torch_AtenRnnTanhDataOp,  # type: ignore
    torch.ops.aten.rnn_tanh.input: Torch_AtenRnnTanhInputOp,  # type: ignore
    torch.ops.aten.rnn_tanh_cell.default: Torch_AtenRnnTanhCellOp,  # type: ignore
    torch.ops.aten.roll.default: Torch_AtenRollOp,  # type: ignore
    torch.ops.aten.rot90.default: Torch_AtenRot90Op,  # type: ignore
    torch.ops.aten.round.Scalar: Torch_AtenRoundScalarOp,  # type: ignore
    torch.ops.aten.round.decimals: Torch_AtenRoundDecimalsOp,  # type: ignore
    torch.ops.aten.round.decimals_out: Torch_AtenRoundDecimalsOutOp,  # type: ignore
    torch.ops.aten.round.default: Torch_AtenRoundOp,  # type: ignore
    torch.ops.aten.round.float: Torch_AtenRoundFloatOp,  # type: ignore
    torch.ops.aten.round.int: Torch_AtenRoundIntOp,  # type: ignore
    torch.ops.aten.round_.decimals: Torch_AtenRound_DecimalsOp,  # type: ignore
    torch.ops.aten.round_.default: Torch_AtenRound_Op,  # type: ignore
    torch.ops.aten.row_indices.default: Torch_AtenRowIndicesOp,  # type: ignore
    torch.ops.aten.row_indices_copy.default: Torch_AtenRowIndicesCopyOp,  # type: ignore
    torch.ops.aten.row_stack.default: Torch_AtenRowStackOp,  # type: ignore
    torch.ops.aten.rrelu_with_noise_backward.default: Torch_AtenRreluWithNoiseBackwardOp,  # type: ignore
    torch.ops.aten.rsqrt.default: Torch_AtenRsqrtOp,  # type: ignore
    torch.ops.aten.rsqrt_.default: Torch_AtenRsqrt_Op,  # type: ignore
    torch.ops.aten.rsub.Scalar: Torch_AtenRsubScalarOp,  # type: ignore
    torch.ops.aten.rsub.Scalar_out: Torch_AtenRsubScalarOutOp,  # type: ignore
    torch.ops.aten.rsub.Tensor: Torch_AtenRsubTensorOp,  # type: ignore
    torch.ops.aten.rsub.Tensor_out: Torch_AtenRsubTensorOutOp,  # type: ignore
    torch.ops.aten.scaled_dot_product_attention.default: Torch_AtenScaledDotProductAttentionOp,  # type: ignore
    torch.ops.aten.scatter.src: Torch_AtenScatterSrcOp,  # type: ignore
    torch.ops.aten.scatter.src_out: Torch_AtenScatterSrcOutOp,  # type: ignore
    torch.ops.aten.scatter.value: Torch_AtenScatterValueOp,  # type: ignore
    torch.ops.aten.scatter.value_out: Torch_AtenScatterValueOutOp,  # type: ignore
    torch.ops.aten.scatter_.src: Torch_AtenScatter_SrcOp,  # type: ignore
    torch.ops.aten.scatter_.value: Torch_AtenScatter_ValueOp,  # type: ignore
    torch.ops.aten.scatter_add.default: Torch_AtenScatterAddOp,  # type: ignore
    torch.ops.aten.scatter_add_.default: Torch_AtenScatterAdd_Op,  # type: ignore
    torch.ops.aten.select.int: Torch_AtenSelectIntOp,  # type: ignore
    torch.ops.aten.select_backward.default: Torch_AtenSelectBackwardOp,  # type: ignore
    torch.ops.aten.select_scatter.default: Torch_AtenSelectScatterOp,  # type: ignore
    torch.ops.aten.selu.default: Torch_AtenSeluOp,  # type: ignore
    torch.ops.aten.selu_.default: Torch_AtenSelu_Op,  # type: ignore
    torch.ops.aten.set_.default: Torch_AtenSet_Op,  # type: ignore
    torch.ops.aten.set_.source_Tensor: Torch_AtenSet_SourceTensorOp,  # type: ignore
    torch.ops.aten.set_.source_Tensor_storage_offset: Torch_AtenSet_SourceTensorStorageOffsetOp,  # type: ignore
    torch.ops.aten.set_data.default: Torch_AtenSetDataOp,  # type: ignore
    torch.ops.aten.sgn.default: Torch_AtenSgnOp,  # type: ignore
    torch.ops.aten.sgn_.default: Torch_AtenSgn_Op,  # type: ignore
    torch.ops.aten.sigmoid.default: Torch_AtenSigmoidOp,  # type: ignore
    torch.ops.aten.sigmoid_.default: Torch_AtenSigmoid_Op,  # type: ignore
    torch.ops.aten.sigmoid_backward.default: Torch_AtenSigmoidBackwardOp,  # type: ignore
    torch.ops.aten.sigmoid_backward.grad_input: Torch_AtenSigmoidBackwardGradInputOp,  # type: ignore
    torch.ops.aten.sign.default: Torch_AtenSignOp,  # type: ignore
    torch.ops.aten.sign_.default: Torch_AtenSign_Op,  # type: ignore
    torch.ops.aten.signbit.default: Torch_AtenSignbitOp,  # type: ignore
    torch.ops.aten.silu.default: Torch_AtenSiluOp,  # type: ignore
    torch.ops.aten.silu_.default: Torch_AtenSilu_Op,  # type: ignore
    torch.ops.aten.silu_backward.default: Torch_AtenSiluBackwardOp,  # type: ignore
    torch.ops.aten.silu_backward.grad_input: Torch_AtenSiluBackwardGradInputOp,  # type: ignore
    torch.ops.aten.sin.Scalar: Torch_AtenSinScalarOp,  # type: ignore
    torch.ops.aten.sin.default: Torch_AtenSinOp,  # type: ignore
    torch.ops.aten.sin.float: Torch_AtenSinFloatOp,  # type: ignore
    torch.ops.aten.sin.int: Torch_AtenSinIntOp,  # type: ignore
    torch.ops.aten.sin_.default: Torch_AtenSin_Op,  # type: ignore
    torch.ops.aten.sinc.default: Torch_AtenSincOp,  # type: ignore
    torch.ops.aten.sinc_.default: Torch_AtenSinc_Op,  # type: ignore
    torch.ops.aten.sinh.Scalar: Torch_AtenSinhScalarOp,  # type: ignore
    torch.ops.aten.sinh.default: Torch_AtenSinhOp,  # type: ignore
    torch.ops.aten.sinh.float: Torch_AtenSinhFloatOp,  # type: ignore
    torch.ops.aten.sinh.int: Torch_AtenSinhIntOp,  # type: ignore
    torch.ops.aten.sinh_.default: Torch_AtenSinh_Op,  # type: ignore
    torch.ops.aten.size.default: Torch_AtenSizeOp,  # type: ignore
    torch.ops.aten.size.int: Torch_AtenSizeIntOp,  # type: ignore
    torch.ops.aten.slice.Tensor: Torch_AtenSliceTensorOp,  # type: ignore
    torch.ops.aten.slice_backward.default: Torch_AtenSliceBackwardOp,  # type: ignore
    torch.ops.aten.slice_inverse.default: Torch_AtenSliceInverseOp,  # type: ignore
    torch.ops.aten.slice_scatter.default: Torch_AtenSliceScatterOp,  # type: ignore
    torch.ops.aten.slogdet.default: Torch_AtenSlogdetOp,  # type: ignore
    torch.ops.aten.slow_conv3d.default: Torch_AtenSlowConv3DOp,  # type: ignore
    torch.ops.aten.slow_conv3d_forward.default: Torch_AtenSlowConv3DForwardOp,  # type: ignore
    torch.ops.aten.slow_conv3d_forward.output: Torch_AtenSlowConv3DForwardOutputOp,  # type: ignore
    torch.ops.aten.slow_conv_dilated2d.default: Torch_AtenSlowConvDilated2DOp,  # type: ignore
    torch.ops.aten.slow_conv_dilated3d.default: Torch_AtenSlowConvDilated3DOp,  # type: ignore
    torch.ops.aten.slow_conv_transpose2d.default: Torch_AtenSlowConvTranspose2DOp,  # type: ignore
    torch.ops.aten.slow_conv_transpose3d.default: Torch_AtenSlowConvTranspose3DOp,  # type: ignore
    torch.ops.aten.smm.default: Torch_AtenSmmOp,  # type: ignore
    torch.ops.aten.smooth_l1_loss.default: Torch_AtenSmoothL1LossOp,  # type: ignore
    torch.ops.aten.smooth_l1_loss_backward.default: Torch_AtenSmoothL1LossBackwardOp,  # type: ignore
    torch.ops.aten.smooth_l1_loss_backward.grad_input: Torch_AtenSmoothL1LossBackwardGradInputOp,  # type: ignore
    torch.ops.aten.soft_margin_loss.default: Torch_AtenSoftMarginLossOp,  # type: ignore
    torch.ops.aten.soft_margin_loss_backward.default: Torch_AtenSoftMarginLossBackwardOp,  # type: ignore
    torch.ops.aten.soft_margin_loss_backward.grad_input: Torch_AtenSoftMarginLossBackwardGradInputOp,  # type: ignore
    torch.ops.aten.softplus.default: Torch_AtenSoftplusOp,  # type: ignore
    torch.ops.aten.softplus_backward.default: Torch_AtenSoftplusBackwardOp,  # type: ignore
    torch.ops.aten.softplus_backward.grad_input: Torch_AtenSoftplusBackwardGradInputOp,  # type: ignore
    torch.ops.aten.softshrink.default: Torch_AtenSoftshrinkOp,  # type: ignore
    torch.ops.aten.softshrink_backward.default: Torch_AtenSoftshrinkBackwardOp,  # type: ignore
    torch.ops.aten.softshrink_backward.grad_input: Torch_AtenSoftshrinkBackwardGradInputOp,  # type: ignore
    torch.ops.aten.sort.Tensor: Torch_AtenSortTensorOp,  # type: ignore
    torch.ops.aten.sort.bool: Torch_AtenSortBoolOp,  # type: ignore
    torch.ops.aten.sort.default: Torch_AtenSortOp,  # type: ignore
    torch.ops.aten.sort.float: Torch_AtenSortFloatOp,  # type: ignore
    torch.ops.aten.sort.int: Torch_AtenSortIntOp,  # type: ignore
    torch.ops.aten.sort.stable: Torch_AtenSortStableOp,  # type: ignore
    torch.ops.aten.sparse_dim.default: Torch_AtenSparseDimOp,  # type: ignore
    torch.ops.aten.sparse_mask.default: Torch_AtenSparseMaskOp,  # type: ignore
    torch.ops.aten.sparse_resize_.default: Torch_AtenSparseResize_Op,  # type: ignore
    torch.ops.aten.sparse_resize_and_clear_.default: Torch_AtenSparseResizeAndClear_Op,  # type: ignore
    torch.ops.aten.sparse_sampled_addmm.default: Torch_AtenSparseSampledAddmmOp,  # type: ignore
    torch.ops.aten.special_airy_ai.default: Torch_AtenSpecialAiryAiOp,  # type: ignore
    torch.ops.aten.special_bessel_j0.default: Torch_AtenSpecialBesselJ0Op,  # type: ignore
    torch.ops.aten.special_bessel_j1.default: Torch_AtenSpecialBesselJ1Op,  # type: ignore
    torch.ops.aten.special_bessel_y0.default: Torch_AtenSpecialBesselY0Op,  # type: ignore
    torch.ops.aten.special_bessel_y1.default: Torch_AtenSpecialBesselY1Op,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_t.default: Torch_AtenSpecialChebyshevPolynomialTOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_t.n_scalar: Torch_AtenSpecialChebyshevPolynomialTNScalarOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_t.n_scalar_out: Torch_AtenSpecialChebyshevPolynomialTNScalarOutOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_t.x_scalar: Torch_AtenSpecialChebyshevPolynomialTXScalarOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_t.x_scalar_out: Torch_AtenSpecialChebyshevPolynomialTXScalarOutOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_u.default: Torch_AtenSpecialChebyshevPolynomialUOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_u.n_scalar: Torch_AtenSpecialChebyshevPolynomialUNScalarOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_u.n_scalar_out: Torch_AtenSpecialChebyshevPolynomialUNScalarOutOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_u.x_scalar: Torch_AtenSpecialChebyshevPolynomialUXScalarOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_u.x_scalar_out: Torch_AtenSpecialChebyshevPolynomialUXScalarOutOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_v.default: Torch_AtenSpecialChebyshevPolynomialVOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_v.n_scalar: Torch_AtenSpecialChebyshevPolynomialVNScalarOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_v.n_scalar_out: Torch_AtenSpecialChebyshevPolynomialVNScalarOutOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_v.x_scalar: Torch_AtenSpecialChebyshevPolynomialVXScalarOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_v.x_scalar_out: Torch_AtenSpecialChebyshevPolynomialVXScalarOutOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_w.default: Torch_AtenSpecialChebyshevPolynomialWOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_w.n_scalar: Torch_AtenSpecialChebyshevPolynomialWNScalarOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_w.n_scalar_out: Torch_AtenSpecialChebyshevPolynomialWNScalarOutOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_w.x_scalar: Torch_AtenSpecialChebyshevPolynomialWXScalarOp,  # type: ignore
    torch.ops.aten.special_chebyshev_polynomial_w.x_scalar_out: Torch_AtenSpecialChebyshevPolynomialWXScalarOutOp,  # type: ignore
    torch.ops.aten.special_digamma.default: Torch_AtenSpecialDigammaOp,  # type: ignore
    torch.ops.aten.special_entr.default: Torch_AtenSpecialEntrOp,  # type: ignore
    torch.ops.aten.special_erf.default: Torch_AtenSpecialErfOp,  # type: ignore
    torch.ops.aten.special_erfc.default: Torch_AtenSpecialErfcOp,  # type: ignore
    torch.ops.aten.special_erfcx.default: Torch_AtenSpecialErfcxOp,  # type: ignore
    torch.ops.aten.special_erfinv.default: Torch_AtenSpecialErfinvOp,  # type: ignore
    torch.ops.aten.special_exp2.default: Torch_AtenSpecialExp2Op,  # type: ignore
    torch.ops.aten.special_expit.default: Torch_AtenSpecialExpitOp,  # type: ignore
    torch.ops.aten.special_expm1.default: Torch_AtenSpecialExpm1Op,  # type: ignore
    torch.ops.aten.special_gammainc.default: Torch_AtenSpecialGammaincOp,  # type: ignore
    torch.ops.aten.special_gammaincc.default: Torch_AtenSpecialGammainccOp,  # type: ignore
    torch.ops.aten.special_gammaln.default: Torch_AtenSpecialGammalnOp,  # type: ignore
    torch.ops.aten.special_hermite_polynomial_h.default: Torch_AtenSpecialHermitePolynomialHOp,  # type: ignore
    torch.ops.aten.special_hermite_polynomial_h.n_scalar: Torch_AtenSpecialHermitePolynomialHNScalarOp,  # type: ignore
    torch.ops.aten.special_hermite_polynomial_h.n_scalar_out: Torch_AtenSpecialHermitePolynomialHNScalarOutOp,  # type: ignore
    torch.ops.aten.special_hermite_polynomial_h.x_scalar: Torch_AtenSpecialHermitePolynomialHXScalarOp,  # type: ignore
    torch.ops.aten.special_hermite_polynomial_h.x_scalar_out: Torch_AtenSpecialHermitePolynomialHXScalarOutOp,  # type: ignore
    torch.ops.aten.special_hermite_polynomial_he.default: Torch_AtenSpecialHermitePolynomialHeOp,  # type: ignore
    torch.ops.aten.special_hermite_polynomial_he.n_scalar: Torch_AtenSpecialHermitePolynomialHeNScalarOp,  # type: ignore
    torch.ops.aten.special_hermite_polynomial_he.n_scalar_out: Torch_AtenSpecialHermitePolynomialHeNScalarOutOp,  # type: ignore
    torch.ops.aten.special_hermite_polynomial_he.x_scalar: Torch_AtenSpecialHermitePolynomialHeXScalarOp,  # type: ignore
    torch.ops.aten.special_hermite_polynomial_he.x_scalar_out: Torch_AtenSpecialHermitePolynomialHeXScalarOutOp,  # type: ignore
    torch.ops.aten.special_i0.default: Torch_AtenSpecialI0Op,  # type: ignore
    torch.ops.aten.special_i0e.default: Torch_AtenSpecialI0EOp,  # type: ignore
    torch.ops.aten.special_i1.default: Torch_AtenSpecialI1Op,  # type: ignore
    torch.ops.aten.special_i1e.default: Torch_AtenSpecialI1EOp,  # type: ignore
    torch.ops.aten.special_laguerre_polynomial_l.default: Torch_AtenSpecialLaguerrePolynomialLOp,  # type: ignore
    torch.ops.aten.special_laguerre_polynomial_l.n_scalar: Torch_AtenSpecialLaguerrePolynomialLNScalarOp,  # type: ignore
    torch.ops.aten.special_laguerre_polynomial_l.n_scalar_out: Torch_AtenSpecialLaguerrePolynomialLNScalarOutOp,  # type: ignore
    torch.ops.aten.special_laguerre_polynomial_l.x_scalar: Torch_AtenSpecialLaguerrePolynomialLXScalarOp,  # type: ignore
    torch.ops.aten.special_laguerre_polynomial_l.x_scalar_out: Torch_AtenSpecialLaguerrePolynomialLXScalarOutOp,  # type: ignore
    torch.ops.aten.special_legendre_polynomial_p.default: Torch_AtenSpecialLegendrePolynomialPOp,  # type: ignore
    torch.ops.aten.special_legendre_polynomial_p.n_scalar: Torch_AtenSpecialLegendrePolynomialPNScalarOp,  # type: ignore
    torch.ops.aten.special_legendre_polynomial_p.n_scalar_out: Torch_AtenSpecialLegendrePolynomialPNScalarOutOp,  # type: ignore
    torch.ops.aten.special_legendre_polynomial_p.x_scalar: Torch_AtenSpecialLegendrePolynomialPXScalarOp,  # type: ignore
    torch.ops.aten.special_legendre_polynomial_p.x_scalar_out: Torch_AtenSpecialLegendrePolynomialPXScalarOutOp,  # type: ignore
    torch.ops.aten.special_log1p.default: Torch_AtenSpecialLog1POp,  # type: ignore
    torch.ops.aten.special_log_ndtr.default: Torch_AtenSpecialLogNdtrOp,  # type: ignore
    torch.ops.aten.special_log_softmax.default: Torch_AtenSpecialLogSoftmaxOp,  # type: ignore
    torch.ops.aten.special_logit.default: Torch_AtenSpecialLogitOp,  # type: ignore
    torch.ops.aten.special_logsumexp.default: Torch_AtenSpecialLogsumexpOp,  # type: ignore
    torch.ops.aten.special_modified_bessel_i0.default: Torch_AtenSpecialModifiedBesselI0Op,  # type: ignore
    torch.ops.aten.special_modified_bessel_i1.default: Torch_AtenSpecialModifiedBesselI1Op,  # type: ignore
    torch.ops.aten.special_modified_bessel_k0.default: Torch_AtenSpecialModifiedBesselK0Op,  # type: ignore
    torch.ops.aten.special_modified_bessel_k1.default: Torch_AtenSpecialModifiedBesselK1Op,  # type: ignore
    torch.ops.aten.special_multigammaln.default: Torch_AtenSpecialMultigammalnOp,  # type: ignore
    torch.ops.aten.special_ndtr.default: Torch_AtenSpecialNdtrOp,  # type: ignore
    torch.ops.aten.special_ndtri.default: Torch_AtenSpecialNdtriOp,  # type: ignore
    torch.ops.aten.special_polygamma.default: Torch_AtenSpecialPolygammaOp,  # type: ignore
    torch.ops.aten.special_psi.default: Torch_AtenSpecialPsiOp,  # type: ignore
    torch.ops.aten.special_round.default: Torch_AtenSpecialRoundOp,  # type: ignore
    torch.ops.aten.special_scaled_modified_bessel_k0.default: Torch_AtenSpecialScaledModifiedBesselK0Op,  # type: ignore
    torch.ops.aten.special_scaled_modified_bessel_k1.default: Torch_AtenSpecialScaledModifiedBesselK1Op,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_t.default: Torch_AtenSpecialShiftedChebyshevPolynomialTOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_t.n_scalar: Torch_AtenSpecialShiftedChebyshevPolynomialTNScalarOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_t.n_scalar_out: Torch_AtenSpecialShiftedChebyshevPolynomialTNScalarOutOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_t.x_scalar: Torch_AtenSpecialShiftedChebyshevPolynomialTXScalarOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_t.x_scalar_out: Torch_AtenSpecialShiftedChebyshevPolynomialTXScalarOutOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_u.default: Torch_AtenSpecialShiftedChebyshevPolynomialUOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_u.n_scalar: Torch_AtenSpecialShiftedChebyshevPolynomialUNScalarOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_u.n_scalar_out: Torch_AtenSpecialShiftedChebyshevPolynomialUNScalarOutOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_u.x_scalar: Torch_AtenSpecialShiftedChebyshevPolynomialUXScalarOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_u.x_scalar_out: Torch_AtenSpecialShiftedChebyshevPolynomialUXScalarOutOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_v.default: Torch_AtenSpecialShiftedChebyshevPolynomialVOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_v.n_scalar: Torch_AtenSpecialShiftedChebyshevPolynomialVNScalarOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_v.n_scalar_out: Torch_AtenSpecialShiftedChebyshevPolynomialVNScalarOutOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_v.x_scalar: Torch_AtenSpecialShiftedChebyshevPolynomialVXScalarOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_v.x_scalar_out: Torch_AtenSpecialShiftedChebyshevPolynomialVXScalarOutOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_w.default: Torch_AtenSpecialShiftedChebyshevPolynomialWOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_w.n_scalar: Torch_AtenSpecialShiftedChebyshevPolynomialWNScalarOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_w.n_scalar_out: Torch_AtenSpecialShiftedChebyshevPolynomialWNScalarOutOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_w.x_scalar: Torch_AtenSpecialShiftedChebyshevPolynomialWXScalarOp,  # type: ignore
    torch.ops.aten.special_shifted_chebyshev_polynomial_w.x_scalar_out: Torch_AtenSpecialShiftedChebyshevPolynomialWXScalarOutOp,  # type: ignore
    torch.ops.aten.special_sinc.default: Torch_AtenSpecialSincOp,  # type: ignore
    torch.ops.aten.special_softmax.default: Torch_AtenSpecialSoftmaxOp,  # type: ignore
    torch.ops.aten.special_spherical_bessel_j0.default: Torch_AtenSpecialSphericalBesselJ0Op,  # type: ignore
    torch.ops.aten.special_xlog1py.default: Torch_AtenSpecialXlog1PyOp,  # type: ignore
    torch.ops.aten.special_xlog1py.other_scalar: Torch_AtenSpecialXlog1PyOtherScalarOp,  # type: ignore
    torch.ops.aten.special_xlog1py.other_scalar_out: Torch_AtenSpecialXlog1PyOtherScalarOutOp,  # type: ignore
    torch.ops.aten.special_xlog1py.self_scalar: Torch_AtenSpecialXlog1PySelfScalarOp,  # type: ignore
    torch.ops.aten.special_xlog1py.self_scalar_out: Torch_AtenSpecialXlog1PySelfScalarOutOp,  # type: ignore
    torch.ops.aten.special_xlogy.default: Torch_AtenSpecialXlogyOp,  # type: ignore
    torch.ops.aten.special_xlogy.other_scalar: Torch_AtenSpecialXlogyOtherScalarOp,  # type: ignore
    torch.ops.aten.special_xlogy.other_scalar_out: Torch_AtenSpecialXlogyOtherScalarOutOp,  # type: ignore
    torch.ops.aten.special_xlogy.self_scalar: Torch_AtenSpecialXlogySelfScalarOp,  # type: ignore
    torch.ops.aten.special_xlogy.self_scalar_out: Torch_AtenSpecialXlogySelfScalarOutOp,  # type: ignore
    torch.ops.aten.special_zeta.default: Torch_AtenSpecialZetaOp,  # type: ignore
    torch.ops.aten.special_zeta.other_scalar: Torch_AtenSpecialZetaOtherScalarOp,  # type: ignore
    torch.ops.aten.special_zeta.other_scalar_out: Torch_AtenSpecialZetaOtherScalarOutOp,  # type: ignore
    torch.ops.aten.special_zeta.self_scalar: Torch_AtenSpecialZetaSelfScalarOp,  # type: ignore
    torch.ops.aten.special_zeta.self_scalar_out: Torch_AtenSpecialZetaSelfScalarOutOp,  # type: ignore
    torch.ops.aten.split.Tensor: Torch_AtenSplitTensorOp,  # type: ignore
    torch.ops.aten.split.default: Torch_AtenSplitOp,  # type: ignore
    torch.ops.aten.split.sizes: Torch_AtenSplitSizesOp,  # type: ignore
    torch.ops.aten.split_with_sizes.default: Torch_AtenSplitWithSizesOp,  # type: ignore
    torch.ops.aten.split_with_sizes_copy.default: Torch_AtenSplitWithSizesCopyOp,  # type: ignore
    torch.ops.aten.sqrt.Scalar: Torch_AtenSqrtScalarOp,  # type: ignore
    torch.ops.aten.sqrt.default: Torch_AtenSqrtOp,  # type: ignore
    torch.ops.aten.sqrt.float: Torch_AtenSqrtFloatOp,  # type: ignore
    torch.ops.aten.sqrt.int: Torch_AtenSqrtIntOp,  # type: ignore
    torch.ops.aten.sqrt_.default: Torch_AtenSqrt_Op,  # type: ignore
    torch.ops.aten.square.default: Torch_AtenSquareOp,  # type: ignore
    torch.ops.aten.square_.default: Torch_AtenSquare_Op,  # type: ignore
    torch.ops.aten.squeeze.default: Torch_AtenSqueezeOp,  # type: ignore
    torch.ops.aten.squeeze.dim: Torch_AtenSqueezeDimOp,  # type: ignore
    torch.ops.aten.squeeze.dims: Torch_AtenSqueezeDimsOp,  # type: ignore
    torch.ops.aten.squeeze_.default: Torch_AtenSqueeze_Op,  # type: ignore
    torch.ops.aten.squeeze_.dim: Torch_AtenSqueeze_DimOp,  # type: ignore
    torch.ops.aten.squeeze_.dims: Torch_AtenSqueeze_DimsOp,  # type: ignore
    torch.ops.aten.squeeze_copy.default: Torch_AtenSqueezeCopyOp,  # type: ignore
    torch.ops.aten.squeeze_copy.dim: Torch_AtenSqueezeCopyDimOp,  # type: ignore
    torch.ops.aten.squeeze_copy.dim_out: Torch_AtenSqueezeCopyDimOutOp,  # type: ignore
    torch.ops.aten.squeeze_copy.dims: Torch_AtenSqueezeCopyDimsOp,  # type: ignore
    torch.ops.aten.squeeze_copy.dims_out: Torch_AtenSqueezeCopyDimsOutOp,  # type: ignore
    torch.ops.aten.sspaddmm.default: Torch_AtenSspaddmmOp,  # type: ignore
    torch.ops.aten.stack.default: Torch_AtenStackOp,  # type: ignore
    torch.ops.aten.std.correction: Torch_AtenStdCorrectionOp,  # type: ignore
    torch.ops.aten.std.correction_out: Torch_AtenStdCorrectionOutOp,  # type: ignore
    torch.ops.aten.std.default: Torch_AtenStdOp,  # type: ignore
    torch.ops.aten.std.dim: Torch_AtenStdDimOp,  # type: ignore
    torch.ops.aten.std_mean.correction: Torch_AtenStdMeanCorrectionOp,  # type: ignore
    torch.ops.aten.std_mean.correction_out: Torch_AtenStdMeanCorrectionOutOp,  # type: ignore
    torch.ops.aten.std_mean.default: Torch_AtenStdMeanOp,  # type: ignore
    torch.ops.aten.std_mean.dim: Torch_AtenStdMeanDimOp,  # type: ignore
    torch.ops.aten.stft.default: Torch_AtenStftOp,  # type: ignore
    torch.ops.aten.storage_offset.default: Torch_AtenStorageOffsetOp,  # type: ignore
    torch.ops.aten.stride.default: Torch_AtenStrideOp,  # type: ignore
    torch.ops.aten.stride.int: Torch_AtenStrideIntOp,  # type: ignore
    torch.ops.aten.sub.Scalar: Torch_AtenSubScalarOp,  # type: ignore
    torch.ops.aten.sub.Scalar_out: Torch_AtenSubScalarOutOp,  # type: ignore
    torch.ops.aten.sub.Tensor: Torch_AtenSubTensorOp,  # type: ignore
    torch.ops.aten.sub.default: Torch_AtenSubOp,  # type: ignore
    torch.ops.aten.sub.float: Torch_AtenSubFloatOp,  # type: ignore
    torch.ops.aten.sub.float_int: Torch_AtenSubFloatIntOp,  # type: ignore
    torch.ops.aten.sub.int: Torch_AtenSubIntOp,  # type: ignore
    torch.ops.aten.sub.int_float: Torch_AtenSubIntFloatOp,  # type: ignore
    torch.ops.aten.sub_.Scalar: Torch_AtenSub_ScalarOp,  # type: ignore
    torch.ops.aten.sub_.Tensor: Torch_AtenSub_TensorOp,  # type: ignore
    torch.ops.aten.subtract.Scalar: Torch_AtenSubtractScalarOp,  # type: ignore
    torch.ops.aten.subtract.Tensor: Torch_AtenSubtractTensorOp,  # type: ignore
    torch.ops.aten.subtract_.Scalar: Torch_AtenSubtract_ScalarOp,  # type: ignore
    torch.ops.aten.subtract_.Tensor: Torch_AtenSubtract_TensorOp,  # type: ignore
    torch.ops.aten.sum.IntList_out: Torch_AtenSumIntlistOutOp,  # type: ignore
    torch.ops.aten.sum.bool: Torch_AtenSumBoolOp,  # type: ignore
    torch.ops.aten.sum.default: Torch_AtenSumOp,  # type: ignore
    torch.ops.aten.sum.dim_IntList: Torch_AtenSumDimIntlistOp,  # type: ignore
    torch.ops.aten.sum.float: Torch_AtenSumFloatOp,  # type: ignore
    torch.ops.aten.sum.int: Torch_AtenSumIntOp,  # type: ignore
    torch.ops.aten.sum_to_size.default: Torch_AtenSumToSizeOp,  # type: ignore
    torch.ops.aten.svd.default: Torch_AtenSvdOp,  # type: ignore
    torch.ops.aten.swapaxes.default: Torch_AtenSwapaxesOp,  # type: ignore
    torch.ops.aten.swapaxes_.default: Torch_AtenSwapaxes_Op,  # type: ignore
    torch.ops.aten.swapdims.default: Torch_AtenSwapdimsOp,  # type: ignore
    torch.ops.aten.swapdims_.default: Torch_AtenSwapdims_Op,  # type: ignore
    torch.ops.aten.sym_constrain_range.default: Torch_AtenSymConstrainRangeOp,  # type: ignore
    torch.ops.aten.sym_constrain_range_for_size.default: Torch_AtenSymConstrainRangeForSizeOp,  # type: ignore
    torch.ops.aten.sym_numel.default: Torch_AtenSymNumelOp,  # type: ignore
    torch.ops.aten.sym_size.default: Torch_AtenSymSizeOp,  # type: ignore
    torch.ops.aten.sym_size.int: Torch_AtenSymSizeIntOp,  # type: ignore
    torch.ops.aten.sym_storage_offset.default: Torch_AtenSymStorageOffsetOp,  # type: ignore
    torch.ops.aten.sym_stride.default: Torch_AtenSymStrideOp,  # type: ignore
    torch.ops.aten.sym_stride.int: Torch_AtenSymStrideIntOp,  # type: ignore
    torch.ops.aten.t.default: Torch_AtenTOp,  # type: ignore
    torch.ops.aten.t_.default: Torch_AtenT_Op,  # type: ignore
    torch.ops.aten.t_copy.default: Torch_AtenTCopyOp,  # type: ignore
    torch.ops.aten.take.default: Torch_AtenTakeOp,  # type: ignore
    torch.ops.aten.take_along_dim.default: Torch_AtenTakeAlongDimOp,  # type: ignore
    torch.ops.aten.tan.Scalar: Torch_AtenTanScalarOp,  # type: ignore
    torch.ops.aten.tan.default: Torch_AtenTanOp,  # type: ignore
    torch.ops.aten.tan.float: Torch_AtenTanFloatOp,  # type: ignore
    torch.ops.aten.tan.int: Torch_AtenTanIntOp,  # type: ignore
    torch.ops.aten.tan_.default: Torch_AtenTan_Op,  # type: ignore
    torch.ops.aten.tanh.Scalar: Torch_AtenTanhScalarOp,  # type: ignore
    torch.ops.aten.tanh.default: Torch_AtenTanhOp,  # type: ignore
    torch.ops.aten.tanh.float: Torch_AtenTanhFloatOp,  # type: ignore
    torch.ops.aten.tanh.int: Torch_AtenTanhIntOp,  # type: ignore
    torch.ops.aten.tanh_.default: Torch_AtenTanh_Op,  # type: ignore
    torch.ops.aten.tanh_backward.default: Torch_AtenTanhBackwardOp,  # type: ignore
    torch.ops.aten.tanh_backward.grad_input: Torch_AtenTanhBackwardGradInputOp,  # type: ignore
    torch.ops.aten.tensor_split.indices: Torch_AtenTensorSplitIndicesOp,  # type: ignore
    torch.ops.aten.tensor_split.sections: Torch_AtenTensorSplitSectionsOp,  # type: ignore
    torch.ops.aten.tensor_split.tensor_indices_or_sections: Torch_AtenTensorSplitTensorIndicesOrSectionsOp,  # type: ignore
    torch.ops.aten.tensordot.default: Torch_AtenTensordotOp,  # type: ignore
    torch.ops.aten.thnn_conv2d.default: Torch_AtenThnnConv2DOp,  # type: ignore
    torch.ops.aten.threshold.default: Torch_AtenThresholdOp,  # type: ignore
    torch.ops.aten.threshold_.default: Torch_AtenThreshold_Op,  # type: ignore
    torch.ops.aten.threshold_backward.default: Torch_AtenThresholdBackwardOp,  # type: ignore
    torch.ops.aten.threshold_backward.grad_input: Torch_AtenThresholdBackwardGradInputOp,  # type: ignore
    torch.ops.aten.tile.default: Torch_AtenTileOp,  # type: ignore
    torch.ops.aten.to.dtype: Torch_AtenToDtypeOp,  # type: ignore
    torch.ops.aten.to.other: Torch_AtenToOtherOp,  # type: ignore
    torch.ops.aten.to.prim_dtype: Torch_AtenToPrimDtypeOp,  # type: ignore
    torch.ops.aten.to.prim_other: Torch_AtenToPrimOtherOp,  # type: ignore
    torch.ops.aten.to_dense.default: Torch_AtenToDenseOp,  # type: ignore
    torch.ops.aten.to_dense_backward.default: Torch_AtenToDenseBackwardOp,  # type: ignore
    torch.ops.aten.to_mkldnn.default: Torch_AtenToMkldnnOp,  # type: ignore
    torch.ops.aten.to_mkldnn_backward.default: Torch_AtenToMkldnnBackwardOp,  # type: ignore
    torch.ops.aten.to_padded_tensor.default: Torch_AtenToPaddedTensorOp,  # type: ignore
    torch.ops.aten.to_sparse.default: Torch_AtenToSparseOp,  # type: ignore
    torch.ops.aten.to_sparse.sparse_dim: Torch_AtenToSparseSparseDimOp,  # type: ignore
    torch.ops.aten.to_sparse_bsc.default: Torch_AtenToSparseBscOp,  # type: ignore
    torch.ops.aten.to_sparse_bsr.default: Torch_AtenToSparseBsrOp,  # type: ignore
    torch.ops.aten.to_sparse_csc.default: Torch_AtenToSparseCscOp,  # type: ignore
    torch.ops.aten.to_sparse_csr.default: Torch_AtenToSparseCsrOp,  # type: ignore
    torch.ops.aten.topk.default: Torch_AtenTopkOp,  # type: ignore
    torch.ops.aten.trace.default: Torch_AtenTraceOp,  # type: ignore
    torch.ops.aten.trace_backward.default: Torch_AtenTraceBackwardOp,  # type: ignore
    torch.ops.aten.transpose.int: Torch_AtenTransposeIntOp,  # type: ignore
    torch.ops.aten.transpose_.default: Torch_AtenTranspose_Op,  # type: ignore
    torch.ops.aten.transpose_copy.int: Torch_AtenTransposeCopyIntOp,  # type: ignore
    torch.ops.aten.transpose_copy.int_out: Torch_AtenTransposeCopyIntOutOp,  # type: ignore
    torch.ops.aten.triangular_solve.X: Torch_AtenTriangularSolveXOp,  # type: ignore
    torch.ops.aten.triangular_solve.default: Torch_AtenTriangularSolveOp,  # type: ignore
    torch.ops.aten.tril.default: Torch_AtenTrilOp,  # type: ignore
    torch.ops.aten.tril_.default: Torch_AtenTril_Op,  # type: ignore
    torch.ops.aten.triplet_margin_loss.default: Torch_AtenTripletMarginLossOp,  # type: ignore
    torch.ops.aten.triu.default: Torch_AtenTriuOp,  # type: ignore
    torch.ops.aten.triu_.default: Torch_AtenTriu_Op,  # type: ignore
    torch.ops.aten.true_divide.Scalar: Torch_AtenTrueDivideScalarOp,  # type: ignore
    torch.ops.aten.true_divide.Tensor: Torch_AtenTrueDivideTensorOp,  # type: ignore
    torch.ops.aten.true_divide_.Scalar: Torch_AtenTrueDivide_ScalarOp,  # type: ignore
    torch.ops.aten.true_divide_.Tensor: Torch_AtenTrueDivide_TensorOp,  # type: ignore
    torch.ops.aten.trunc.default: Torch_AtenTruncOp,  # type: ignore
    torch.ops.aten.trunc_.default: Torch_AtenTrunc_Op,  # type: ignore
    torch.ops.aten.type_as.default: Torch_AtenTypeAsOp,  # type: ignore
    torch.ops.aten.unbind.int: Torch_AtenUnbindIntOp,  # type: ignore
    torch.ops.aten.unflatten_dense_tensors.default: Torch_AtenUnflattenDenseTensorsOp,  # type: ignore
    torch.ops.aten.unfold.default: Torch_AtenUnfoldOp,  # type: ignore
    torch.ops.aten.unfold_backward.default: Torch_AtenUnfoldBackwardOp,  # type: ignore
    torch.ops.aten.unfold_copy.default: Torch_AtenUnfoldCopyOp,  # type: ignore
    torch.ops.aten.unique_consecutive.default: Torch_AtenUniqueConsecutiveOp,  # type: ignore
    torch.ops.aten.unique_dim.default: Torch_AtenUniqueDimOp,  # type: ignore
    torch.ops.aten.unique_dim_consecutive.default: Torch_AtenUniqueDimConsecutiveOp,  # type: ignore
    torch.ops.aten.unsafe_chunk.default: Torch_AtenUnsafeChunkOp,  # type: ignore
    torch.ops.aten.unsafe_split.Tensor: Torch_AtenUnsafeSplitTensorOp,  # type: ignore
    torch.ops.aten.unsafe_split.Tensor_out: Torch_AtenUnsafeSplitTensorOutOp,  # type: ignore
    torch.ops.aten.unsafe_split_with_sizes.default: Torch_AtenUnsafeSplitWithSizesOp,  # type: ignore
    torch.ops.aten.unsqueeze.default: Torch_AtenUnsqueezeOp,  # type: ignore
    torch.ops.aten.unsqueeze_.default: Torch_AtenUnsqueeze_Op,  # type: ignore
    torch.ops.aten.unsqueeze_copy.default: Torch_AtenUnsqueezeCopyOp,  # type: ignore
    torch.ops.aten.upsample_bicubic2d.default: Torch_AtenUpsampleBicubic2DOp,  # type: ignore
    torch.ops.aten.upsample_bicubic2d.vec: Torch_AtenUpsampleBicubic2DVecOp,  # type: ignore
    torch.ops.aten.upsample_bicubic2d_backward.default: Torch_AtenUpsampleBicubic2DBackwardOp,  # type: ignore
    torch.ops.aten.upsample_bicubic2d_backward.grad_input: Torch_AtenUpsampleBicubic2DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.upsample_bilinear2d.default: Torch_AtenUpsampleBilinear2DOp,  # type: ignore
    torch.ops.aten.upsample_bilinear2d.vec: Torch_AtenUpsampleBilinear2DVecOp,  # type: ignore
    torch.ops.aten.upsample_bilinear2d.vec_out: Torch_AtenUpsampleBilinear2DVecOutOp,  # type: ignore
    torch.ops.aten.upsample_bilinear2d_backward.default: Torch_AtenUpsampleBilinear2DBackwardOp,  # type: ignore
    torch.ops.aten.upsample_bilinear2d_backward.grad_input: Torch_AtenUpsampleBilinear2DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.upsample_linear1d.default: Torch_AtenUpsampleLinear1DOp,  # type: ignore
    torch.ops.aten.upsample_linear1d.vec: Torch_AtenUpsampleLinear1DVecOp,  # type: ignore
    torch.ops.aten.upsample_linear1d_backward.default: Torch_AtenUpsampleLinear1DBackwardOp,  # type: ignore
    torch.ops.aten.upsample_linear1d_backward.grad_input: Torch_AtenUpsampleLinear1DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.upsample_nearest1d.default: Torch_AtenUpsampleNearest1DOp,  # type: ignore
    torch.ops.aten.upsample_nearest1d.vec: Torch_AtenUpsampleNearest1DVecOp,  # type: ignore
    torch.ops.aten.upsample_nearest1d_backward.default: Torch_AtenUpsampleNearest1DBackwardOp,  # type: ignore
    torch.ops.aten.upsample_nearest1d_backward.grad_input: Torch_AtenUpsampleNearest1DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.upsample_nearest2d.default: Torch_AtenUpsampleNearest2DOp,  # type: ignore
    torch.ops.aten.upsample_nearest2d.vec: Torch_AtenUpsampleNearest2DVecOp,  # type: ignore
    torch.ops.aten.upsample_nearest2d.vec_out: Torch_AtenUpsampleNearest2DVecOutOp,  # type: ignore
    torch.ops.aten.upsample_nearest2d_backward.default: Torch_AtenUpsampleNearest2DBackwardOp,  # type: ignore
    torch.ops.aten.upsample_nearest2d_backward.grad_input: Torch_AtenUpsampleNearest2DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.upsample_nearest3d.default: Torch_AtenUpsampleNearest3DOp,  # type: ignore
    torch.ops.aten.upsample_nearest3d.vec: Torch_AtenUpsampleNearest3DVecOp,  # type: ignore
    torch.ops.aten.upsample_nearest3d_backward.default: Torch_AtenUpsampleNearest3DBackwardOp,  # type: ignore
    torch.ops.aten.upsample_nearest3d_backward.grad_input: Torch_AtenUpsampleNearest3DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.upsample_trilinear3d.default: Torch_AtenUpsampleTrilinear3DOp,  # type: ignore
    torch.ops.aten.upsample_trilinear3d.vec: Torch_AtenUpsampleTrilinear3DVecOp,  # type: ignore
    torch.ops.aten.upsample_trilinear3d_backward.default: Torch_AtenUpsampleTrilinear3DBackwardOp,  # type: ignore
    torch.ops.aten.upsample_trilinear3d_backward.grad_input: Torch_AtenUpsampleTrilinear3DBackwardGradInputOp,  # type: ignore
    torch.ops.aten.value_selecting_reduction_backward.default: Torch_AtenValueSelectingReductionBackwardOp,  # type: ignore
    torch.ops.aten.values.default: Torch_AtenValuesOp,  # type: ignore
    torch.ops.aten.values_copy.default: Torch_AtenValuesCopyOp,  # type: ignore
    torch.ops.aten.vander.default: Torch_AtenVanderOp,  # type: ignore
    torch.ops.aten.var.correction: Torch_AtenVarCorrectionOp,  # type: ignore
    torch.ops.aten.var.correction_out: Torch_AtenVarCorrectionOutOp,  # type: ignore
    torch.ops.aten.var.default: Torch_AtenVarOp,  # type: ignore
    torch.ops.aten.var.dim: Torch_AtenVarDimOp,  # type: ignore
    torch.ops.aten.var_mean.correction: Torch_AtenVarMeanCorrectionOp,  # type: ignore
    torch.ops.aten.var_mean.correction_out: Torch_AtenVarMeanCorrectionOutOp,  # type: ignore
    torch.ops.aten.var_mean.default: Torch_AtenVarMeanOp,  # type: ignore
    torch.ops.aten.var_mean.dim: Torch_AtenVarMeanDimOp,  # type: ignore
    torch.ops.aten.vdot.default: Torch_AtenVdotOp,  # type: ignore
    torch.ops.aten.view.default: Torch_AtenViewOp,  # type: ignore
    torch.ops.aten.view.dtype: Torch_AtenViewDtypeOp,  # type: ignore
    torch.ops.aten.view_as.default: Torch_AtenViewAsOp,  # type: ignore
    torch.ops.aten.view_as_complex.default: Torch_AtenViewAsComplexOp,  # type: ignore
    torch.ops.aten.view_as_complex_copy.default: Torch_AtenViewAsComplexCopyOp,  # type: ignore
    torch.ops.aten.view_as_real.default: Torch_AtenViewAsRealOp,  # type: ignore
    torch.ops.aten.view_as_real_copy.default: Torch_AtenViewAsRealCopyOp,  # type: ignore
    torch.ops.aten.view_copy.default: Torch_AtenViewCopyOp,  # type: ignore
    torch.ops.aten.view_copy.dtype: Torch_AtenViewCopyDtypeOp,  # type: ignore
    torch.ops.aten.view_copy.dtype_out: Torch_AtenViewCopyDtypeOutOp,  # type: ignore
    torch.ops.aten.vstack.default: Torch_AtenVstackOp,  # type: ignore
    torch.ops.aten.where.Scalar: Torch_AtenWhereScalarOp,  # type: ignore
    torch.ops.aten.where.ScalarOther: Torch_AtenWhereScalarotherOp,  # type: ignore
    torch.ops.aten.where.ScalarSelf: Torch_AtenWhereScalarselfOp,  # type: ignore
    torch.ops.aten.where.default: Torch_AtenWhereOp,  # type: ignore
    torch.ops.aten.where.self: Torch_AtenWhereSelfOp,  # type: ignore
    torch.ops.aten.where.self_out: Torch_AtenWhereSelfOutOp,  # type: ignore
    torch.ops.aten.xlogy.OutScalar_Other: Torch_AtenXlogyOutscalarOtherOp,  # type: ignore
    torch.ops.aten.xlogy.OutScalar_Self: Torch_AtenXlogyOutscalarSelfOp,  # type: ignore
    torch.ops.aten.xlogy.OutTensor: Torch_AtenXlogyOuttensorOp,  # type: ignore
    torch.ops.aten.xlogy.Scalar_Other: Torch_AtenXlogyScalarOtherOp,  # type: ignore
    torch.ops.aten.xlogy.Scalar_Self: Torch_AtenXlogyScalarSelfOp,  # type: ignore
    torch.ops.aten.xlogy.Tensor: Torch_AtenXlogyTensorOp,  # type: ignore
    torch.ops.aten.xlogy_.Scalar_Other: Torch_AtenXlogy_ScalarOtherOp,  # type: ignore
    torch.ops.aten.xlogy_.Tensor: Torch_AtenXlogy_TensorOp,  # type: ignore
    torch.ops.aten.zero.default: Torch_AtenZeroOp,  # type: ignore
    torch.ops.aten.zero_.default: Torch_AtenZero_Op,  # type: ignore
    torch.ops.inductor.accumulate_grad_.default: Torch_InductorAccumulateGrad_Op,  # type: ignore
    torch.ops.inductor.resize_storage_bytes_.default: Torch_InductorResizeStorageBytes_Op,  # type: ignore
    torch.ops.prims._make_token.default: Torch_Prims_MakeTokenOp,  # type: ignore
    torch.ops.prims._sink_tokens.default: Torch_Prims_SinkTokensOp,  # type: ignore
    torch.ops.prims.abs.default: Torch_PrimsAbsOp,  # type: ignore
    torch.ops.prims.acos.default: Torch_PrimsAcosOp,  # type: ignore
    torch.ops.prims.acosh.default: Torch_PrimsAcoshOp,  # type: ignore
    torch.ops.prims.add.default: Torch_PrimsAddOp,  # type: ignore
    torch.ops.prims.amax.default: Torch_PrimsAmaxOp,  # type: ignore
    torch.ops.prims.amin.default: Torch_PrimsAminOp,  # type: ignore
    torch.ops.prims.as_strided.default: Torch_PrimsAsStridedOp,  # type: ignore
    torch.ops.prims.as_strided_scatter.default: Torch_PrimsAsStridedScatterOp,  # type: ignore
    torch.ops.prims.asin.default: Torch_PrimsAsinOp,  # type: ignore
    torch.ops.prims.asinh.default: Torch_PrimsAsinhOp,  # type: ignore
    torch.ops.prims.atan.default: Torch_PrimsAtanOp,  # type: ignore
    torch.ops.prims.atan2.default: Torch_PrimsAtan2Op,  # type: ignore
    torch.ops.prims.atanh.default: Torch_PrimsAtanhOp,  # type: ignore
    torch.ops.prims.bessel_i0.default: Torch_PrimsBesselI0Op,  # type: ignore
    torch.ops.prims.bessel_i0e.default: Torch_PrimsBesselI0EOp,  # type: ignore
    torch.ops.prims.bessel_i1.default: Torch_PrimsBesselI1Op,  # type: ignore
    torch.ops.prims.bessel_i1e.default: Torch_PrimsBesselI1EOp,  # type: ignore
    torch.ops.prims.bessel_j0.default: Torch_PrimsBesselJ0Op,  # type: ignore
    torch.ops.prims.bessel_j1.default: Torch_PrimsBesselJ1Op,  # type: ignore
    torch.ops.prims.bitwise_and.default: Torch_PrimsBitwiseAndOp,  # type: ignore
    torch.ops.prims.bitwise_not.default: Torch_PrimsBitwiseNotOp,  # type: ignore
    torch.ops.prims.bitwise_or.default: Torch_PrimsBitwiseOrOp,  # type: ignore
    torch.ops.prims.bitwise_xor.default: Torch_PrimsBitwiseXorOp,  # type: ignore
    torch.ops.prims.broadcast_in_dim.default: Torch_PrimsBroadcastInDimOp,  # type: ignore
    torch.ops.prims.cat.default: Torch_PrimsCatOp,  # type: ignore
    torch.ops.prims.cbrt.default: Torch_PrimsCbrtOp,  # type: ignore
    torch.ops.prims.ceil.default: Torch_PrimsCeilOp,  # type: ignore
    torch.ops.prims.clone.default: Torch_PrimsCloneOp,  # type: ignore
    torch.ops.prims.collapse.default: Torch_PrimsCollapseOp,  # type: ignore
    torch.ops.prims.collapse_view.default: Torch_PrimsCollapseViewOp,  # type: ignore
    torch.ops.prims.conj.default: Torch_PrimsConjOp,  # type: ignore
    torch.ops.prims.conj_physical.default: Torch_PrimsConjPhysicalOp,  # type: ignore
    torch.ops.prims.convert_element_type.default: Torch_PrimsConvertElementTypeOp,  # type: ignore
    torch.ops.prims.copy_strided.default: Torch_PrimsCopyStridedOp,  # type: ignore
    torch.ops.prims.copy_to.default: Torch_PrimsCopyToOp,  # type: ignore
    torch.ops.prims.cos.default: Torch_PrimsCosOp,  # type: ignore
    torch.ops.prims.cosh.default: Torch_PrimsCoshOp,  # type: ignore
    torch.ops.prims.digamma.default: Torch_PrimsDigammaOp,  # type: ignore
    torch.ops.prims.div.default: Torch_PrimsDivOp,  # type: ignore
    torch.ops.prims.eq.default: Torch_PrimsEqOp,  # type: ignore
    torch.ops.prims.erf.default: Torch_PrimsErfOp,  # type: ignore
    torch.ops.prims.erf_inv.default: Torch_PrimsErfInvOp,  # type: ignore
    torch.ops.prims.erfc.default: Torch_PrimsErfcOp,  # type: ignore
    torch.ops.prims.erfcx.default: Torch_PrimsErfcxOp,  # type: ignore
    torch.ops.prims.exp.default: Torch_PrimsExpOp,  # type: ignore
    torch.ops.prims.exp2.default: Torch_PrimsExp2Op,  # type: ignore
    torch.ops.prims.expm1.default: Torch_PrimsExpm1Op,  # type: ignore
    torch.ops.prims.fft_c2c.default: Torch_PrimsFftC2COp,  # type: ignore
    torch.ops.prims.fft_c2r.default: Torch_PrimsFftC2ROp,  # type: ignore
    torch.ops.prims.fft_r2c.default: Torch_PrimsFftR2COp,  # type: ignore
    torch.ops.prims.fill.default: Torch_PrimsFillOp,  # type: ignore
    torch.ops.prims.floor.default: Torch_PrimsFloorOp,  # type: ignore
    torch.ops.prims.fmax.default: Torch_PrimsFmaxOp,  # type: ignore
    torch.ops.prims.fmin.default: Torch_PrimsFminOp,  # type: ignore
    torch.ops.prims.fmod.default: Torch_PrimsFmodOp,  # type: ignore
    torch.ops.prims.frexp.default: Torch_PrimsFrexpOp,  # type: ignore
    torch.ops.prims.gcd.default: Torch_PrimsGcdOp,  # type: ignore
    torch.ops.prims.ge.default: Torch_PrimsGeOp,  # type: ignore
    torch.ops.prims.gt.default: Torch_PrimsGtOp,  # type: ignore
    torch.ops.prims.hypot.default: Torch_PrimsHypotOp,  # type: ignore
    torch.ops.prims.igamma.default: Torch_PrimsIgammaOp,  # type: ignore
    torch.ops.prims.igammac.default: Torch_PrimsIgammacOp,  # type: ignore
    torch.ops.prims.imag.default: Torch_PrimsImagOp,  # type: ignore
    torch.ops.prims.isfinite.default: Torch_PrimsIsfiniteOp,  # type: ignore
    torch.ops.prims.item.default: Torch_PrimsItemOp,  # type: ignore
    torch.ops.prims.le.default: Torch_PrimsLeOp,  # type: ignore
    torch.ops.prims.lgamma.default: Torch_PrimsLgammaOp,  # type: ignore
    torch.ops.prims.log.default: Torch_PrimsLogOp,  # type: ignore
    torch.ops.prims.log10.default: Torch_PrimsLog10Op,  # type: ignore
    torch.ops.prims.log1p.default: Torch_PrimsLog1POp,  # type: ignore
    torch.ops.prims.log2.default: Torch_PrimsLog2Op,  # type: ignore
    torch.ops.prims.lt.default: Torch_PrimsLtOp,  # type: ignore
    torch.ops.prims.maximum.default: Torch_PrimsMaximumOp,  # type: ignore
    torch.ops.prims.maximum_value.default: Torch_PrimsMaximumValueOp,  # type: ignore
    torch.ops.prims.minimum.default: Torch_PrimsMinimumOp,  # type: ignore
    torch.ops.prims.minimum_value.default: Torch_PrimsMinimumValueOp,  # type: ignore
    torch.ops.prims.mul.default: Torch_PrimsMulOp,  # type: ignore
    torch.ops.prims.ndtri.default: Torch_PrimsNdtriOp,  # type: ignore
    torch.ops.prims.ne.default: Torch_PrimsNeOp,  # type: ignore
    torch.ops.prims.neg.default: Torch_PrimsNegOp,  # type: ignore
    torch.ops.prims.nextafter.default: Torch_PrimsNextafterOp,  # type: ignore
    torch.ops.prims.pow.default: Torch_PrimsPowOp,  # type: ignore
    torch.ops.prims.prod.default: Torch_PrimsProdOp,  # type: ignore
    torch.ops.prims.real.default: Torch_PrimsRealOp,  # type: ignore
    torch.ops.prims.reciprocal.default: Torch_PrimsReciprocalOp,  # type: ignore
    torch.ops.prims.remainder.default: Torch_PrimsRemainderOp,  # type: ignore
    torch.ops.prims.reshape.default: Torch_PrimsReshapeOp,  # type: ignore
    torch.ops.prims.resize.default: Torch_PrimsResizeOp,  # type: ignore
    torch.ops.prims.rev.default: Torch_PrimsRevOp,  # type: ignore
    torch.ops.prims.round.default: Torch_PrimsRoundOp,  # type: ignore
    torch.ops.prims.rsqrt.default: Torch_PrimsRsqrtOp,  # type: ignore
    torch.ops.prims.shift_left.default: Torch_PrimsShiftLeftOp,  # type: ignore
    torch.ops.prims.shift_right_arithmetic.default: Torch_PrimsShiftRightArithmeticOp,  # type: ignore
    torch.ops.prims.sign.default: Torch_PrimsSignOp,  # type: ignore
    torch.ops.prims.signbit.default: Torch_PrimsSignbitOp,  # type: ignore
    torch.ops.prims.sin.default: Torch_PrimsSinOp,  # type: ignore
    torch.ops.prims.sinh.default: Torch_PrimsSinhOp,  # type: ignore
    torch.ops.prims.spherical_bessel_j0.default: Torch_PrimsSphericalBesselJ0Op,  # type: ignore
    torch.ops.prims.split_dim.default: Torch_PrimsSplitDimOp,  # type: ignore
    torch.ops.prims.sqrt.default: Torch_PrimsSqrtOp,  # type: ignore
    torch.ops.prims.squeeze.default: Torch_PrimsSqueezeOp,  # type: ignore
    torch.ops.prims.sub.default: Torch_PrimsSubOp,  # type: ignore
    torch.ops.prims.sum.default: Torch_PrimsSumOp,  # type: ignore
    torch.ops.prims.svd.default: Torch_PrimsSvdOp,  # type: ignore
    torch.ops.prims.tan.default: Torch_PrimsTanOp,  # type: ignore
    torch.ops.prims.tanh.default: Torch_PrimsTanhOp,  # type: ignore
    torch.ops.prims.transpose.default: Torch_PrimsTransposeOp,  # type: ignore
    torch.ops.prims.trunc.default: Torch_PrimsTruncOp,  # type: ignore
    torch.ops.prims.var.default: Torch_PrimsVarOp,  # type: ignore
    torch.ops.prims.view_of.default: Torch_PrimsViewOfOp,  # type: ignore
    torch.ops.prims.view_of_dtype.default: Torch_PrimsViewOfDtypeOp,  # type: ignore
    torch.ops.prims.where.default: Torch_PrimsWhereOp,  # type: ignore
    torch.ops.prims.xor_sum.default: Torch_PrimsXorSumOp,  # type: ignore
    torch.ops.prims.zeta.default: Torch_PrimsZetaOp,  # type: ignore
    torch.ops.profiler._record_function_exit.default: Torch_Profiler_RecordFunctionExitOp,  # type: ignore
    torch.ops.quantized.dropout.default: Torch_QuantizedDropoutOp,  # type: ignore
    torch.ops.quantized.elu.default: Torch_QuantizedEluOp,  # type: ignore
    torch.ops.quantized.hardswish.default: Torch_QuantizedHardswishOp,  # type: ignore
    torch.ops.quantized.instance_norm.default: Torch_QuantizedInstanceNormOp,  # type: ignore
    torch.ops.quantized.layer_norm.default: Torch_QuantizedLayerNormOp,  # type: ignore
    torch.ops.quantized.leaky_relu.default: Torch_QuantizedLeakyReluOp,  # type: ignore
}

REVERSE_XDSL_TORCH_OPS = {
    xdsl_op: torch_op for torch_op, xdsl_op in XDSL_TORCH_OPS.items()
}
